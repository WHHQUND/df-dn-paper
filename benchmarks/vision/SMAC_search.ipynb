{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAC Search for Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import *\n",
    "import warnings\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import librosa\n",
    "import time\n",
    "import re\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ConfigSpace import (\n",
    "    Categorical,\n",
    "    Configuration,\n",
    "    ConfigurationSpace,\n",
    "    EqualsCondition,\n",
    "    Float,\n",
    "    InCondition,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt.callbacks import DeadlineStopper\n",
    "\n",
    "from smac import MultiFidelityFacade as MFFacade\n",
    "from smac import Scenario\n",
    "from smac.facade import AbstractFacade\n",
    "from smac.intensifier.hyperband import Hyperband\n",
    "from smac.intensifier.successive_halving import SuccessiveHalving\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as trans\n",
    "import re\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def combinations_45(iterable, r):\n",
    "    \"\"\"Extracts 45 combinations from given list\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    count = 0\n",
    "    while count < 44:\n",
    "        count += 1\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i + 1, r):\n",
    "            indices[j] = indices[j - 1] + 1\n",
    "        yield tuple(pool[i] for i in indices)\n",
    "\n",
    "torch.multiprocessing.freeze_support()\n",
    "n_classes = 3 # number of classes\n",
    "samples_space = np.geomspace(10, 9000, num=8, dtype=int)\n",
    "nums = list(range(10))\n",
    "random.shuffle(nums)\n",
    "classes_space = list(combinations_45(nums, n_classes))\n",
    "normalize = lambda x: x / 255.0\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root=\"./\", train=True, download=True, transform=None\n",
    ")\n",
    "cifar_train_images = normalize(cifar_trainset.data)\n",
    "cifar_train_labels = np.array(cifar_trainset.targets)\n",
    "\n",
    "cifar_testset = datasets.CIFAR10(\n",
    "    root=\"./\", train=False, download=True, transform=None\n",
    ")\n",
    "cifar_test_images = normalize(cifar_testset.data)\n",
    "cifar_test_labels = np.array(cifar_testset.targets)\n",
    "\n",
    "# cifar_train_images = cifar_train_images.reshape(-1, 32 * 32 * 3)\n",
    "# cifar_test_images = cifar_test_images.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "images = np.concatenate((cifar_train_images, cifar_test_images))\n",
    "labels = np.concatenate((cifar_train_labels, cifar_test_labels))\n",
    "\n",
    "indices = np.arange(images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_images, test_valid_images, train_labels, test_valid_labels = train_test_split(\n",
    "    images, labels, test_size=0.5, random_state=317\n",
    ")\n",
    "\n",
    "test_images, valid_images, test_labels, valid_labels = train_test_split(\n",
    "    test_valid_images, test_valid_labels, test_size=0.5, random_state=317\n",
    ")\n",
    "\n",
    "train_x = train_images\n",
    "train_y = train_labels\n",
    "test_x = test_images\n",
    "test_y = test_labels\n",
    "valid_x = valid_images\n",
    "valid_y = valid_labels\n",
    "test_valid_x = test_valid_images\n",
    "test_valid_y = test_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN32Filter(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a simple CNN arhcitecture with 1 layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=10, stride=2)\n",
    "        self.fc1 = nn.Linear(144 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, 144 * 32)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCNN32Filter2Layers(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a simple CNN arhcitecture with 2 layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.fc1 = nn.Linear(12 * 12 * 32, 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(b, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCNN32Filter5Layers(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a simple CNN arhcitecture with 5 layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(8192, 200)\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        self.maxpool = nn.MaxPool2d((2, 2))\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = F.relu(self.bn(self.conv1(x)))\n",
    "        x = F.relu(self.bn(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = F.relu(self.bn2(self.conv4(x)))\n",
    "        x = F.relu(self.bn3(self.conv5(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(b, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn32 = SimpleCNN32Filter(num_classes=18)\n",
    "cnn32_2l = SimpleCNN32Filter2Layers(num_classes=18)\n",
    "cnn32_5l = SimpleCNN32Filter5Layers(num_classes=18)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBT tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='multi:softprob', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.seed= seed\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.colsample_bynode = colsample_bynode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.objective = objective\n",
    "        self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "        gamma = Float(\"gamma\", (0.1, 1), default=0.1)\n",
    "        subsample = Float(\"subsample\", (0.5, 1), default=0.8)\n",
    "        colsample_bytree = Float(\"colsample_bytree\", (0.3, 1), default=0.6)\n",
    "        colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1), default=0.6)\n",
    "        colsample_bynode = Float(\"colsample_bynode\", (0.3, 1), default=0.6)\n",
    "        learning_rate = Float(\"learning_rate\", (0.01, 0.3), default=0.1)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        scores = cross_val_score(self.model, X, y, cv=5)\n",
    "        return 1 - np.mean(scores)\n",
    "    \n",
    "\n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 22:20:58,800 - distributed.nanny - WARNING - Worker process still alive after 3.1999014282226566 seconds, killing\n",
      "2024-05-07 22:20:58,810 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2024-05-07 22:20:58,824 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2024-05-07 22:20:58,831 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -6370.488914966583\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 01:14:25,544 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.61 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:26,367 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 6.52 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:27,642 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 6.24 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:35,642 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.24 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:36,090 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.47 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:36,143 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 6.16 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:39,442 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.43 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:39,742 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 6.34 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:45,643 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.28 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:14:55,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.19 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:05,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.20 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:15,942 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.62 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:25,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.94 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:36,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.05 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:46,141 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.08 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:15:56,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.28 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:06,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.16 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:16,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.68 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:26,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.87 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:36,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.94 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:46,342 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.10 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:16:57,044 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.74 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:07,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.61 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:17,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.92 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:27,942 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.62 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:37,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.82 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:47,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.76 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:17:57,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.75 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:07,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.95 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:17,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.05 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:28,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.11 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:38,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.15 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:48,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.94 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:18:58,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.01 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:08,342 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.99 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:18,343 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.97 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:28,443 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.82 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:38,543 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.77 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:48,643 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.00 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:19:58,643 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.87 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:08,643 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.97 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:18,742 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.98 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:28,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.97 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:38,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.02 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:48,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.99 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:20:59,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.01 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:09,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.07 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:19,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.00 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:29,343 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.79 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:39,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.63 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:49,842 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.00 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:21:59,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.00 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:22:09,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.05 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:22:19,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.84 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:22:29,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.03 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:22:40,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.84 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:22:50,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.98 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:00,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.80 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:10,241 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.99 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:20,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.97 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:30,243 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.07 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:40,343 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.89 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:23:50,443 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 6.03 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:24:00,541 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.82 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:24:10,543 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.82 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:24:20,543 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.85 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:24:30,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.76 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:24:40,643 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.76 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:42:34,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.69 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:42:34,858 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 6.44 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:42:36,042 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 6.27 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 01:42:48,842 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.61 GiB -- Worker memory limit: 8.00 GiB\n",
      "2024-05-08 02:14:12,339 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.64 GiB -- Worker memory limit: 8.00 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incumbent: None\n",
      "Default cost (Hyperband): 0.5995666666666667\n",
      "Incumbent cost (Hyperband): inf\n",
      "ask <bound method AbstractFacade.ask of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>>\n",
      "get_acquisition_function <function HyperparameterOptimizationFacade.get_acquisition_function at 0x7f87e0c768b0>\n",
      "get_acquisition_maximizer <function HyperparameterOptimizationFacade.get_acquisition_maximizer at 0x7f87e0c76940>\n",
      "get_config_selector <function AbstractFacade.get_config_selector at 0x7f88334b2f70>\n",
      "get_initial_design <function MultiFidelityFacade.get_initial_design at 0x7f87e0c76f70>\n",
      "get_intensifier <function MultiFidelityFacade.get_intensifier at 0x7f87e0c76ee0>\n",
      "get_model <function HyperparameterOptimizationFacade.get_model at 0x7f87e0c76820>\n",
      "get_multi_objective_algorithm <function HyperparameterOptimizationFacade.get_multi_objective_algorithm at 0x7f87e0c76b80>\n",
      "get_random_design <function HyperparameterOptimizationFacade.get_random_design at 0x7f87e0c76af0>\n",
      "get_runhistory_encoder <function HyperparameterOptimizationFacade.get_runhistory_encoder at 0x7f87e0c76c10>\n",
      "intensifier <smac.intensifier.hyperband.Hyperband object at 0x7f8808f944f0>\n",
      "meta {'facade': {'name': 'MultiFidelityFacade'}, 'runner': {'name': 'DaskParallelRunner'}, 'model': {'name': 'RandomForest', 'types': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'bounds': [(0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0)], 'pca_components': 7, 'n_trees': 10, 'n_points_per_tree': -1, 'ratio_features': 1.0, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 1048576, 'eps_purity': 1e-08, 'max_nodes': 1048576, 'bootstrapping': True}, 'acquisition_maximizer': {'name': 'LocalAndSortedRandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 10000, 'seed': 0, 'random_search': {'name': 'RandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0}, 'local_search': {'name': 'LocalSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0, 'max_steps': None, 'n_steps_plateau_walk': 10, 'vectorization_min_obtain': 2, 'vectorization_max_obtain': 64}}, 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'intensifier': {'name': 'Hyperband', 'max_incumbents': 10, 'seed': 0, 'eta': 3, 'instance_seed_order': 'shuffle_once', 'incumbent_selection': 'highest_budget'}, 'initial_design': {'name': 'RandomInitialDesign', 'n_configs': 5, 'n_configs_per_hyperparameter': 10, 'additional_configs': [], 'seed': 0}, 'random_design': {'name': 'ProbabilityRandomDesign', 'seed': 0, 'probability': 0.2}, 'runhistory_encoder': {'name': 'RunHistoryLogScaledEncoder', 'considered_states': [<StatusType.SUCCESS: 1>, <StatusType.CRASHED: 2>, <StatusType.MEMORYOUT: 4>], 'lower_budget_states': [], 'scale_percentage': 5, 'seed': 0}, 'multi_objective_algorithm': None, 'config_selector': {'name': 'ConfigSelector', 'retrain_after': 8, 'retries': 16, 'min_trials': 1}, 'version': '2.0.2'}\n",
      "optimize <bound method AbstractFacade.optimize of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>>\n",
      "optimizer <smac.main.smbo.SMBO object at 0x7f8833ff8df0>\n",
      "runhistory <smac.runhistory.runhistory.RunHistory object at 0x7f87e0e08a00>\n",
      "scenario Scenario(configspace=Configuration space object:\n",
      "  Hyperparameters:\n",
      "    colsample_bylevel, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bynode, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bytree, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    gamma, Type: UniformFloat, Range: [0.1, 1.0], Default: 0.1\n",
      "    learning_rate, Type: UniformFloat, Range: [0.01, 0.3], Default: 0.1\n",
      "    max_depth, Type: UniformInteger, Range: [2, 21], Default: 2\n",
      "    min_child_weight, Type: UniformInteger, Range: [1, 10], Default: 1\n",
      "    n_estimators, Type: UniformInteger, Range: [100, 1200], Default: 100\n",
      "    subsample, Type: UniformFloat, Range: [0.5, 1.0], Default: 0.8\n",
      ", name='56e93b9169af80edb12371783e3d29b7', output_directory=PosixPath('smac_hyperband_output_budget_30mins_XGBT/56e93b9169af80edb12371783e3d29b7/0'), deterministic=False, objectives='cost', crash_cost=inf, termination_cost_threshold=inf, walltime_limit=1800, cputime_limit=inf, trial_walltime_limit=None, trial_memory_limit=None, n_trials=10000, use_default_config=False, instances=None, instance_features=None, min_budget=100, max_budget=1000, seed=0, n_workers=2)\n",
      "tell <bound method AbstractFacade.tell of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>>\n",
      "validate <bound method AbstractFacade.validate of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>>\n",
      "facades: [<smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f8810f69e50>]\n",
      "\n",
      "facades[0]: Scenario(configspace=Configuration space object:\n",
      "  Hyperparameters:\n",
      "    colsample_bylevel, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bynode, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bytree, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    gamma, Type: UniformFloat, Range: [0.1, 1.0], Default: 0.1\n",
      "    learning_rate, Type: UniformFloat, Range: [0.01, 0.3], Default: 0.1\n",
      "    max_depth, Type: UniformInteger, Range: [2, 21], Default: 2\n",
      "    min_child_weight, Type: UniformInteger, Range: [1, 10], Default: 1\n",
      "    n_estimators, Type: UniformInteger, Range: [100, 1200], Default: 100\n",
      "    subsample, Type: UniformFloat, Range: [0.5, 1.0], Default: 0.8\n",
      ", name='56e93b9169af80edb12371783e3d29b7', output_directory=PosixPath('smac_hyperband_output_budget_30mins_XGBT/56e93b9169af80edb12371783e3d29b7/0'), deterministic=False, objectives='cost', crash_cost=inf, termination_cost_threshold=inf, walltime_limit=1800, cputime_limit=inf, trial_walltime_limit=None, trial_memory_limit=None, n_trials=10000, use_default_config=False, instances=None, instance_features=None, min_budget=100, max_budget=1000, seed=0, n_workers=2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 18708.7 s\n",
      "File: /var/folders/w0/pq4q2kj50fvdt6t_4y4gqfvw0000gn/T/ipykernel_3285/3359647891.py\n",
      "Function: main at line 2\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     2                                           def main():\n",
      "     3         1     501000.0 501000.0      0.0      GBT = XGBWrapper()\n",
      "     4                                           \n",
      "     5         1          0.0      0.0      0.0      facades: list[AbstractFacade] = []\n",
      "     6         2       3000.0   1500.0      0.0      for intensifier_object in [Hyperband]:\n",
      "     7                                           \n",
      "     8         2     194000.0  97000.0      0.0          scenario = Scenario(\n",
      "     9         1    4724000.0    5e+06      0.0              GBT.configspace,\n",
      "    10         1       1000.0   1000.0      0.0              walltime_limit = 1800,\n",
      "    11         1     355000.0 355000.0      0.0              output_directory=Path(\"smac_hyperband_output_budget_30mins_XGBT\"),\n",
      "    12         1       1000.0   1000.0      0.0              n_trials=10000,\n",
      "    13         1          0.0      0.0      0.0              min_budget=100,\n",
      "    14         1          0.0      0.0      0.0              max_budget=1000,\n",
      "    15         1       1000.0   1000.0      0.0              n_workers=2,\n",
      "    16                                           \n",
      "    17                                                   )\n",
      "    18                                           \n",
      "    19         1    6199000.0    6e+06      0.0          initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
      "    20         1    2772000.0    3e+06      0.0          intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
      "    21                                           \n",
      "    22         2 1633239000.0    8e+08      0.0          smac = MFFacade(\n",
      "    23         1       1000.0   1000.0      0.0              scenario,\n",
      "    24         1       3000.0   3000.0      0.0              GBT.fit,\n",
      "    25         1          0.0      0.0      0.0              initial_design=initial_design,\n",
      "    26         1          0.0      0.0      0.0              intensifier=intensifier,\n",
      "    27         1          0.0      0.0      0.0              overwrite=True,\n",
      "    28                                                   )\n",
      "    29                                           \n",
      "    30         1     129000.0 129000.0      0.0          print(\"optimiizing\")\n",
      "    31         1      19000.0  19000.0      0.0          print(type(smac), \"|\", smac)\n",
      "    32         1        2e+13    2e+13     96.0          incumbent = smac.optimize()\n",
      "    33         1     323000.0 323000.0      0.0          print(\"incumbent:\", incumbent)\n",
      "    34         1        7e+11    7e+11      4.0          default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
      "    35         1     104000.0 104000.0      0.0          print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
      "    36         1    2600000.0    3e+06      0.0          incumbent_cost = smac.validate(incumbent)\n",
      "    37         1      29000.0  29000.0      0.0          print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
      "    38                                           \n",
      "    39         1          0.0      0.0      0.0          facades.append(smac)\n",
      "    40        64      86000.0   1343.8      0.0          for arrt in dir(smac):\n",
      "    41        63      20000.0    317.5      0.0              if not arrt.startswith(\"_\"):\n",
      "    42        18     609000.0  33833.3      0.0                  print(arrt, getattr(smac, arrt))\n",
      "    43                                           \n",
      "    44         1       9000.0   9000.0      0.0      print(\"facades:\", facades)\n",
      "    45         1          0.0      0.0      0.0      BO_file = None\n",
      "    46         1  268764000.0    3e+08      0.0      plot_trajectory(facades, BO_file)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @timeout(1800)\n",
    "def main():\n",
    "    GBT = XGBWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            GBT.configspace,\n",
    "            walltime_limit = 1800,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_30mins_XGBT\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=2,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            GBT.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = None\n",
    "    plot_trajectory(facades, BO_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features   \n",
    "        self.criterion = \"gini\"\n",
    "        self.max_samples = max_samples\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "        min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "        criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "        max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "        max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        if config['max_features'] == 'None':\n",
    "            config['max_features'] = None\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        scores = cross_val_score(self.model, X, y, cv=5)\n",
    "        return 1 - np.mean(scores)\n",
    "    \n",
    "\n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7f98907230d0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "'Timed Out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m profiler\u001b[39m.\u001b[39madd_function(main)\n\u001b[1;32m     54\u001b[0m profiler\u001b[39m.\u001b[39menable()\n\u001b[0;32m---> 56\u001b[0m main()\n\u001b[1;32m     58\u001b[0m profiler\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m     59\u001b[0m profiler\u001b[39m.\u001b[39mprint_stats()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:82\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.new_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m new_seconds:\n",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moptimiizing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(smac), \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m, smac)\n\u001b[0;32m---> 32\u001b[0m incumbent \u001b[39m=\u001b[39m smac\u001b[39m.\u001b[39;49moptimize()\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mincumbent:\u001b[39m\u001b[39m\"\u001b[39m, incumbent)\n\u001b[1;32m     34\u001b[0m default_cost \u001b[39m=\u001b[39m smac\u001b[39m.\u001b[39mvalidate(RF\u001b[39m.\u001b[39mconfigspace\u001b[39m.\u001b[39mget_default_configuration())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/facade/abstract_facade.py:319\u001b[0m, in \u001b[0;36mAbstractFacade.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata_to_scatter must be None or dict with some elements, but got an empty dict.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     incumbents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer\u001b[39m.\u001b[39;49moptimize(data_to_scatter\u001b[39m=\u001b[39;49mdata_to_scatter)\n\u001b[1;32m    320\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/main/smbo.py:304\u001b[0m, in \u001b[0;36mSMBO.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    300\u001b[0m     trial_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mask()\n\u001b[1;32m    302\u001b[0m     \u001b[39m# We submit the trial to the runner\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# In multi-worker mode, SMAC waits till a new worker is available here\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runner\u001b[39m.\u001b[39;49msubmit_trial(trial_info\u001b[39m=\u001b[39;49mtrial_info, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdask_data_to_scatter)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/runner/dask_runner.py:144\u001b[0m, in \u001b[0;36mDaskParallelRunner.submit_trial\u001b[0;34m(self, trial_info, **dask_data_to_scatter)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTried to execute a job, but no worker was ever available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis likely means that a worker crashed or no workers were properly configured.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    143\u001b[0m \u001b[39m# At this point we can submit the job\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msubmit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_single_worker\u001b[39m.\u001b[39;49mrun_wrapper, trial_info\u001b[39m=\u001b[39;49mtrial_info, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdask_data_to_scatter)\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_trials\u001b[39m.\u001b[39mappend(trial)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/client.py:1942\u001b[0m, in \u001b[0;36mClient.submit\u001b[0;34m(self, func, key, workers, resources, retries, priority, fifo_timeout, allow_other_workers, actor, actors, pure, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mif\u001b[39;00m pure:\n\u001b[0;32m-> 1942\u001b[0m         key \u001b[39m=\u001b[39m funcname(func) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m tokenize(func, kwargs, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1943\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m         key \u001b[39m=\u001b[39m funcname(func) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/dask/base.py:998\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    990\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deterministic token\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[39m    >>> tokenize([1, 2, '3'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 998\u001b[0m     hasher \u001b[39m=\u001b[39m _md5(\u001b[39mstr\u001b[39;49m(\u001b[39mtuple\u001b[39;49m(\u001b[39mmap\u001b[39;49m(normalize_token, args)))\u001b[39m.\u001b[39mencode())\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m   1000\u001b[0m         hasher\u001b[39m.\u001b[39mupdate(\u001b[39mstr\u001b[39m(normalize_token(kwargs))\u001b[39m.\u001b[39mencode())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:69\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[0;32m---> 69\u001b[0m     _raise_exception(timeout_exception, exception_message)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:45\u001b[0m, in \u001b[0;36m_raise_exception\u001b[0;34m(exception, exception_message)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" This function checks if a exception message is given.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39mIf there is no exception message, the default behaviour is maintained.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mIf there is an exception message, the message is passed to the exception with the 'value' keyword.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m exception_message \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m exception()\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m exception(exception_message)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: 'Timed Out'"
     ]
    }
   ],
   "source": [
    "# @timeout(60)\n",
    "def main():\n",
    "    RF = RFWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            RF.configspace,\n",
    "            walltime_limit = 1800,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_30mins_RF\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=4,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            RF.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = None\n",
    "    plot_trajectory(facades, BO_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DN tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "# train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "# test_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "# test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "# valid_tensor = torch.tensor(valid_images, dtype=torch.float32)\n",
    "# valid_labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels_tensor)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels_tensor)\n",
    "# valid_dataset = torch.utils.data.TensorDataset(valid_tensor, valid_labels_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=4)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=4)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNN \n",
    "data_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN32Wrapper(BaseEstimator):\n",
    "    def __init__(self, criterion = nn.CrossEntropyLoss()):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = cnn32.to(self.device)\n",
    "        self.criterion = criterion\n",
    "        self.lr = None\n",
    "        self.batch_size = None\n",
    "        self.epochs = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    @property\n",
    "    def configspace(self)-> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        lr_init = Float(\"lr_init\", (0.001, 0.1), log=True)\n",
    "        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n",
    "        solver = Categorical(\"solver\", [\"sgd\", \"adam\"])\n",
    "        batch_size = Integer(\"batch_size\", (16, 1024))\n",
    "        epochs = Integer(\"epochs\", (30, 120))\n",
    "        cs.add_hyperparameters([lr_init, learning_rate, solver, batch_size, epochs])\n",
    "\n",
    "        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n",
    "        cs.add_conditions([use_lr])\n",
    "\n",
    "        weight_decay = Float(\"weight_decay\", (0.0001, 0.1), log=True)\n",
    "        momentum = Float(\"momentum\", (0, 0.99))\n",
    "        dampening = Float(\"dampening\", (0, 0.99))\n",
    "        cs.add_hyperparameters([weight_decay, momentum, dampening])\n",
    "\n",
    "        weight_decay_condition = InCondition(child=weight_decay, parent=solver, values=[\"sgd\", \"adam\"])\n",
    "        momentum_condition = InCondition(child=momentum, parent=solver, values=[\"sgd\"])\n",
    "        dampening_condition = InCondition(child=dampening, parent=solver, values=[\"sgd\"])\n",
    "        cs.add_conditions([use_lr, weight_decay_condition, momentum_condition, dampening_condition])\n",
    "\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        self.trial_start_time = time.perf_counter()\n",
    "\n",
    "        print(\"Config:\", config)\n",
    "        # print(time.time())\n",
    "        epoch_start_timer = time.perf_counter()\n",
    "        # print(\"okay here?\")\n",
    "        max_epochs = [0]\n",
    "        # print(\"what abt here?\")\n",
    "\n",
    "        X = torch.tensor(train_x.reshape(-1, 3, 32, 32), dtype=torch.float32)\n",
    "        y = torch.tensor(train_y, dtype=torch.long)\n",
    "        valid_x_inFit = torch.tensor(valid_x.reshape(-1, 3, 32, 32), dtype=torch.float32)\n",
    "        valid_y_inFit = torch.tensor(valid_y, dtype=torch.long)\n",
    "\n",
    "\n",
    "        # ### only for resnet:\n",
    "        # X = X.repeat(1, 3, 1, 1)\n",
    "        # valid_x_inFit = valid_x_inFit.repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "        # print(\"so far so good\")\n",
    "        # print(\"Model:\", self.model)\n",
    "        model = self.model\n",
    "        # print(\"break1\")\n",
    "        # print(\"Criterion:\", self.criterion)\n",
    "        criterion = self.criterion\n",
    "\n",
    "        self.lr = config.get('lr_init')\n",
    "        # print(\"lr:\", self.lr)\n",
    "        self.epochs = config.get('epochs')\n",
    "        # print(\"epochs:\", self.epochs)\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        # print(\"batch_size:\", self.batch_size)\n",
    "        # print(\"solver type:\", type(self.sovler))\n",
    "        solver_name = config.get('solver', 'sgd')\n",
    "        # print(\"solver_name:\", solver_name, type(solver_name))\n",
    "        momentum = config.get('momentum', 0.9)\n",
    "        weight_decay = config.get('weight_decay', 0.9)\n",
    "        dampening = config.get('dampening', 0.9)\n",
    "        print(\"is sgd:\", solver_name=='sgd', \"| is adam:\", solver_name=='adam')\n",
    "        # print(self.model.parameters())\n",
    "        if solver_name == 'sgd':\n",
    "            # optimizer_args = {\n",
    "            #     'lr': self.lr,\n",
    "            #     'momentum': config.get('momentum', 0),\n",
    "            #     'weight_decay': config.get('weight_decay', 0),\n",
    "            #     'dampening': config.get('dampening', 0)\n",
    "            # }\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=momentum, weight_decay=weight_decay, dampening=dampening)\n",
    "\n",
    "        elif solver_name == 'adam':\n",
    "            # optimizer_args = {\n",
    "            #     'lr': self.lr,\n",
    "            #     'weight_decay': config.get('weight_decay', 0) \n",
    "            # }\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {solver_name}\")\n",
    "        # print(\"solver type:\", type(self.optimizer))\n",
    "        # print(\"solver:\", self.optimizer)\n",
    "        prev_loss = float(\"inf\")\n",
    "        flag = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                # print(type(X), type(y))\n",
    "                inputs = X[i : i + self.batch_size].to(self.device)\n",
    "                labels = y[i : i + self.batch_size].to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                if inputs.shape[0] <= 2:\n",
    "                    continue\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            cur_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(valid_x_inFit), self.batch_size):\n",
    "                    # get the inputs\n",
    "                    inputs = valid_x_inFit[i : i + self.batch_size].to(self.device)\n",
    "                    labels = valid_y_inFit[i : i + self.batch_size].to(self.device)\n",
    "                    if inputs.shape[0] == 1:\n",
    "                        inputs = torch.cat((inputs, inputs, inputs), dim = 0)\n",
    "                        labels = torch.cat((labels, labels, labels), dim = 0)\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    cur_loss += loss\n",
    "            # early stop if 3 epochs in a row no loss decrease\n",
    "            # print(round(time.time()-epoch_start_timer, 2))\n",
    "            # if time.perf_counter() - epoch_start_timer > budget:\n",
    "            #     print(f\"Stopping training early at epoch {epoch} due to time budget.\")\n",
    "            #     break  # Stop training if budget exceeded\n",
    "            \n",
    "            if cur_loss < prev_loss:\n",
    "                prev_loss = cur_loss\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag += 1\n",
    "                if flag >= 3:\n",
    "                    max_epochs.append(epoch)\n",
    "                    break\n",
    "\n",
    "        # print(np.max(max_epochs))\n",
    "        print(\"evaluating...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(valid_x_inFit.to(self.device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions = predicted.cpu().numpy()\n",
    "        acc = accuracy_score(valid_y, predictions)\n",
    "        print(\"accuracy:\", acc)\n",
    "\n",
    "        self.trial_end_time = time.perf_counter()\n",
    "        valid_probs = self.predict_proba(valid_x_inFit)\n",
    "        valid_kappa = cohen_kappa_score(valid_y, predictions)\n",
    "        valid_ece = get_ece(valid_probs, predictions, valid_y)\n",
    "        with open('smac_results_2h.txt', \"a\") as f:\n",
    "            f.write(f\"Valid Accuracy: {acc}, Valid Kappa: {valid_kappa}, Valid ECE: {valid_ece}, Time: {self.trial_end_time - self.trial_start_time}\\n\")\n",
    "            \n",
    "        return 1-acc\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # X = X.reshape(-1, 3, 32, 32)\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        test_probs = []\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X.to(self.device))\n",
    "            test_prob = nn.Softmax(dim=1)(outputs)\n",
    "            test_probs = test_prob.cpu().numpy()\n",
    "            return test_probs\n",
    "        \n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "    # print(\"\\nfacades[1]:\", facades[1].scenario)\n",
    "    # plt.ylim(0, 0.4)\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing...\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7ff37ba03df0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "Config: Configuration(values={\n",
      "  'batch_size': 569,\n",
      "  'dampening': 0.9688321588104364,\n",
      "  'epochs': 88,\n",
      "  'learning_rate': 'invscaling',\n",
      "  'lr_init': 0.03832216850492789,\n",
      "  'momentum': 0.2619100559835807,\n",
      "  'solver': 'sgd',\n",
      "  'weight_decay': 0.007126995609048582,\n",
      "})\n",
      "\n",
      "is sgd: True | is adam: False\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "'Timed Out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m profiler\u001b[39m.\u001b[39madd_function(main)\n\u001b[1;32m     75\u001b[0m profiler\u001b[39m.\u001b[39menable()\n\u001b[0;32m---> 77\u001b[0m main()\n\u001b[1;32m     79\u001b[0m profiler\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m     80\u001b[0m profiler\u001b[39m.\u001b[39mprint_stats()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:82\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.new_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m new_seconds:\n",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(smac), \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m, smac)\n\u001b[1;32m     46\u001b[0m opt_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 47\u001b[0m incumbent \u001b[39m=\u001b[39m smac\u001b[39m.\u001b[39;49moptimize()\n\u001b[1;32m     48\u001b[0m opt_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mopt execution time: \u001b[39m\u001b[39m{\u001b[39;00mopt_end_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mopt_start_time\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/facade/abstract_facade.py:319\u001b[0m, in \u001b[0;36mAbstractFacade.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata_to_scatter must be None or dict with some elements, but got an empty dict.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     incumbents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer\u001b[39m.\u001b[39;49moptimize(data_to_scatter\u001b[39m=\u001b[39;49mdata_to_scatter)\n\u001b[1;32m    320\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/main/smbo.py:304\u001b[0m, in \u001b[0;36mSMBO.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    300\u001b[0m     trial_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mask()\n\u001b[1;32m    302\u001b[0m     \u001b[39m# We submit the trial to the runner\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# In multi-worker mode, SMAC waits till a new worker is available here\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runner\u001b[39m.\u001b[39;49msubmit_trial(trial_info\u001b[39m=\u001b[39;49mtrial_info, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdask_data_to_scatter)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/smac/runner/dask_runner.py:144\u001b[0m, in \u001b[0;36mDaskParallelRunner.submit_trial\u001b[0;34m(self, trial_info, **dask_data_to_scatter)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTried to execute a job, but no worker was ever available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis likely means that a worker crashed or no workers were properly configured.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    143\u001b[0m \u001b[39m# At this point we can submit the job\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msubmit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_single_worker\u001b[39m.\u001b[39;49mrun_wrapper, trial_info\u001b[39m=\u001b[39;49mtrial_info, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdask_data_to_scatter)\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pending_trials\u001b[39m.\u001b[39mappend(trial)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/client.py:1942\u001b[0m, in \u001b[0;36mClient.submit\u001b[0;34m(self, func, key, workers, resources, retries, priority, fifo_timeout, allow_other_workers, actor, actors, pure, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mif\u001b[39;00m pure:\n\u001b[0;32m-> 1942\u001b[0m         key \u001b[39m=\u001b[39m funcname(func) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m tokenize(func, kwargs, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1943\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m         key \u001b[39m=\u001b[39m funcname(func) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/dask/base.py:998\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    990\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deterministic token\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[39m    >>> tokenize([1, 2, '3'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 998\u001b[0m     hasher \u001b[39m=\u001b[39m _md5(\u001b[39mstr\u001b[39;49m(\u001b[39mtuple\u001b[39;49m(\u001b[39mmap\u001b[39;49m(normalize_token, args)))\u001b[39m.\u001b[39;49mencode())\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m   1000\u001b[0m         hasher\u001b[39m.\u001b[39mupdate(\u001b[39mstr\u001b[39m(normalize_token(kwargs))\u001b[39m.\u001b[39mencode())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/dask/base.py:986\u001b[0m, in \u001b[0;36m_md5\u001b[0;34m(x, _hashlib_md5)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_md5\u001b[39m(x: ReadableBuffer, _hashlib_md5: _HashFactory \u001b[39m=\u001b[39m hashlib\u001b[39m.\u001b[39mmd5) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m hashlib\u001b[39m.\u001b[39m_Hash:\n\u001b[0;32m--> 986\u001b[0m     \u001b[39mreturn\u001b[39;00m _hashlib_md5(x, usedforsecurity\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:69\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[0;32m---> 69\u001b[0m     _raise_exception(timeout_exception, exception_message)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:45\u001b[0m, in \u001b[0;36m_raise_exception\u001b[0;34m(exception, exception_message)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" This function checks if a exception message is given.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39mIf there is no exception message, the default behaviour is maintained.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mIf there is an exception message, the message is passed to the exception with the 'value' keyword.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m exception_message \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m exception()\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m exception(exception_message)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: 'Timed Out'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Configuration(values={\n",
      "  'batch_size': 737,\n",
      "  'dampening': 0.7911669785745563,\n",
      "  'epochs': 69,\n",
      "  'learning_rate': 'constant',\n",
      "  'lr_init': 0.011423254155608374,\n",
      "  'momentum': 0.7664913525398744,\n",
      "  'solver': 'sgd',\n",
      "  'weight_decay': 0.006859416411328701,\n",
      "})\n",
      "\n",
      "is sgd: True | is adam: False\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "@timeout(60)\n",
    "def main():\n",
    "\n",
    "    \n",
    "    CNN32 = CNN32Wrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "        # Define our environment variables\n",
    "        \n",
    "        scenario = Scenario(\n",
    "            CNN32.configspace,\n",
    "            walltime_limit=60,  # After 3600 seconds, we stop the hyperparameter optimization\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_1min_CNN\"),\n",
    "            n_trials=10000,  # Evaluate max 500 different trials\n",
    "            min_budget=100,  # Train the MLP using a hyperparameter configuration for at least 5 epochs\n",
    "            # max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs\n",
    "            max_budget=1000,  # only for CNN32_5layers, Resnet18\n",
    "            n_workers=4,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # We want to run five random configurations before starting the optimization.\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        \n",
    "        # Create our intensifier\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "        \n",
    "        # print(\"Holy shit I'm gonna fiiiiiiiiiiiiiit\")\n",
    "        # Create our SMAC object and pass the scenario and the train method\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            CNN32.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "        # print(\"I fitted!!!!!!!!!!!!!!:\", smac)\n",
    "\n",
    "        # Let's optimize\n",
    "        print(\"optimizing...\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        opt_start_time = time.time()\n",
    "        incumbent = smac.optimize()\n",
    "        opt_end_time = time.time()\n",
    "        print(f\"opt execution time: {opt_end_time - opt_start_time} seconds\")\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        # break\n",
    "        # Get cost of default configuration\n",
    "        default_cost = smac.validate(CNN32.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "\n",
    "        # Let's calculate the cost of the incumbent\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "        print(\"appending...\")\n",
    "        facades.append(smac)\n",
    "        for attr in dir(smac):\n",
    "            if not attr.startswith('__'):\n",
    "                print(attr, getattr(smac, attr))\n",
    "    print(\"attempting to plot\")\n",
    "    # Let's plot it\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = \"CNN32_5l_BO_results_2h.txt\"\n",
    "    plot_trajectory(facades)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('smac_results_2h.txt', \"w\") as f:\n",
    "        pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN min 5:\n",
      "Config ID: 3, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.7219343632501507, 'colsample_bynode': 0.9242411005474558, 'colsample_bytree': 0.6976311927657526, 'gamma': 0.8493578609931441, 'learning_rate': 0.14382901505335025, 'max_depth': 20, 'min_child_weight': 5, 'n_estimators': 779, 'subsample': 0.8488155979636325}\n",
      "\n",
      "Config ID: 5, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.5965583595372332, 'colsample_bynode': 0.5684090631780443, 'colsample_bytree': 0.34972524073852085, 'gamma': 0.8830109334221372, 'learning_rate': 0.04429958350199063, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 850, 'subsample': 0.8333833577228338}\n",
      "\n",
      "Config ID: 1, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2937993192475016, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "\n",
      "Config ID: 4, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.6814182280978278, 'colsample_bynode': 0.9745639323507205, 'colsample_bytree': 0.9479176468048627, 'gamma': 0.8003410758548655, 'learning_rate': 0.23635346112307207, 'max_depth': 12, 'min_child_weight': 6, 'n_estimators': 1139, 'subsample': 0.5301127358146349}\n",
      "\n",
      "Config ID: 2, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.8006325564606935, 'colsample_bynode': 0.6063110478838847, 'colsample_bytree': 0.6702264438270331, 'gamma': 0.11819655769629316, 'learning_rate': 0.24175598362284983, 'max_depth': 4, 'min_child_weight': 8, 'n_estimators': 773, 'subsample': 0.7185159768996707}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runhistory_path = \"smac_hyperband_output_budget_10mins_XGBT/9aae8f279166de5bc6c9f468216e8aca/0/runhistory.json\"\n",
    "\n",
    "with open(runhistory_path, \"r\") as file:\n",
    "    runhistory_data_CNN = json.load(file)\n",
    "\n",
    "configs_costs_CNN = []\n",
    "for entry in runhistory_data_CNN[\"data\"]:\n",
    "    # print(entry)\n",
    "    config_id = entry[0]  \n",
    "    cost = entry[4]  \n",
    "    configs_costs_CNN.append((entry[0], entry[4]))\n",
    "    configs_costs_CNN = list(set(configs_costs_CNN))\n",
    "\n",
    "    # print(f\"Config ID: {config_id}, Cost: {cost}\")\n",
    "    if config_id == 533:# or config_id == 21:\n",
    "        params_CNN = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "        # print(entry)\n",
    "        print(f\"CNN config ID: {config_id}, Cost: {cost}, Parameters: {params_CNN}\\n\")\n",
    "\n",
    "\n",
    "min_cost_config_CNN = []\n",
    "min_cost_config_CNN = sorted(configs_costs_CNN, key=lambda x: x[1])[:5]\n",
    "print(\"CNN min 5:\")\n",
    "for config_id, cost in min_cost_config_CNN:\n",
    "    \n",
    "    params = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "    print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "file.close()\n",
    "# min_cost_config = min(configs_costs, key=lambda x: x[1])\n",
    "# min_cost_config_id, min_cost = min_cost_config\n",
    "\n",
    "# print(f\"min id:{min_cost_config_id}, min cost:{min_cost}\")\n",
    "\n",
    "# min_cost_params = runhistory_data[\"configs\"][str(min_cost_config_id)]\n",
    "# print(min_cost_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Resnet min 5:\n",
      "Config ID: 2, Cost: 0.45306666666666673, Parameters: {'colsample_bylevel': 0.8006325564606935, 'colsample_bynode': 0.6063110478838847, 'colsample_bytree': 0.6702264438270331, 'gamma': 0.11819655769629316, 'learning_rate': 0.24175598362284983, 'max_depth': 4, 'min_child_weight': 8, 'n_estimators': 773, 'subsample': 0.7185159768996707}\n",
      "\n",
      "Config ID: 3, Cost: 0.5013, Parameters: {'colsample_bylevel': 0.7219343632501507, 'colsample_bynode': 0.9242411005474558, 'colsample_bytree': 0.6976311927657526, 'gamma': 0.8493578609931441, 'learning_rate': 0.14382901505335025, 'max_depth': 20, 'min_child_weight': 5, 'n_estimators': 779, 'subsample': 0.8488155979636325}\n",
      "\n",
      "Config ID: 1, Cost: 0.5050333333333333, Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2937993192475016, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runhistory_path_RES = \"smac_hyperband_output_budget_30mins_XGBT/56e93b9169af80edb12371783e3d29b7/0/runhistory.json\"\n",
    "\n",
    "with open(runhistory_path_RES, \"r\") as file:\n",
    "    runhistory_data_RES = json.load(file)\n",
    "\n",
    "configs_costs_RES = []\n",
    "for entry in runhistory_data_RES[\"data\"]:\n",
    "    config_id = entry[0]  \n",
    "    cost = entry[4]\n",
    "    configs_costs_RES.append((entry[0], entry[4]))\n",
    "    configs_costs_RES = list(set(configs_costs_RES))\n",
    "    # if config_id == 11:\n",
    "    #     params = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    #     # print(entry)\n",
    "    #     print(f\"Resnet Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "    # if config_id == 11:# or config_id == 21:\n",
    "    #     params_RES = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    #     # print(entry)\n",
    "    #     print(f\"RES config ID: {config_id}, Cost: {cost}, Parameters: {params_RES}\\n\")\n",
    "    #     RES_11 = params_RES\n",
    "\n",
    "min_cost_config_RES = []\n",
    "min_cost_config_RES = sorted(configs_costs_RES, key=lambda x: x[1])[:5]\n",
    "print(type(min_cost_config_RES))\n",
    "print(\"Resnet min 5:\")\n",
    "for config_id, cost in min_cost_config_RES:\n",
    "    \n",
    "    params = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2d6f2cc3753f9b3dea2c319b7106c95a5cf0ce448613bf856a4723ad4bfccc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
