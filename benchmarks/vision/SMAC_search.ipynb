{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAC Search for Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import *\n",
    "import warnings\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import librosa\n",
    "import time\n",
    "import re\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ConfigSpace import (\n",
    "    Categorical,\n",
    "    Configuration,\n",
    "    ConfigurationSpace,\n",
    "    EqualsCondition,\n",
    "    Float,\n",
    "    InCondition,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt.callbacks import DeadlineStopper\n",
    "\n",
    "from smac import MultiFidelityFacade as MFFacade\n",
    "from smac import Scenario\n",
    "from smac.facade import AbstractFacade\n",
    "from smac.intensifier.hyperband import Hyperband\n",
    "from smac.intensifier.successive_halving import SuccessiveHalving\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as trans\n",
    "import re\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def combinations_45(iterable, r):\n",
    "    \"\"\"Extracts 45 combinations from given list\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    count = 0\n",
    "    while count < 44:\n",
    "        count += 1\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i + 1, r):\n",
    "            indices[j] = indices[j - 1] + 1\n",
    "        yield tuple(pool[i] for i in indices)\n",
    "\n",
    "torch.multiprocessing.freeze_support()\n",
    "n_classes = 3 # number of classes\n",
    "samples_space = np.geomspace(10, 9000, num=8, dtype=int)\n",
    "nums = list(range(10))\n",
    "random.shuffle(nums)\n",
    "classes_space = list(combinations_45(nums, n_classes))\n",
    "normalize = lambda x: x / 255.0\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root=\"./\", train=True, download=True, transform=None\n",
    ")\n",
    "cifar_train_images = normalize(cifar_trainset.data)\n",
    "cifar_train_labels = np.array(cifar_trainset.targets)\n",
    "\n",
    "cifar_testset = datasets.CIFAR10(\n",
    "    root=\"./\", train=False, download=True, transform=None\n",
    ")\n",
    "cifar_test_images = normalize(cifar_testset.data)\n",
    "cifar_test_labels = np.array(cifar_testset.targets)\n",
    "\n",
    "# cifar_train_images = cifar_train_images.reshape(-1, 32 * 32 * 3)\n",
    "# cifar_test_images = cifar_test_images.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "images = np.concatenate((cifar_train_images, cifar_test_images))\n",
    "labels = np.concatenate((cifar_train_labels, cifar_test_labels))\n",
    "\n",
    "indices = np.arange(images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_images, test_valid_images, train_labels, test_valid_labels = train_test_split(\n",
    "    images, labels, test_size=0.5, random_state=317\n",
    ")\n",
    "\n",
    "test_images, valid_images, test_labels, valid_labels = train_test_split(\n",
    "    test_valid_images, test_valid_labels, test_size=0.5, random_state=317\n",
    ")\n",
    "\n",
    "train_x = train_images\n",
    "train_y = train_labels\n",
    "test_x = test_images\n",
    "test_y = test_labels\n",
    "valid_x = valid_images\n",
    "valid_y = valid_labels\n",
    "test_valid_x = test_valid_images\n",
    "test_valid_y = test_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 32, 32, 3) (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN32Filter(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a simple CNN arhcitecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=10, stride=2)\n",
    "        self.fc1 = nn.Linear(144 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, 144 * 32)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleCNN32Filter2Layers(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a simple CNN arhcitecture with 2 layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.fc1 = nn.Linear(12 * 12 * 32, 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(b, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleCNN32Filter5Layers(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a simple CNN arhcitecture with 5 layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(8192, 200)\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "        self.maxpool = nn.MaxPool2d((2, 2))\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = F.relu(self.bn(self.conv1(x)))\n",
    "        x = F.relu(self.bn(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = F.relu(self.bn2(self.conv4(x)))\n",
    "        x = F.relu(self.bn3(self.conv5(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(b, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn32 = SimpleCNN32Filter(num_classes=18)\n",
    "cnn32_2l = SimpleCNN32Filter2Layers(num_classes=18)\n",
    "cnn32_5l = SimpleCNN32Filter5Layers(num_classes=18)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBT tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='multi:softprob', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.seed= seed\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.colsample_bynode = colsample_bynode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.objective = objective\n",
    "        self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "        gamma = Float(\"gamma\", (0.1, 1), default=0.1)\n",
    "        subsample = Float(\"subsample\", (0.5, 1), default=0.8)\n",
    "        colsample_bytree = Float(\"colsample_bytree\", (0.3, 1), default=0.6)\n",
    "        colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1), default=0.6)\n",
    "        colsample_bynode = Float(\"colsample_bynode\", (0.3, 1), default=0.6)\n",
    "        learning_rate = Float(\"learning_rate\", (0.01, 0.3), default=0.1)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        self.model.set_params(**config)\n",
    "        X = train_x.reshape(-1, 32 * 32 * 3)\n",
    "        y = train_y\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        scores = cross_val_score(self.model, X, y, cv=5)\n",
    "        return 1 - np.mean(scores)\n",
    "    \n",
    "\n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fcba29d3f70>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    }
   ],
   "source": [
    "@timeout(600)\n",
    "def main():\n",
    "    GBT = XGBWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            GBT.configspace,\n",
    "            walltime_limit = 600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_XGBT\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=4,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            GBT.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = None\n",
    "    plot_trajectory(facades, BO_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features   \n",
    "        self.criterion = \"gini\"\n",
    "        self.max_samples = max_samples\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "        min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "        criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "        max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "        max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        if config['max_features'] == 'None':\n",
    "            config['max_features'] = None\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X = X.reshape(-1, 32 * 32 * 3)\n",
    "        \n",
    "        scores = cross_val_score(self.model, X, y, cv=5)\n",
    "        return 1 - np.mean(scores)\n",
    "    \n",
    "\n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeout(14400)\n",
    "def main():\n",
    "    RF = RFWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            RF.configspace,\n",
    "            walltime_limit = 14400,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_4hr_RF_15classes\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            RF.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = None\n",
    "    plot_trajectory(facades, BO_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DN tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
    "# train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "# test_tensor = torch.tensor(test_images, dtype=torch.float32)\n",
    "# test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "# valid_tensor = torch.tensor(valid_images, dtype=torch.float32)\n",
    "# valid_labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(train_tensor, train_labels_tensor)\n",
    "# test_dataset = torch.utils.data.TensorDataset(test_tensor, test_labels_tensor)\n",
    "# valid_dataset = torch.utils.data.TensorDataset(valid_tensor, valid_labels_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=4)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=4)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNN \n",
    "data_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN32Wrapper(BaseEstimator):\n",
    "    def __init__(self, criterion = nn.CrossEntropyLoss()):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = cnn32.to(self.device)\n",
    "        self.criterion = criterion\n",
    "        self.lr = None\n",
    "        self.batch_size = None\n",
    "        self.epochs = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    @property\n",
    "    def configspace(self)-> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        lr_init = Float(\"lr_init\", (0.001, 0.1), log=True)\n",
    "        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n",
    "        solver = Categorical(\"solver\", [\"sgd\", \"adam\"])\n",
    "        batch = Integer(\"batch\", (16, 1024))\n",
    "        epochs = Integer(\"epochs\", (30, 120))\n",
    "        cs.add_hyperparameters([lr_init, learning_rate, solver, batch, epochs])\n",
    "\n",
    "        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n",
    "        cs.add_conditions([use_lr])\n",
    "\n",
    "        weight_decay = Float(\"weight_decay\", (0.0001, 0.1), log=True)\n",
    "        momentum = Float(\"momentum\", (0, 0.99))\n",
    "        dampening = Float(\"dampening\", (0, 0.99))\n",
    "        cs.add_hyperparameters([weight_decay, momentum, dampening])\n",
    "\n",
    "        weight_decay_condition = InCondition(child=weight_decay, parent=solver, values=[\"sgd\", \"adam\"])\n",
    "        momentum_condition = InCondition(child=momentum, parent=solver, values=[\"sgd\"])\n",
    "        dampening_condition = InCondition(child=dampening, parent=solver, values=[\"sgd\"])\n",
    "        cs.add_conditions([use_lr, weight_decay_condition, momentum_condition, dampening_condition])\n",
    "\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        self.trial_start_time = time.perf_counter()\n",
    "\n",
    "        print(\"Config:\", config)\n",
    "        # print(time.time())\n",
    "        epoch_start_timer = time.perf_counter()\n",
    "        # print(\"okay here?\")\n",
    "        max_epochs = [0]\n",
    "        # print(\"what abt here?\")\n",
    "\n",
    "        X = train_x.reshape(-1, 3, 32, 32)\n",
    "        y = train_y\n",
    "        valid_x_inFit = valid_x.reshape(-1, 3, 32, 32)\n",
    "\n",
    "        # ### only for resnet:\n",
    "        # X = X.repeat(1, 3, 1, 1)\n",
    "        # valid_x_inFit = valid_x_inFit.repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "        # print(\"so far so good\")\n",
    "        # print(\"Model:\", self.model)\n",
    "        model = self.model\n",
    "        # print(\"break1\")\n",
    "        # print(\"Criterion:\", self.criterion)\n",
    "        criterion = self.criterion\n",
    "\n",
    "        self.lr = config.get('lr_init')\n",
    "        # print(\"lr:\", self.lr)\n",
    "        self.epochs = config.get('epochs')\n",
    "        # print(\"epochs:\", self.epochs)\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        # print(\"batch_size:\", self.batch_size)\n",
    "        # print(\"solver type:\", type(self.sovler))\n",
    "        solver_name = config.get('solver', 'sgd')\n",
    "        # print(\"solver_name:\", solver_name, type(solver_name))\n",
    "        momentum = config.get('momentum', 0.9)\n",
    "        weight_decay = config.get('weight_decay', 0.9)\n",
    "        dampening = config.get('dampening', 0.9)\n",
    "        print(\"is sgd:\", solver_name=='sgd', \"| is adam:\", solver_name=='adam')\n",
    "        # print(self.model.parameters())\n",
    "        if solver_name == 'sgd':\n",
    "            # optimizer_args = {\n",
    "            #     'lr': self.lr,\n",
    "            #     'momentum': config.get('momentum', 0),\n",
    "            #     'weight_decay': config.get('weight_decay', 0),\n",
    "            #     'dampening': config.get('dampening', 0)\n",
    "            # }\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=momentum, weight_decay=weight_decay, dampening=dampening)\n",
    "\n",
    "        elif solver_name == 'adam':\n",
    "            # optimizer_args = {\n",
    "            #     'lr': self.lr,\n",
    "            #     'weight_decay': config.get('weight_decay', 0) \n",
    "            # }\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=weight_decay)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {solver_name}\")\n",
    "        # print(\"solver type:\", type(self.optimizer))\n",
    "        # print(\"solver:\", self.optimizer)\n",
    "        prev_loss = float(\"inf\")\n",
    "        flag = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train()\n",
    "            for i in range(0, len(X), self.batch_size):\n",
    "                inputs = X[i : i + self.batch_size].to(self.device)\n",
    "                labels = y[i : i + self.batch_size].to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                if inputs.shape[0] <= 2:\n",
    "                    continue\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            cur_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(valid_x_inFit), self.batch_size):\n",
    "                    # get the inputs\n",
    "                    inputs = valid_x_inFit[i : i + self.batch_size].to(self.device)\n",
    "                    labels = valid_y[i : i + self.batch_size].to(self.device)\n",
    "                    if inputs.shape[0] == 1:\n",
    "                        inputs = torch.cat((inputs, inputs, inputs), dim = 0)\n",
    "                        labels = torch.cat((labels, labels, labels), dim = 0)\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    cur_loss += loss\n",
    "            # early stop if 3 epochs in a row no loss decrease\n",
    "            # print(round(time.time()-epoch_start_timer, 2))\n",
    "            # if time.perf_counter() - epoch_start_timer > budget:\n",
    "            #     print(f\"Stopping training early at epoch {epoch} due to time budget.\")\n",
    "            #     break  # Stop training if budget exceeded\n",
    "            \n",
    "            if cur_loss < prev_loss:\n",
    "                prev_loss = cur_loss\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag += 1\n",
    "                if flag >= 3:\n",
    "                    max_epochs.append(epoch)\n",
    "                    break\n",
    "\n",
    "        # print(np.max(max_epochs))\n",
    "        print(\"evaluating...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(valid_x_inFit.to(self.device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions = predicted.cpu().numpy()\n",
    "        acc = accuracy_score(valid_y, predictions)\n",
    "        print(\"accuracy:\", acc)\n",
    "\n",
    "        self.trial_end_time = time.perf_counter()\n",
    "        valid_probs = self.predict_proba(valid_x_inFit)\n",
    "        valid_kappa = cohen_kappa_score(valid_y, predictions)\n",
    "        valid_ece = get_ece(valid_probs, predictions, valid_y)\n",
    "        with open('smac_results_2h.txt', \"a\") as f:\n",
    "            f.write(f\"Valid Accuracy: {acc}, Valid Kappa: {valid_kappa}, Valid ECE: {valid_ece}, Time: {self.trial_end_time - self.trial_start_time}\\n\")\n",
    "            \n",
    "        return 1-acc\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # X = X.reshape(-1, 3, 32, 32)\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "        test_probs = []\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X.to(self.device))\n",
    "            test_prob = nn.Softmax(dim=1)(outputs)\n",
    "            test_probs = test_prob.cpu().numpy()\n",
    "            return test_probs\n",
    "        \n",
    "def plot_trajectory(facades: list[AbstractFacade], BO_file = None) -> None:\n",
    "    \"\"\"Plots the trajectory (incumbents) of the optimization process.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(\"Trajectory\")\n",
    "    plt.xlabel(\"Wallclock time [s]\")\n",
    "    # print(len(facades))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    print(\"\\nfacades[0]:\", facades[0].scenario)\n",
    "    # print(\"\\nfacades[1]:\", facades[1].scenario)\n",
    "    # plt.ylim(0, 0.4)\n",
    "\n",
    "    for facade in facades:\n",
    "        X, Y = [], []\n",
    "        for item in facade.intensifier.trajectory:\n",
    "            # Single-objective optimization\n",
    "            assert len(item.config_ids) == 1\n",
    "            assert len(item.costs) == 1\n",
    "            # print(1 - round(item.costs[0], 3))\n",
    "            y = item.costs[0]\n",
    "            x = item.walltime\n",
    "            print(type(item.costs[0]), item.costs[0])\n",
    "            X.append(x)\n",
    "            Y.append(1-y)\n",
    "\n",
    "        plt.plot(X, Y, label=facade.intensifier.__class__.__name__)\n",
    "        plt.scatter(X, Y, marker=\"o\")\n",
    "\n",
    "    if BO_file:\n",
    "        with open(BO_file, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "        bo_accuracy, bo_kappa, bo_ece, bo_time = [], [], [], []\n",
    "\n",
    "        # Parse the data\n",
    "        for line in data:\n",
    "            # Using regular expression to find all the floating point numbers in each line\n",
    "            if \"Time: \" in line:\n",
    "                numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
    "                if numbers:\n",
    "                    bo_accuracy.append(float(numbers[0]))\n",
    "                    bo_kappa.append(float(numbers[1]))\n",
    "                    bo_ece.append(float(numbers[2]))\n",
    "                    bo_time.append(float(numbers[3]))\n",
    "\n",
    "        max_bo_accs = np.maximum.accumulate(bo_accuracy)\n",
    "        bo_cum_time = np.cumsum(bo_time)\n",
    "        plt.plot(bo_cum_time, max_bo_accs, label='Max BO Accuracies', marker='x')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "# @timeout(1800)\n",
    "def main():\n",
    "\n",
    "    \n",
    "    CNN32 = CNN32Wrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "        # Define our environment variables\n",
    "        \n",
    "        scenario = Scenario(\n",
    "            CNN32.configspace,\n",
    "            walltime_limit=1800,  # After 3600 seconds, we stop the hyperparameter optimization\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_30mins_CNN\"),\n",
    "            n_trials=10000,  # Evaluate max 500 different trials\n",
    "            min_budget=100,  # Train the MLP using a hyperparameter configuration for at least 5 epochs\n",
    "            # max_budget=25,  # Train the MLP using a hyperparameter configuration for at most 25 epochs\n",
    "            max_budget=1000,  # only for CNN32_5layers, Resnet18\n",
    "            n_workers=8,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # We want to run five random configurations before starting the optimization.\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        \n",
    "        # Create our intensifier\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "        \n",
    "        # print(\"Holy shit I'm gonna fiiiiiiiiiiiiiit\")\n",
    "        # Create our SMAC object and pass the scenario and the train method\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            CNN32.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "        # print(\"I fitted!!!!!!!!!!!!!!:\", smac)\n",
    "\n",
    "        # Let's optimize\n",
    "        print(\"optimizing...\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        opt_start_time = time.time()\n",
    "        incumbent = smac.optimize()\n",
    "        opt_end_time = time.time()\n",
    "        print(f\"opt execution time: {opt_end_time - opt_start_time} seconds\")\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        # break\n",
    "        # Get cost of default configuration\n",
    "        default_cost = smac.validate(CNN32.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "\n",
    "        # Let's calculate the cost of the incumbent\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "        print(\"appending...\")\n",
    "        facades.append(smac)\n",
    "        for attr in dir(smac):\n",
    "            if not attr.startswith('__'):\n",
    "                print(attr, getattr(smac, attr))\n",
    "    print(\"attempting to plot\")\n",
    "    # Let's plot it\n",
    "    print(\"facades:\", facades)\n",
    "    BO_file = \"CNN32_5l_BO_results_2h.txt\"\n",
    "    plot_trajectory(facades)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('smac_results_2h.txt', \"w\") as f:\n",
    "        pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN min 5:\n",
      "Config ID: 8, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.39672806594402965, 'colsample_bynode': 0.43760765317603745, 'colsample_bytree': 0.5581076194626748, 'gamma': 0.8388939068631416, 'learning_rate': 0.03815936997998777, 'max_depth': 18, 'min_child_weight': 1, 'n_estimators': 1175, 'subsample': 0.7343256008238508}\n",
      "\n",
      "Config ID: 3, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.7219343632501507, 'colsample_bynode': 0.9242411005474558, 'colsample_bytree': 0.6976311927657526, 'gamma': 0.8493578609931441, 'learning_rate': 0.14382901505335025, 'max_depth': 20, 'min_child_weight': 5, 'n_estimators': 779, 'subsample': 0.8488155979636325}\n",
      "\n",
      "Config ID: 5, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.5965583595372332, 'colsample_bynode': 0.5684090631780443, 'colsample_bytree': 0.34972524073852085, 'gamma': 0.8830109334221372, 'learning_rate': 0.04429958350199063, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 850, 'subsample': 0.8333833577228338}\n",
      "\n",
      "Config ID: 6, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.7694465087327116, 'colsample_bynode': 0.4472677927516886, 'colsample_bytree': 0.3902484083583973, 'gamma': 0.38388551583176544, 'learning_rate': 0.11547612357336054, 'max_depth': 13, 'min_child_weight': 5, 'n_estimators': 1188, 'subsample': 0.551022405374014}\n",
      "\n",
      "Config ID: 1, Cost: 2147483647.0, Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2937993192475016, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runhistory_path = \"smac_hyperband_output_budget_10mins_XGBT/6b8645f89bf2fbd8feff497a8de9c333/0/runhistory.json\"\n",
    "\n",
    "with open(runhistory_path, \"r\") as file:\n",
    "    runhistory_data_CNN = json.load(file)\n",
    "\n",
    "configs_costs_CNN = []\n",
    "for entry in runhistory_data_CNN[\"data\"]:\n",
    "    # print(entry)\n",
    "    config_id = entry[0]  \n",
    "    cost = entry[4]  \n",
    "    configs_costs_CNN.append((entry[0], entry[4]))\n",
    "    configs_costs_CNN = list(set(configs_costs_CNN))\n",
    "\n",
    "    # print(f\"Config ID: {config_id}, Cost: {cost}\")\n",
    "    if config_id == 533:# or config_id == 21:\n",
    "        params_CNN = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "        # print(entry)\n",
    "        print(f\"CNN config ID: {config_id}, Cost: {cost}, Parameters: {params_CNN}\\n\")\n",
    "\n",
    "\n",
    "min_cost_config_CNN = []\n",
    "min_cost_config_CNN = sorted(configs_costs_CNN, key=lambda x: x[1])[:5]\n",
    "print(\"CNN min 5:\")\n",
    "for config_id, cost in min_cost_config_CNN:\n",
    "    \n",
    "    params = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "    print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "file.close()\n",
    "# min_cost_config = min(configs_costs, key=lambda x: x[1])\n",
    "# min_cost_config_id, min_cost = min_cost_config\n",
    "\n",
    "# print(f\"min id:{min_cost_config_id}, min cost:{min_cost}\")\n",
    "\n",
    "# min_cost_params = runhistory_data[\"configs\"][str(min_cost_config_id)]\n",
    "# print(min_cost_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runhistory_path_RES = \"smac_hyperband_output_budget_4hr_XGBT_15classes/41485f39f3768c7023139bcb1db7a8bf/0/runhistory.json\"\n",
    "\n",
    "with open(runhistory_path_RES, \"r\") as file:\n",
    "    runhistory_data_RES = json.load(file)\n",
    "\n",
    "configs_costs_RES = []\n",
    "for entry in runhistory_data_RES[\"data\"]:\n",
    "    config_id = entry[0]  \n",
    "    cost = entry[4]\n",
    "    configs_costs_RES.append((entry[0], entry[4]))\n",
    "    configs_costs_RES = list(set(configs_costs_RES))\n",
    "    # if config_id == 11:\n",
    "    #     params = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    #     # print(entry)\n",
    "    #     print(f\"Resnet Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "    # if config_id == 11:# or config_id == 21:\n",
    "    #     params_RES = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    #     # print(entry)\n",
    "    #     print(f\"RES config ID: {config_id}, Cost: {cost}, Parameters: {params_RES}\\n\")\n",
    "    #     RES_11 = params_RES\n",
    "\n",
    "min_cost_config_RES = []\n",
    "min_cost_config_RES = sorted(configs_costs_RES, key=lambda x: x[1])[:5]\n",
    "print(type(min_cost_config_RES))\n",
    "print(\"Resnet min 5:\")\n",
    "for config_id, cost in min_cost_config_RES:\n",
    "    \n",
    "    params = runhistory_data_RES[\"configs\"][str(config_id)]\n",
    "    print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2d6f2cc3753f9b3dea2c319b7106c95a5cf0ce448613bf856a4723ad4bfccc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
