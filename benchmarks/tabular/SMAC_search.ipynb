{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import *\n",
    "import warnings\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import librosa\n",
    "import time\n",
    "import re\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ConfigSpace import (\n",
    "    Categorical,\n",
    "    Configuration,\n",
    "    ConfigurationSpace,\n",
    "    EqualsCondition,\n",
    "    Float,\n",
    "    InCondition,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt.callbacks import DeadlineStopper\n",
    "\n",
    "from smac import MultiFidelityFacade as MFFacade\n",
    "from smac import Scenario\n",
    "from smac.facade import AbstractFacade\n",
    "from smac.intensifier.hyperband import Hyperband\n",
    "from smac.intensifier.successive_halving import SuccessiveHalving\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as trans\n",
    "import re\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_combination = [20, 100, 180, 260, 340, 400]\n",
    "dataset_indices_max = 7\n",
    "max_shape_to_run = 10000\n",
    "alpha_range_nn = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.8, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Dataset:  0\n",
      "X shape:  (10000, 8)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (10000, 8)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  1\n",
      "X shape:  (7608, 23)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (7608, 23)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  2\n",
      "X shape:  (10000, 54)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (10000, 54)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  3\n",
      "X shape:  (10000, 31)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (10000, 31)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  4\n",
      "X shape:  (10000, 21)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (10000, 21)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  5\n",
      "X shape:  (10000, 32)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (10000, 32)\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  6\n",
      "X shape:  (4966, 11)\n",
      "No string columns to encode\n",
      "X_encoded shape:  (4966, 11)\n",
      "X scalered\n"
     ]
    }
   ],
   "source": [
    "dataset_indices = list(range(dataset_indices_max))\n",
    "dict_data_indices = {dataset_ind: {} for dataset_ind in dataset_indices}\n",
    "encode_cnt = 0\n",
    "\n",
    "X_data_list = []\n",
    "y_data_list = []\n",
    "dataset_names = []\n",
    "def import_datasets():\n",
    "    SUITE_ID = [334]\n",
    "    for i in SUITE_ID:\n",
    "        benchmark_suite = openml.study.get_suite(i)\n",
    "        for task_id in benchmark_suite.tasks:  # iterate over all tasks\n",
    "            task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "            dataset = task.get_dataset()\n",
    "            X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "                dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "            )   \n",
    "\n",
    "            # ### Covert labels to numerical values\n",
    "            # le = LabelEncoder()\n",
    "            # y_encoded = le.fit_transform(y)\n",
    "            # y = pd.DataFrame(y_encoded)\n",
    "\n",
    "            X_data_list.append(X)\n",
    "            y_data_list.append(y)\n",
    "            dataset_names.append(dataset.name)\n",
    "\n",
    "            # print(\" \")\n",
    "            # print(\" SUITE_ID:\", i)\n",
    "            # print(\"X_data_list length:\", len(X_data_list))\n",
    "            # print(\" \")\n",
    "    \n",
    "import_datasets()\n",
    "\n",
    "train_x_list = X_data_list.copy()\n",
    "train_y_list = y_data_list.copy()\n",
    "\n",
    "for dataset_index, dataset in enumerate(dataset_indices):\n",
    "    print(\"\\n\\nCurrent Dataset: \", dataset)\n",
    "\n",
    "    X = X_data_list[dataset]\n",
    "    y = y_data_list[dataset]\n",
    "\n",
    "    if X.shape[0] > max_shape_to_run:\n",
    "        X, y = sample_large_datasets(X, y)\n",
    "    \n",
    "    np.random.seed(dataset_index)\n",
    "    dict_data_indices = find_indices_train_val_test(\n",
    "        X.shape[0], dict_data_indices=dict_data_indices, dataset_ind=dataset_index\n",
    "    )\n",
    "    train_indices = dict_data_indices[dataset_index][\"train\"]\n",
    "    val_indices = dict_data_indices[dataset_index][\"val\"]\n",
    "\n",
    "    ### Covert labels to numerical values\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    # y = pd.DataFrame(y_encoded)\n",
    "    y = y_encoded\n",
    "\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    # print(X.dtypes)\n",
    "\n",
    "    ### Convert categories features to numerical features\n",
    "    print(\"X shape: \", X.shape)\n",
    "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    if len(categorical_columns) > 0:\n",
    "        X_encoded_strings = encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "        X = np.hstack((X[numeric_columns].values, X_encoded_strings))\n",
    "        print(\"Encoded\", len(categorical_columns), \" columns\")\n",
    "        encode_cnt += 1\n",
    "    else:\n",
    "        print(\"No string columns to encode\")\n",
    "    \n",
    "    print(\"X_encoded shape: \", X.shape)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    # X = pd.DataFrame(X)\n",
    "    train_x_list[dataset] = X\n",
    "    train_y_list[dataset] = y\n",
    "    print(\"X scalered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_list[0]\n",
    "train_y = train_y_list[0]\n",
    "num_class = len(np.unique(train_y))\n",
    "print(np.unique(train_y))\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(train_y.flatten().shape)\n",
    "\n",
    "# categorical_columns = train_x.select_dtypes(include=['category']).columns\n",
    "\n",
    "# if not categorical_columns.empty:\n",
    "#     print(\"There are categorical columns:\", categorical_columns)\n",
    "# else:\n",
    "#     print(\"No categorical columns found.\")\n",
    "\n",
    "# print(train_x.head(5))\n",
    "# print(train_x['day'].cat.categories.is_numeric())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, random_state=317)\n",
    "XGBT = xgb.XGBClassifier(n_estimators=100, random_state=317)\n",
    "TabNet = TabNetClassifier(n_d=64, n_a=64, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='binary:logistic', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.seed= seed\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.colsample_bynode = colsample_bynode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.objective = objective\n",
    "        # self.num_class = num_class\n",
    "        self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2, 21), default=2)\n",
    "        min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "        gamma = Float(\"gamma\", (0.1, 1.0), default=0.1)\n",
    "        subsample = Float(\"subsample\", (0.5, 1.0), default=0.8)\n",
    "        colsample_bytree = Float(\"colsample_bytree\", (0.3, 1.0), default=0.6)\n",
    "        colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1.0), default=0.6)\n",
    "        colsample_bynode = Float(\"colsample_bynode\", (0.3, 1.0), default=0.6)\n",
    "        learning_rate = Float(\"learning_rate\", (0.001, 0.3), default=0.1)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        # print(\"X shape: \", X.shape)\n",
    "        # print(\"y shape: \", y.shape)\n",
    "        self.model.fit(X, y)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = accuracy_score(y, preds)\n",
    "        \n",
    "        return 1 - scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "epoch 91 | loss: 0.38851 | val_0_auc: 0.87132 |  0:36:22s\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "epoch 22 | loss: 0.4552  | val_0_auc: 0.8633  |  0:07:44s\n",
      "epoch 57 | loss: 0.42857 | val_0_auc: 0.86627 |  0:10:16s\n",
      "epoch 93 | loss: 0.38267 | val_0_auc: 0.87052 |  0:36:35s\n",
      "epoch 58 | loss: 0.41993 | val_0_auc: 0.87282 |  0:10:26s\n",
      "epoch 92 | loss: 0.39947 | val_0_auc: 0.878   |  0:36:45s\n",
      "epoch 23 | loss: 0.45337 | val_0_auc: 0.86018 |  0:08:04s\n",
      "epoch 59 | loss: 0.42209 | val_0_auc: 0.87384 |  0:10:36s\n",
      "[INFO][abstract_intensifier.py:515] Added config 753c3d as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config 2e04f1 and rejected config 753c3d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config 2e04f1 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 60 | loss: 0.41767 | val_0_auc: 0.87702 |  0:10:46s\n",
      "epoch 94 | loss: 0.39152 | val_0_auc: 0.87252 |  0:36:58s\n",
      "epoch 24 | loss: 0.45042 | val_0_auc: 0.86008 |  0:08:24s\n",
      "epoch 61 | loss: 0.41766 | val_0_auc: 0.87229 |  0:10:56s\n",
      "epoch 93 | loss: 0.39327 | val_0_auc: 0.87685 |  0:37:09s\n",
      "epoch 62 | loss: 0.41517 | val_0_auc: 0.87467 |  0:11:07s\n",
      "epoch 95 | loss: 0.38164 | val_0_auc: 0.87081 |  0:37:22s\n",
      "epoch 25 | loss: 0.44976 | val_0_auc: 0.86007 |  0:08:43s\n",
      "epoch 63 | loss: 0.4156  | val_0_auc: 0.87805 |  0:11:17s\n",
      "epoch 94 | loss: 0.38359 | val_0_auc: 0.88439 |  0:37:32s\n",
      "epoch 64 | loss: 0.41401 | val_0_auc: 0.87617 |  0:11:28s\n",
      "epoch 26 | loss: 0.44952 | val_0_auc: 0.86097 |  0:09:03s\n",
      "epoch 96 | loss: 0.38358 | val_0_auc: 0.86763 |  0:37:45s\n",
      "epoch 65 | loss: 0.4187  | val_0_auc: 0.87345 |  0:11:38s\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "epoch 95 | loss: 0.38502 | val_0_auc: 0.87816 |  0:37:55s\n",
      "epoch 66 | loss: 0.42047 | val_0_auc: 0.87481 |  0:11:48s\n",
      "[INFO][abstract_intensifier.py:594] Added config 2c1c28 and rejected config af1315 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 27 | loss: 0.44543 | val_0_auc: 0.86385 |  0:09:23s\n",
      "epoch 97 | loss: 0.3816  | val_0_auc: 0.8679  |  0:38:06s\n",
      "epoch 67 | loss: 0.40854 | val_0_auc: 0.87502 |  0:11:59s\n",
      "epoch 96 | loss: 0.37898 | val_0_auc: 0.87376 |  0:38:19s\n",
      "epoch 68 | loss: 0.41355 | val_0_auc: 0.87418 |  0:12:09s\n",
      "epoch 28 | loss: 0.4532  | val_0_auc: 0.86551 |  0:09:43s\n",
      "epoch 98 | loss: 0.38021 | val_0_auc: 0.86928 |  0:38:29s\n",
      "epoch 69 | loss: 0.41373 | val_0_auc: 0.87412 |  0:12:19s\n",
      "epoch 70 | loss: 0.41266 | val_0_auc: 0.8767  |  0:12:30s\n",
      "epoch 97 | loss: 0.37944 | val_0_auc: 0.87773 |  0:38:42s\n",
      "epoch 29 | loss: 0.45582 | val_0_auc: 0.86281 |  0:10:02s\n",
      "epoch 71 | loss: 0.41204 | val_0_auc: 0.8731  |  0:12:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 | loss: 0.37577 | val_0_auc: 0.87015 |  0:38:52s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 90 and best_val_0_auc = 0.87289\n",
      "epoch 72 | loss: 0.40954 | val_0_auc: 0.8787  |  0:12:51s\n",
      "epoch 30 | loss: 0.44854 | val_0_auc: 0.86423 |  0:10:23s\n",
      "epoch 98 | loss: 0.38909 | val_0_auc: 0.87462 |  0:39:05s\n",
      "epoch 73 | loss: 0.41608 | val_0_auc: 0.87425 |  0:13:01s\n",
      "epoch 74 | loss: 0.4123  | val_0_auc: 0.87552 |  0:13:12s\n",
      "epoch 31 | loss: 0.44613 | val_0_auc: 0.86355 |  0:10:42s\n",
      "epoch 99 | loss: 0.38644 | val_0_auc: 0.87838 |  0:39:28s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 82 and best_val_0_auc = 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 | loss: 0.41274 | val_0_auc: 0.86772 |  0:13:22s\n",
      "epoch 76 | loss: 0.41153 | val_0_auc: 0.87715 |  0:13:33s\n",
      "epoch 32 | loss: 0.44449 | val_0_auc: 0.85953 |  0:11:02s\n",
      "epoch 77 | loss: 0.41436 | val_0_auc: 0.87629 |  0:13:43s\n",
      "epoch 78 | loss: 0.41906 | val_0_auc: 0.87544 |  0:13:53s\n",
      "epoch 33 | loss: 0.43989 | val_0_auc: 0.86762 |  0:11:22s\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "epoch 79 | loss: 0.40211 | val_0_auc: 0.87451 |  0:14:04s\n",
      "epoch 34 | loss: 0.44505 | val_0_auc: 0.8697  |  0:11:42s\n",
      "epoch 80 | loss: 0.4173  | val_0_auc: 0.87874 |  0:14:14s\n",
      "epoch 81 | loss: 0.41133 | val_0_auc: 0.8805  |  0:14:25s\n",
      "epoch 35 | loss: 0.44057 | val_0_auc: 0.8607  |  0:12:02s\n",
      "epoch 82 | loss: 0.40873 | val_0_auc: 0.87674 |  0:14:36s\n",
      "epoch 83 | loss: 0.4031  | val_0_auc: 0.87859 |  0:14:46s\n",
      "epoch 36 | loss: 0.4414  | val_0_auc: 0.86087 |  0:12:22s\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "epoch 84 | loss: 0.40042 | val_0_auc: 0.87697 |  0:14:57s\n",
      "epoch 85 | loss: 0.40748 | val_0_auc: 0.88105 |  0:15:07s\n",
      "epoch 37 | loss: 0.43982 | val_0_auc: 0.85824 |  0:12:42s\n",
      "epoch 86 | loss: 0.39742 | val_0_auc: 0.87705 |  0:15:18s\n",
      "epoch 87 | loss: 0.39997 | val_0_auc: 0.8774  |  0:15:28s\n",
      "epoch 38 | loss: 0.4358  | val_0_auc: 0.86346 |  0:13:01s\n",
      "epoch 88 | loss: 0.40064 | val_0_auc: 0.883   |  0:15:38s\n",
      "epoch 89 | loss: 0.3982  | val_0_auc: 0.87739 |  0:15:49s\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "epoch 39 | loss: 0.44024 | val_0_auc: 0.8658  |  0:13:21s\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "epoch 90 | loss: 0.39733 | val_0_auc: 0.87967 |  0:15:59s\n",
      "epoch 91 | loss: 0.39323 | val_0_auc: 0.88058 |  0:16:10s\n",
      "epoch 40 | loss: 0.43895 | val_0_auc: 0.86356 |  0:13:41s\n",
      "epoch 92 | loss: 0.38616 | val_0_auc: 0.87754 |  0:16:20s\n",
      "epoch 93 | loss: 0.39485 | val_0_auc: 0.87925 |  0:16:31s\n",
      "epoch 41 | loss: 0.43254 | val_0_auc: 0.86261 |  0:14:01s\n",
      "epoch 94 | loss: 0.38593 | val_0_auc: 0.87768 |  0:16:41s\n",
      "epoch 95 | loss: 0.38896 | val_0_auc: 0.87741 |  0:16:52s\n",
      "epoch 42 | loss: 0.42965 | val_0_auc: 0.86354 |  0:14:21s\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "epoch 96 | loss: 0.40191 | val_0_auc: 0.87354 |  0:17:02s\n",
      "epoch 43 | loss: 0.43445 | val_0_auc: 0.86747 |  0:14:41s\n",
      "epoch 97 | loss: 0.39492 | val_0_auc: 0.8801  |  0:17:13s\n",
      "epoch 98 | loss: 0.39353 | val_0_auc: 0.87897 |  0:17:23s\n",
      "epoch 44 | loss: 0.43909 | val_0_auc: 0.86613 |  0:15:02s\n",
      "epoch 99 | loss: 0.39762 | val_0_auc: 0.87788 |  0:17:34s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 88 and best_val_0_auc = 0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 | loss: 0.44172 | val_0_auc: 0.86165 |  0:15:20s\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "epoch 46 | loss: 0.44357 | val_0_auc: 0.86777 |  0:15:41s\n",
      "epoch 47 | loss: 0.43356 | val_0_auc: 0.86652 |  0:16:01s\n",
      "epoch 48 | loss: 0.4349  | val_0_auc: 0.86701 |  0:16:21s\n",
      "epoch 49 | loss: 0.43213 | val_0_auc: 0.86612 |  0:16:41s\n",
      "epoch 50 | loss: 0.43017 | val_0_auc: 0.86201 |  0:17:01s\n",
      "epoch 51 | loss: 0.42606 | val_0_auc: 0.87211 |  0:17:21s\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.4297318458557129\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9439\n",
      "epoch 52 | loss: 0.41854 | val_0_auc: 0.87367 |  0:17:42s\n",
      "incumbent: Configuration(values={\n",
      "  'colsample_bylevel': 0.9932103869366515,\n",
      "  'colsample_bynode': 0.6917578508788402,\n",
      "  'colsample_bytree': 0.951941640794665,\n",
      "  'gamma': 0.11292898427928434,\n",
      "  'learning_rate': 0.29590233778911207,\n",
      "  'max_depth': 7,\n",
      "  'min_child_weight': 3,\n",
      "  'n_estimators': 818,\n",
      "  'subsample': 0.7892309122532764,\n",
      "})\n",
      "Default cost (Hyperband): 0.20679999999999998\n",
      "epoch 53 | loss: 0.42645 | val_0_auc: 0.87164 |  0:17:59s\n",
      "epoch 54 | loss: 0.42042 | val_0_auc: 0.86869 |  0:18:16s\n",
      "epoch 55 | loss: 0.4225  | val_0_auc: 0.8693  |  0:18:33s\n",
      "epoch 56 | loss: 0.42472 | val_0_auc: 0.86633 |  0:18:50s\n",
      "epoch 57 | loss: 0.41917 | val_0_auc: 0.86918 |  0:19:08s\n",
      "epoch 58 | loss: 0.43003 | val_0_auc: 0.86586 |  0:19:25s\n",
      "epoch 59 | loss: 0.41853 | val_0_auc: 0.864   |  0:19:42s\n",
      "epoch 60 | loss: 0.42766 | val_0_auc: 0.86848 |  0:19:58s\n",
      "epoch 61 | loss: 0.42431 | val_0_auc: 0.87315 |  0:20:15s\n",
      "epoch 62 | loss: 0.41942 | val_0_auc: 0.87353 |  0:20:32s\n",
      "Incumbent cost (Hyperband): 0.0\n",
      "ask <bound method AbstractFacade.ask of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>>\n",
      "get_acquisition_function <function HyperparameterOptimizationFacade.get_acquisition_function at 0x7fe1b4961ca0>\n",
      "get_acquisition_maximizer <function HyperparameterOptimizationFacade.get_acquisition_maximizer at 0x7fe1b4961d30>\n",
      "get_config_selector <function AbstractFacade.get_config_selector at 0x7fe1b49ff5e0>\n",
      "get_initial_design <function MultiFidelityFacade.get_initial_design at 0x7fe1b496a280>\n",
      "get_intensifier <function MultiFidelityFacade.get_intensifier at 0x7fe1b496a1f0>\n",
      "get_model <function HyperparameterOptimizationFacade.get_model at 0x7fe1b4961c10>\n",
      "get_multi_objective_algorithm <function HyperparameterOptimizationFacade.get_multi_objective_algorithm at 0x7fe1b4961f70>\n",
      "get_random_design <function HyperparameterOptimizationFacade.get_random_design at 0x7fe1b4961ee0>\n",
      "get_runhistory_encoder <function HyperparameterOptimizationFacade.get_runhistory_encoder at 0x7fe1b496a040>\n",
      "intensifier <smac.intensifier.hyperband.Hyperband object at 0x7fde6dd15100>\n",
      "meta {'facade': {'name': 'MultiFidelityFacade'}, 'runner': {'name': 'DaskParallelRunner'}, 'model': {'name': 'RandomForest', 'types': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'bounds': [(0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0)], 'pca_components': 7, 'n_trees': 10, 'n_points_per_tree': -1, 'ratio_features': 1.0, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 1048576, 'eps_purity': 1e-08, 'max_nodes': 1048576, 'bootstrapping': True}, 'acquisition_maximizer': {'name': 'LocalAndSortedRandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 10000, 'seed': 0, 'random_search': {'name': 'RandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0}, 'local_search': {'name': 'LocalSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0, 'max_steps': None, 'n_steps_plateau_walk': 10, 'vectorization_min_obtain': 2, 'vectorization_max_obtain': 64}}, 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'intensifier': {'name': 'Hyperband', 'max_incumbents': 10, 'seed': 0, 'eta': 3, 'instance_seed_order': 'shuffle_once', 'incumbent_selection': 'highest_budget'}, 'initial_design': {'name': 'RandomInitialDesign', 'n_configs': 5, 'n_configs_per_hyperparameter': 10, 'additional_configs': [], 'seed': 0}, 'random_design': {'name': 'ProbabilityRandomDesign', 'seed': 0, 'probability': 0.2}, 'runhistory_encoder': {'name': 'RunHistoryLogScaledEncoder', 'considered_states': [<StatusType.SUCCESS: 1>, <StatusType.CRASHED: 2>, <StatusType.MEMORYOUT: 4>], 'lower_budget_states': [], 'scale_percentage': 5, 'seed': 0}, 'multi_objective_algorithm': None, 'config_selector': {'name': 'ConfigSelector', 'retrain_after': 8, 'retries': 16, 'min_trials': 1}, 'version': '2.1.0'}\n",
      "optimize <bound method AbstractFacade.optimize of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>>\n",
      "optimizer <smac.main.smbo.SMBO object at 0x7fdeea8466a0>\n",
      "runhistory <smac.runhistory.runhistory.RunHistory object at 0x7fdf0ac64430>\n",
      "scenario Scenario(configspace=Configuration space object:\n",
      "  Hyperparameters:\n",
      "    colsample_bylevel, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bynode, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    colsample_bytree, Type: UniformFloat, Range: [0.3, 1.0], Default: 0.6\n",
      "    gamma, Type: UniformFloat, Range: [0.1, 1.0], Default: 0.1\n",
      "    learning_rate, Type: UniformFloat, Range: [0.001, 0.3], Default: 0.1\n",
      "    max_depth, Type: UniformInteger, Range: [2, 21], Default: 2\n",
      "    min_child_weight, Type: UniformInteger, Range: [1, 10], Default: 1\n",
      "    n_estimators, Type: UniformInteger, Range: [100, 1200], Default: 100\n",
      "    subsample, Type: UniformFloat, Range: [0.5, 1.0], Default: 0.8\n",
      ", name='4b0226f5e9dfeb37cd1d7f4050b4368e', output_directory=PosixPath('smac_hyperband_output_budget_10mins_XGBT/4b0226f5e9dfeb37cd1d7f4050b4368e/0'), deterministic=False, objectives='cost', crash_cost=inf, termination_cost_threshold=inf, walltime_limit=600, cputime_limit=inf, trial_walltime_limit=None, trial_memory_limit=None, n_trials=10000, use_default_config=False, instances=None, instance_features=None, min_budget=100, max_budget=1000, seed=0, n_workers=8)\n",
      "tell <bound method AbstractFacade.tell of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>>\n",
      "validate <bound method AbstractFacade.validate of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>>\n",
      "facades: [<smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fe0ee789eb0>]\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 789.233 s\n",
      "File: /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py\n",
      "Function: new_function at line 71\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    71                                                       @wraps(function)\n",
      "    72                                                       def new_function(*args, **kwargs):\n",
      "    73         1       7738.0   7738.0      0.0                  new_seconds = kwargs.pop('timeout', seconds)\n",
      "    74         1        710.0    710.0      0.0                  if new_seconds:\n",
      "    75         1      43605.0  43605.0      0.0                      old = signal.signal(signal.SIGALRM, handler)\n",
      "    76         1       9814.0   9814.0      0.0                      signal.setitimer(signal.ITIMER_REAL, new_seconds)\n",
      "    77                                           \n",
      "    78         1        583.0    583.0      0.0                  if not seconds:\n",
      "    79                                                               return function(*args, **kwargs)\n",
      "    80                                           \n",
      "    81         1        375.0    375.0      0.0                  try:\n",
      "    82         2        8e+11    4e+11    100.0                      return function(*args, **kwargs)\n",
      "    83                                                           finally:\n",
      "    84         1       1190.0   1190.0      0.0                      if new_seconds:\n",
      "    85         1      24716.0  24716.0      0.0                          signal.setitimer(signal.ITIMER_REAL, 0)\n",
      "    86         1      94951.0  94951.0      0.0                          signal.signal(signal.SIGALRM, old)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    GBT = XGBTWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            GBT.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_XGBT\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            GBT.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fac99056a90>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.06975579261779785\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9874\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fabd2d682b0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.337233543395996\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9903\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fac7dee3e20>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.9578540325164795\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9902\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7faa98016cd0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.0679450035095215\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9902\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fabd2d38ca0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 5a5556 and rejected config af1315 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.38858604431152344\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9912\n",
      "[INFO][abstract_intensifier.py:594] Added config add540 and rejected config 5a5556 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 1fbeb3 and rejected config add540 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fa9f2bebe20>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.0503990650177002\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9902\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fabf7072d60>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 18:09:41,401 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60348 remote=tcp://127.0.0.1:35265>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 68c154 and rejected config af1315 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c59acd and rejected config 68c154 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config dfeeb1 and rejected config c59acd as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.9683279991149902\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9869\n"
     ]
    }
   ],
   "source": [
    "params_dict_XGBT = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    \n",
    "    class XGBTWrapper(BaseEstimator):\n",
    "        def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='binary:logistic', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "            self.n_estimators = n_estimators\n",
    "            self.max_depth = max_depth\n",
    "            self.seed= seed\n",
    "            self.min_child_weight = min_child_weight\n",
    "            self.gamma = gamma\n",
    "            self.subsample = subsample\n",
    "            self.colsample_bytree = colsample_bytree\n",
    "            self.colsample_bylevel = colsample_bylevel\n",
    "            self.colsample_bynode = colsample_bynode\n",
    "            self.learning_rate = learning_rate\n",
    "            self.objective = objective\n",
    "            # self.num_class = num_class\n",
    "            self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "            max_depth = Integer(\"max_depth\", (2, 21), default=2)\n",
    "            min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "            gamma = Float(\"gamma\", (0.1, 1.0), default=0.1)\n",
    "            subsample = Float(\"subsample\", (0.5, 1.0), default=0.8)\n",
    "            colsample_bytree = Float(\"colsample_bytree\", (0.3, 1.0), default=0.6)\n",
    "            colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1.0), default=0.6)\n",
    "            colsample_bynode = Float(\"colsample_bynode\", (0.3, 1.0), default=0.6)\n",
    "            learning_rate = Float(\"learning_rate\", (0.001, 0.3), default=0.1)\n",
    "            cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "            return cs\n",
    "        \n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "            config = dict(config)  \n",
    "            self.model.set_params(**config)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            # print(\"X shape: \", X.shape)\n",
    "            # print(\"y shape: \", y.shape)\n",
    "            self.model.fit(X, y)\n",
    "            preds = self.model.predict(X)\n",
    "            scores = accuracy_score(y, preds)\n",
    "            \n",
    "            return 1 - scores\n",
    "\n",
    "    # @timeout(90)\n",
    "    def main():\n",
    "        GBT = XGBTWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                GBT.configspace,\n",
    "                walltime_limit=60,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_1mins_XGBT/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                GBT.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "            print(\"optimiizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            # print(\"incumbent:\", incumbent)\n",
    "\n",
    "            # Store the best configuration as dictionary\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_XGBT[dataset_names[i]] = best_params\n",
    "\n",
    "            run_history = smac.get_runhistory()\n",
    "            incumbent_cost = incumbent_run.cost\n",
    "            incumbent_run_id = incumbent_run.config_id\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "\n",
    "        # profiler.disable()\n",
    "        # profiler.print_stats()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: electricity, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2936068843275964, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: eye_movements, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: covertype, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: albert, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: default-of-credit-card-clients, Best Parameters: {'colsample_bylevel': 0.7091444390579225, 'colsample_bynode': 0.8527780012430866, 'colsample_bytree': 0.9817727626117918, 'gamma': 0.11093974747206795, 'learning_rate': 0.2923629799382114, 'max_depth': 21, 'min_child_weight': 2, 'n_estimators': 406, 'subsample': 0.6446242372832947}\n",
      "Dataset: road-safety, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: compas-two-years, Best Parameters: {'colsample_bylevel': 0.7314071575989163, 'colsample_bynode': 0.871113021396617, 'colsample_bytree': 0.8984442277993192, 'gamma': 0.15660811935607621, 'learning_rate': 0.2986528847138854, 'max_depth': 11, 'min_child_weight': 1, 'n_estimators': 780, 'subsample': 0.7050681737572965}\n"
     ]
    }
   ],
   "source": [
    "# for dataset, params in params_dict_XGBT.items():\n",
    "#         run_history = smac.get_runhistory()\n",
    "#         config_id = run_history.get_id_for_config(smac.solver.incumbent)\n",
    "#         cost = run_history.get_cost(smac.solver.incumbent)\n",
    "        \n",
    "#         print(f\"Dataset: {dataset}\")\n",
    "#         print(f\"Best Parameters: {params}\")\n",
    "#         print(f\"Config ID: {config_id}\")\n",
    "#         print(f\"Cost: {cost}\")\n",
    "for dataset, params in params_dict_XGBT.items():\n",
    "    print(f\"Dataset: {dataset}, Best Parameters: {params}\")\n",
    "    # print(f\"Best Parameters: {params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features   \n",
    "        self.criterion = \"gini\"\n",
    "        self.max_samples = max_samples\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "        min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "        criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "        max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "        max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        if config['max_features'] == 'None':\n",
    "            config['max_features'] = None\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        self.model.fit(X, y)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = accuracy_score(y, preds)\n",
    "\n",
    "        return 1 - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "epoch 65 | loss: 0.41395 | val_0_auc: 0.8711  |  0:21:23s\n",
      "epoch 66 | loss: 0.42144 | val_0_auc: 0.86858 |  0:21:42s\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 67 | loss: 0.41503 | val_0_auc: 0.86731 |  0:22:00s\n",
      "epoch 68 | loss: 0.4147  | val_0_auc: 0.87063 |  0:22:19s\n",
      "epoch 69 | loss: 0.41007 | val_0_auc: 0.87032 |  0:22:38s\n",
      "epoch 70 | loss: 0.41036 | val_0_auc: 0.8671  |  0:22:57s\n",
      "epoch 71 | loss: 0.41709 | val_0_auc: 0.86625 |  0:23:16s\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 72 | loss: 0.40179 | val_0_auc: 0.86789 |  0:23:34s\n",
      "epoch 73 | loss: 0.41152 | val_0_auc: 0.87187 |  0:23:53s\n",
      "[INFO][abstract_intensifier.py:594] Added config 73c6d7 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 74 | loss: 0.40943 | val_0_auc: 0.86941 |  0:24:12s\n",
      "epoch 75 | loss: 0.4066  | val_0_auc: 0.86622 |  0:24:31s\n",
      "epoch 76 | loss: 0.41576 | val_0_auc: 0.87509 |  0:24:49s\n",
      "[INFO][abstract_intensifier.py:594] Added config 1e44c8 and rejected config 73c6d7 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 77 | loss: 0.39675 | val_0_auc: 0.8669  |  0:25:08s\n",
      "[INFO][abstract_intensifier.py:594] Added config 4d3877 and rejected config 1e44c8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 78 | loss: 0.40507 | val_0_auc: 0.87092 |  0:25:27s\n",
      "epoch 79 | loss: 0.40096 | val_0_auc: 0.8733  |  0:25:45s\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config b20d63 and rejected config 4d3877 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 80 | loss: 0.39883 | val_0_auc: 0.87405 |  0:26:05s\n",
      "[INFO][abstract_intensifier.py:594] Added config d80be8 and rejected config b20d63 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 44600e and rejected config d80be8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 81 | loss: 0.40122 | val_0_auc: 0.86886 |  0:26:24s\n",
      "[INFO][abstract_intensifier.py:594] Added config c92c1c and rejected config 44600e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 82 | loss: 0.40016 | val_0_auc: 0.87496 |  0:26:43s\n",
      "epoch 83 | loss: 0.40053 | val_0_auc: 0.86546 |  0:27:01s\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "epoch 84 | loss: 0.40604 | val_0_auc: 0.87449 |  0:27:18s\n",
      "[INFO][abstract_intensifier.py:594] Added config 6dc3c2 and rejected config c92c1c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 85 | loss: 0.40699 | val_0_auc: 0.86747 |  0:27:36s\n",
      "epoch 86 | loss: 0.39741 | val_0_auc: 0.86573 |  0:27:54s\n",
      "[INFO][abstract_intensifier.py:594] Added config 2d39b2 and rejected config 6dc3c2 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "epoch 87 | loss: 0.39742 | val_0_auc: 0.86741 |  0:28:13s\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "epoch 88 | loss: 0.3951  | val_0_auc: 0.86963 |  0:28:31s\n",
      "epoch 89 | loss: 0.39424 | val_0_auc: 0.87024 |  0:28:50s\n",
      "epoch 90 | loss: 0.38928 | val_0_auc: 0.87029 |  0:29:09s\n",
      "epoch 91 | loss: 0.39335 | val_0_auc: 0.87072 |  0:29:28s\n",
      "epoch 92 | loss: 0.39711 | val_0_auc: 0.87319 |  0:29:46s\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "epoch 93 | loss: 0.39656 | val_0_auc: 0.86709 |  0:30:05s\n",
      "epoch 94 | loss: 0.38864 | val_0_auc: 0.87125 |  0:30:23s\n",
      "epoch 95 | loss: 0.38899 | val_0_auc: 0.86146 |  0:30:41s\n",
      "epoch 96 | loss: 0.39255 | val_0_auc: 0.87289 |  0:31:00s\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.860384464263916\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9663\n",
      "epoch 97 | loss: 0.38451 | val_0_auc: 0.87185 |  0:31:18s\n",
      "epoch 98 | loss: 0.39211 | val_0_auc: 0.8781  |  0:31:38s\n",
      "epoch 99 | loss: 0.38106 | val_0_auc: 0.86863 |  0:31:57s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_val_0_auc = 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incumbent: Configuration(values={\n",
      "  'criterion': 'gini',\n",
      "  'max_depth': 20,\n",
      "  'max_features': 'sqrt',\n",
      "  'max_samples': 0.9064575310082272,\n",
      "  'min_samples_leaf': 1,\n",
      "  'min_samples_split': 2,\n",
      "  'n_estimators': 224,\n",
      "})\n",
      "Default cost (Hyperband): 0.2724\n",
      "Incumbent cost (Hyperband): 0.0\n",
      "ask <bound method AbstractFacade.ask of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>>\n",
      "get_acquisition_function <function HyperparameterOptimizationFacade.get_acquisition_function at 0x7fe1b4961ca0>\n",
      "get_acquisition_maximizer <function HyperparameterOptimizationFacade.get_acquisition_maximizer at 0x7fe1b4961d30>\n",
      "get_config_selector <function AbstractFacade.get_config_selector at 0x7fe1b49ff5e0>\n",
      "get_initial_design <function MultiFidelityFacade.get_initial_design at 0x7fe1b496a280>\n",
      "get_intensifier <function MultiFidelityFacade.get_intensifier at 0x7fe1b496a1f0>\n",
      "get_model <function HyperparameterOptimizationFacade.get_model at 0x7fe1b4961c10>\n",
      "get_multi_objective_algorithm <function HyperparameterOptimizationFacade.get_multi_objective_algorithm at 0x7fe1b4961f70>\n",
      "get_random_design <function HyperparameterOptimizationFacade.get_random_design at 0x7fe1b4961ee0>\n",
      "get_runhistory_encoder <function HyperparameterOptimizationFacade.get_runhistory_encoder at 0x7fe1b496a040>\n",
      "intensifier <smac.intensifier.hyperband.Hyperband object at 0x7fde7c3d81c0>\n",
      "meta {'facade': {'name': 'MultiFidelityFacade'}, 'runner': {'name': 'DaskParallelRunner'}, 'model': {'name': 'RandomForest', 'types': [3, 0, 3, 0, 0, 0, 0], 'bounds': [(3, nan), (0, 1.0), (3, nan), (0, 1.0), (0, 1.0), (0, 1.0), (0, 1.0)], 'pca_components': 7, 'n_trees': 10, 'n_points_per_tree': -1, 'ratio_features': 1.0, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 1048576, 'eps_purity': 1e-08, 'max_nodes': 1048576, 'bootstrapping': True}, 'acquisition_maximizer': {'name': 'LocalAndSortedRandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 10000, 'seed': 0, 'random_search': {'name': 'RandomSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0}, 'local_search': {'name': 'LocalSearch', 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'challengers': 5000, 'seed': 0, 'max_steps': None, 'n_steps_plateau_walk': 10, 'vectorization_min_obtain': 2, 'vectorization_max_obtain': 64}}, 'acquisition_function': {'name': 'EI', 'xi': 0.0, 'log': True}, 'intensifier': {'name': 'Hyperband', 'max_incumbents': 10, 'seed': 0, 'eta': 3, 'instance_seed_order': 'shuffle_once', 'incumbent_selection': 'highest_budget'}, 'initial_design': {'name': 'RandomInitialDesign', 'n_configs': 5, 'n_configs_per_hyperparameter': 10, 'additional_configs': [], 'seed': 0}, 'random_design': {'name': 'ProbabilityRandomDesign', 'seed': 0, 'probability': 0.2}, 'runhistory_encoder': {'name': 'RunHistoryLogScaledEncoder', 'considered_states': [<StatusType.SUCCESS: 1>, <StatusType.CRASHED: 2>, <StatusType.MEMORYOUT: 4>], 'lower_budget_states': [], 'scale_percentage': 5, 'seed': 0}, 'multi_objective_algorithm': None, 'config_selector': {'name': 'ConfigSelector', 'retrain_after': 8, 'retries': 16, 'min_trials': 1}, 'version': '2.1.0'}\n",
      "optimize <bound method AbstractFacade.optimize of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>>\n",
      "optimizer <smac.main.smbo.SMBO object at 0x7fde6c57b580>\n",
      "runhistory <smac.runhistory.runhistory.RunHistory object at 0x7fde6d274a00>\n",
      "scenario Scenario(configspace=Configuration space object:\n",
      "  Hyperparameters:\n",
      "    criterion, Type: Categorical, Choices: {gini, entropy, log_loss}, Default: gini\n",
      "    max_depth, Type: UniformInteger, Range: [2, 21], Default: 2\n",
      "    max_features, Type: Categorical, Choices: {sqrt, log2, None}, Default: None\n",
      "    max_samples, Type: UniformFloat, Range: [0.1, 0.99], Default: 0.3146426545, on log-scale\n",
      "    min_samples_leaf, Type: UniformInteger, Range: [1, 20], Default: 1\n",
      "    min_samples_split, Type: UniformInteger, Range: [2, 20], Default: 2\n",
      "    n_estimators, Type: UniformInteger, Range: [100, 1200], Default: 100\n",
      ", name='d6bdb85a752e4c566ac140a7f8a78918', output_directory=PosixPath('smac_hyperband_output_budget_10mins_RF/d6bdb85a752e4c566ac140a7f8a78918/0'), deterministic=False, objectives='cost', crash_cost=inf, termination_cost_threshold=inf, walltime_limit=600, cputime_limit=inf, trial_walltime_limit=None, trial_memory_limit=None, n_trials=10000, use_default_config=False, instances=None, instance_features=None, min_budget=100, max_budget=1000, seed=0, n_workers=8)\n",
      "tell <bound method AbstractFacade.tell of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>>\n",
      "validate <bound method AbstractFacade.validate of <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>>\n",
      "facades: [<smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde0fc88880>]\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 714.814 s\n",
      "File: /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py\n",
      "Function: new_function at line 71\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    71                                                       @wraps(function)\n",
      "    72                                                       def new_function(*args, **kwargs):\n",
      "    73         1       4118.0   4118.0      0.0                  new_seconds = kwargs.pop('timeout', seconds)\n",
      "    74         1        688.0    688.0      0.0                  if new_seconds:\n",
      "    75         1      51837.0  51837.0      0.0                      old = signal.signal(signal.SIGALRM, handler)\n",
      "    76         1      10440.0  10440.0      0.0                      signal.setitimer(signal.ITIMER_REAL, new_seconds)\n",
      "    77                                           \n",
      "    78         1        589.0    589.0      0.0                  if not seconds:\n",
      "    79                                                               return function(*args, **kwargs)\n",
      "    80                                           \n",
      "    81         1        416.0    416.0      0.0                  try:\n",
      "    82         2        7e+11    4e+11    100.0                      return function(*args, **kwargs)\n",
      "    83                                                           finally:\n",
      "    84         1       1615.0   1615.0      0.0                      if new_seconds:\n",
      "    85         1      29964.0  29964.0      0.0                          signal.setitimer(signal.ITIMER_REAL, 0)\n",
      "    86         1     105609.0 105609.0      0.0                          signal.signal(signal.SIGALRM, old)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    RF = RFWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            RF.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_RF\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            RF.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 19:11:44,053 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:51182 remote=tcp://127.0.0.1:34187>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.470794677734375\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9967\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "Cost: 0.09550000000000003 | Config ID: 17\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -7.922731637954712\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9970\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "Cost: 0.06742902208201895 | Config ID: 17\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -4.751017093658447\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9961\n",
      "[INFO][abstract_intensifier.py:594] Added config dfe2ca and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9bce44 and rejected config dfe2ca as incumbent because it is not better than the incumbents on 1 instances:\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 13, 'max_features': 'None', 'max_samples': 0.275725107753872, 'min_samples_leaf': 11, 'min_samples_split': 20, 'n_estimators': 898}\n",
      "Cost: 0.16610000000000003 | Config ID: 33\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 19:18:22,654 - distributed.utils_perf - INFO - full garbage collection released 37.85 MiB from 4858 reference cycles (threshold: 9.54 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.6591498851776123\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9965\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "Cost: 0.1652 | Config ID: 17\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.0323939323425293\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9974\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "Cost: 0.18479999999999996 | Config ID: 17\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -4.5697340965271\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9968\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "Cost: 0.1382 | Config ID: 17\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config 0807dc and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 0807dc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config ee463e and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 8713d4 and rejected config ee463e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a70669 and rejected config 8713d4 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config fc232b and rejected config a70669 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 753001 and rejected config fc232b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 394543 and rejected config 753001 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 77f9ba and rejected config 394543 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -4.147078990936279\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9891\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'max_samples': 0.8210682252212592, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 101}\n",
      "Cost: 0.19089810712847366 | Config ID: 58\n"
     ]
    }
   ],
   "source": [
    "params_dict_RF = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    class RFWrapper(BaseEstimator):\n",
    "        def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "            self.n_estimators = n_estimators\n",
    "            self.max_depth = max_depth\n",
    "            self.random_state = random_state\n",
    "            self.min_samples_split = min_samples_split\n",
    "            self.min_samples_leaf = min_samples_leaf\n",
    "            self.max_features = max_features   \n",
    "            self.criterion = \"gini\"\n",
    "            self.max_samples = max_samples\n",
    "            self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "            max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "            min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "            min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "            criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "            max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "            max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "            cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "            return cs\n",
    "        \n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "            config = dict(config)  \n",
    "            if config['max_features'] == 'None':\n",
    "                config['max_features'] = None\n",
    "            self.model.set_params(**config)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            self.model.fit(X, y)\n",
    "            preds = self.model.predict(X)\n",
    "            scores = accuracy_score(y, preds)\n",
    "\n",
    "            return 1 - scores\n",
    "\n",
    "    # @timeout(90)\n",
    "    def main():\n",
    "        RF = RFWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                RF.configspace,\n",
    "                walltime_limit=60,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_1mins_RF/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                RF.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "            print(\"optimiizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_RF[dataset_names[i]] = best_params\n",
    "\n",
    "            incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "            incumbent_run_id = incumbent.config_id\n",
    "\n",
    "            print(f\"Parameters: {best_params}\")\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "\n",
    "        # profiler.disable()\n",
    "        # profiler.print_stats()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electricity : {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "eye_movements : {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "covertype : {'criterion': 'log_loss', 'max_depth': 13, 'max_features': 'None', 'max_samples': 0.275725107753872, 'min_samples_leaf': 11, 'min_samples_split': 20, 'n_estimators': 898}\n",
      "albert : {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "default-of-credit-card-clients : {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "road-safety : {'criterion': 'gini', 'max_depth': 15, 'max_features': 'log2', 'max_samples': 0.9510983452704872, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 175}\n",
      "compas-two-years : {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'max_samples': 0.8210682252212592, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 101}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, params in params_dict_RF.items():\n",
    "    print(dataset_name, \":\", params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabWrapper(BaseEstimator):\n",
    "    def __init__(self, n_d=64, n_a=64, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax', verbose=0):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.seed = seed\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.model = TabNetClassifier(n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, gamma=self.gamma, n_independent=self.n_independent, n_shared=self.n_shared, seed=self.seed, optimizer_fn=self.optimizer_fn, optimizer_params=self.optimizer_params, scheduler_params=self.scheduler_params, scheduler_fn=self.scheduler_fn, mask_type=self.mask_type)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_d = Integer(\"n_d\", (4, 256), default=64)\n",
    "        n_a = Integer(\"n_a\", (4, 256), default=64)\n",
    "        # n_steps = Integer(\"n_steps\", (3, 10), default=5)\n",
    "        # gamma = Float(\"gamma\", (0.9, 2.0), default=1.3)\n",
    "        # n_independent = Integer(\"n_independent\", (1, 10), default=2)\n",
    "        # n_shared = Integer(\"n_shared\", (1, 10), default=2)\n",
    "        # seed = Integer(\"seed\", (0, 1000), default=317)\n",
    "        # optimizer_fn = Categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.AdamW], default=torch.optim.Adam)\n",
    "        # scheduler_fn = Categorical(\"scheduler_fn\", [torch.optim.lr_scheduler.StepLR, torch.optim.lr_scheduler.MultiStepLR], default=torch.optim.lr_scheduler.StepLR)\n",
    "        # mask_type = Categorical(\"mask_type\", ['sparsemax', 'entmax'], default='entmax')\n",
    "        cs.add_hyperparameters([n_d, n_a])\n",
    "        # cs.add_hyperparameters([n_d, n_a, n_steps, gamma, n_independent, n_shared, seed, optimizer_fn, scheduler_fn, mask_type])\n",
    "        return cs\n",
    "\n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float:\n",
    "        config = dict(config)\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], patience=50)\n",
    "        preds = self.model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        return 1 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "<class 'smac.facade.multi_fidelity_facade.MultiFidelityFacade'> | <smac.facade.multi_fidelity_facade.MultiFidelityFacade object at 0x7fde7c3c2ca0>\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.83758 | val_0_auc: 0.76598 |  0:00:02s\n",
      "epoch 1  | loss: 0.57687 | val_0_auc: 0.79049 |  0:00:04s\n",
      "epoch 2  | loss: 0.5471  | val_0_auc: 0.82639 |  0:00:07s\n",
      "epoch 0  | loss: 1.3403  | val_0_auc: 0.74762 |  0:00:09s\n",
      "epoch 3  | loss: 0.52554 | val_0_auc: 0.83619 |  0:00:09s\n",
      "epoch 4  | loss: 0.50996 | val_0_auc: 0.84552 |  0:00:11s\n",
      "epoch 0  | loss: 1.71805 | val_0_auc: 0.70575 |  0:00:15s\n",
      "epoch 0  | loss: 1.33967 | val_0_auc: 0.69962 |  0:00:15s\n",
      "epoch 5  | loss: 0.50691 | val_0_auc: 0.84556 |  0:00:14s\n",
      "epoch 6  | loss: 0.5032  | val_0_auc: 0.84244 |  0:00:16s\n",
      "epoch 1  | loss: 0.75264 | val_0_auc: 0.77813 |  0:00:19s\n",
      "epoch 0  | loss: 1.89356 | val_0_auc: 0.71991 |  0:00:20s\n",
      "epoch 7  | loss: 0.49526 | val_0_auc: 0.84954 |  0:00:18s\n",
      "epoch 0  | loss: 1.9091  | val_0_auc: 0.67222 |  0:00:20s\n",
      "epoch 0  | loss: 2.24179 | val_0_auc: 0.73574 |  0:00:21s\n",
      "epoch 0  | loss: 2.04471 | val_0_auc: 0.74623 |  0:00:21s\n",
      "epoch 8  | loss: 0.49497 | val_0_auc: 0.85019 |  0:00:20s\n",
      "epoch 9  | loss: 0.48985 | val_0_auc: 0.85094 |  0:00:23s\n",
      "epoch 10 | loss: 0.48105 | val_0_auc: 0.84464 |  0:00:25s\n",
      "epoch 11 | loss: 0.48493 | val_0_auc: 0.85357 |  0:00:27s\n",
      "epoch 2  | loss: 0.62659 | val_0_auc: 0.81397 |  0:00:30s\n",
      "epoch 1  | loss: 0.85525 | val_0_auc: 0.75498 |  0:00:30s\n",
      "epoch 1  | loss: 1.03153 | val_0_auc: 0.75284 |  0:00:31s\n",
      "epoch 12 | loss: 0.47552 | val_0_auc: 0.85395 |  0:00:29s\n",
      "epoch 13 | loss: 0.46697 | val_0_auc: 0.85518 |  0:00:32s\n",
      "epoch 14 | loss: 0.46639 | val_0_auc: 0.84925 |  0:00:34s\n",
      "epoch 15 | loss: 0.47099 | val_0_auc: 0.85611 |  0:00:36s\n",
      "epoch 16 | loss: 0.4679  | val_0_auc: 0.86016 |  0:00:38s\n",
      "epoch 3  | loss: 0.56257 | val_0_auc: 0.82427 |  0:00:40s\n",
      "epoch 1  | loss: 0.82269 | val_0_auc: 0.80681 |  0:00:39s\n",
      "epoch 1  | loss: 0.99164 | val_0_auc: 0.73693 |  0:00:42s\n",
      "epoch 17 | loss: 0.46565 | val_0_auc: 0.8622  |  0:00:41s\n",
      "epoch 1  | loss: 0.97753 | val_0_auc: 0.74721 |  0:00:42s\n",
      "epoch 1  | loss: 1.07866 | val_0_auc: 0.72905 |  0:00:42s\n",
      "epoch 18 | loss: 0.46634 | val_0_auc: 0.85855 |  0:00:43s\n",
      "epoch 2  | loss: 0.63267 | val_0_auc: 0.80667 |  0:00:45s\n",
      "epoch 2  | loss: 0.69336 | val_0_auc: 0.7982  |  0:00:46s\n",
      "epoch 19 | loss: 0.4642  | val_0_auc: 0.86411 |  0:00:45s\n",
      "epoch 20 | loss: 0.45682 | val_0_auc: 0.86455 |  0:00:47s\n",
      "epoch 4  | loss: 0.54013 | val_0_auc: 0.83376 |  0:00:51s\n",
      "epoch 21 | loss: 0.45557 | val_0_auc: 0.86056 |  0:00:49s\n",
      "epoch 22 | loss: 0.46123 | val_0_auc: 0.86203 |  0:00:52s\n",
      "epoch 23 | loss: 0.45879 | val_0_auc: 0.86352 |  0:00:54s\n",
      "epoch 24 | loss: 0.45554 | val_0_auc: 0.86353 |  0:00:56s\n",
      "epoch 2  | loss: 0.64339 | val_0_auc: 0.80628 |  0:00:58s\n",
      "epoch 25 | loss: 0.45204 | val_0_auc: 0.86525 |  0:00:58s\n",
      "epoch 3  | loss: 0.57335 | val_0_auc: 0.81471 |  0:01:00s\n",
      "epoch 5  | loss: 0.52576 | val_0_auc: 0.8346  |  0:01:01s\n",
      "epoch 2  | loss: 0.77551 | val_0_auc: 0.80844 |  0:01:03s\n",
      "epoch 26 | loss: 0.45126 | val_0_auc: 0.8628  |  0:01:00s\n",
      "epoch 3  | loss: 0.60465 | val_0_auc: 0.8135  |  0:01:03s\n",
      "epoch 2  | loss: 0.78051 | val_0_auc: 0.8004  |  0:01:03s\n",
      "epoch 2  | loss: 0.77673 | val_0_auc: 0.77942 |  0:01:03s\n",
      "epoch 27 | loss: 0.45697 | val_0_auc: 0.86514 |  0:01:03s\n",
      "epoch 28 | loss: 0.45011 | val_0_auc: 0.86347 |  0:01:05s\n",
      "epoch 29 | loss: 0.44974 | val_0_auc: 0.86456 |  0:01:07s\n",
      "epoch 30 | loss: 0.44315 | val_0_auc: 0.86331 |  0:01:09s\n",
      "epoch 6  | loss: 0.50952 | val_0_auc: 0.84902 |  0:01:11s\n",
      "epoch 31 | loss: 0.43905 | val_0_auc: 0.86703 |  0:01:11s\n",
      "epoch 4  | loss: 0.52246 | val_0_auc: 0.83033 |  0:01:15s\n",
      "epoch 32 | loss: 0.44689 | val_0_auc: 0.86355 |  0:01:14s\n",
      "epoch 4  | loss: 0.55758 | val_0_auc: 0.82668 |  0:01:18s\n",
      "epoch 33 | loss: 0.45118 | val_0_auc: 0.86777 |  0:01:16s\n",
      "epoch 3  | loss: 0.59953 | val_0_auc: 0.82073 |  0:01:17s\n",
      "epoch 34 | loss: 0.4449  | val_0_auc: 0.86804 |  0:01:18s\n",
      "epoch 7  | loss: 0.49487 | val_0_auc: 0.84729 |  0:01:21s\n",
      "epoch 35 | loss: 0.43999 | val_0_auc: 0.86567 |  0:01:21s\n",
      "epoch 3  | loss: 0.68236 | val_0_auc: 0.79608 |  0:01:23s\n",
      "epoch 3  | loss: 0.64023 | val_0_auc: 0.80383 |  0:01:23s\n",
      "epoch 36 | loss: 0.4402  | val_0_auc: 0.87029 |  0:01:23s\n",
      "epoch 3  | loss: 0.76122 | val_0_auc: 0.78749 |  0:01:24s\n",
      "epoch 37 | loss: 0.43465 | val_0_auc: 0.87406 |  0:01:25s\n",
      "epoch 38 | loss: 0.43903 | val_0_auc: 0.86901 |  0:01:27s\n",
      "epoch 5  | loss: 0.50514 | val_0_auc: 0.85322 |  0:01:30s\n",
      "epoch 39 | loss: 0.44391 | val_0_auc: 0.87275 |  0:01:29s\n",
      "epoch 8  | loss: 0.49158 | val_0_auc: 0.8467  |  0:01:31s\n",
      "epoch 5  | loss: 0.55886 | val_0_auc: 0.81898 |  0:01:33s\n",
      "epoch 40 | loss: 0.44207 | val_0_auc: 0.87243 |  0:01:32s\n",
      "epoch 41 | loss: 0.43774 | val_0_auc: 0.86941 |  0:01:34s\n",
      "epoch 42 | loss: 0.42661 | val_0_auc: 0.8707  |  0:01:36s\n",
      "epoch 4  | loss: 0.53113 | val_0_auc: 0.82868 |  0:01:37s\n",
      "epoch 43 | loss: 0.43594 | val_0_auc: 0.87301 |  0:01:38s\n",
      "epoch 9  | loss: 0.47954 | val_0_auc: 0.8546  |  0:01:42s\n",
      "epoch 44 | loss: 0.43497 | val_0_auc: 0.86997 |  0:01:40s\n",
      "epoch 4  | loss: 0.63705 | val_0_auc: 0.82828 |  0:01:45s\n",
      "epoch 45 | loss: 0.43289 | val_0_auc: 0.86847 |  0:01:43s\n",
      "epoch 6  | loss: 0.49793 | val_0_auc: 0.8468  |  0:01:45s\n",
      "epoch 4  | loss: 0.56073 | val_0_auc: 0.82632 |  0:01:45s\n",
      "epoch 46 | loss: 0.43773 | val_0_auc: 0.86958 |  0:01:45s\n",
      "epoch 4  | loss: 0.61057 | val_0_auc: 0.8368  |  0:01:46s\n",
      "epoch 47 | loss: 0.42973 | val_0_auc: 0.87579 |  0:01:47s\n",
      "epoch 6  | loss: 0.54025 | val_0_auc: 0.84575 |  0:01:49s\n",
      "epoch 48 | loss: 0.42755 | val_0_auc: 0.87446 |  0:01:50s\n",
      "epoch 10 | loss: 0.482   | val_0_auc: 0.85504 |  0:01:52s\n",
      "epoch 49 | loss: 0.42712 | val_0_auc: 0.87215 |  0:01:52s\n",
      "epoch 50 | loss: 0.42972 | val_0_auc: 0.87076 |  0:01:54s\n",
      "epoch 5  | loss: 0.5325  | val_0_auc: 0.83895 |  0:01:56s\n",
      "epoch 51 | loss: 0.43076 | val_0_auc: 0.87169 |  0:01:56s\n",
      "epoch 7  | loss: 0.49057 | val_0_auc: 0.84831 |  0:02:00s\n",
      "epoch 52 | loss: 0.42696 | val_0_auc: 0.87517 |  0:01:59s\n",
      "epoch 53 | loss: 0.43035 | val_0_auc: 0.87497 |  0:02:01s\n",
      "epoch 11 | loss: 0.48337 | val_0_auc: 0.85452 |  0:02:02s\n",
      "epoch 54 | loss: 0.43115 | val_0_auc: 0.87461 |  0:02:03s\n",
      "epoch 7  | loss: 0.52065 | val_0_auc: 0.84692 |  0:02:05s\n",
      "epoch 5  | loss: 0.54813 | val_0_auc: 0.83021 |  0:02:06s\n",
      "epoch 5  | loss: 0.53371 | val_0_auc: 0.83803 |  0:02:05s\n",
      "epoch 55 | loss: 0.41911 | val_0_auc: 0.87635 |  0:02:05s\n",
      "epoch 5  | loss: 0.53966 | val_0_auc: 0.82567 |  0:02:07s\n",
      "epoch 56 | loss: 0.42202 | val_0_auc: 0.87437 |  0:02:08s\n",
      "epoch 57 | loss: 0.41876 | val_0_auc: 0.8661  |  0:02:10s\n",
      "epoch 12 | loss: 0.4812  | val_0_auc: 0.86103 |  0:02:12s\n",
      "epoch 58 | loss: 0.4247  | val_0_auc: 0.87229 |  0:02:12s\n",
      "epoch 8  | loss: 0.49248 | val_0_auc: 0.84082 |  0:02:16s\n",
      "epoch 59 | loss: 0.42447 | val_0_auc: 0.87255 |  0:02:14s\n",
      "epoch 6  | loss: 0.5091  | val_0_auc: 0.84326 |  0:02:15s\n",
      "epoch 60 | loss: 0.42488 | val_0_auc: 0.86963 |  0:02:17s\n",
      "epoch 61 | loss: 0.42064 | val_0_auc: 0.87299 |  0:02:19s\n",
      "epoch 8  | loss: 0.50323 | val_0_auc: 0.84526 |  0:02:21s\n",
      "epoch 62 | loss: 0.40965 | val_0_auc: 0.87354 |  0:02:21s\n",
      "epoch 13 | loss: 0.46438 | val_0_auc: 0.8618  |  0:02:22s\n",
      "epoch 63 | loss: 0.41566 | val_0_auc: 0.87136 |  0:02:23s\n",
      "epoch 6  | loss: 0.51752 | val_0_auc: 0.8394  |  0:02:27s\n",
      "epoch 6  | loss: 0.52126 | val_0_auc: 0.83644 |  0:02:26s\n",
      "epoch 64 | loss: 0.42077 | val_0_auc: 0.87252 |  0:02:25s\n",
      "epoch 6  | loss: 0.51919 | val_0_auc: 0.84474 |  0:02:28s\n",
      "epoch 65 | loss: 0.42106 | val_0_auc: 0.87036 |  0:02:28s\n",
      "epoch 9  | loss: 0.47852 | val_0_auc: 0.85409 |  0:02:31s\n",
      "epoch 66 | loss: 0.42268 | val_0_auc: 0.87625 |  0:02:30s\n",
      "epoch 14 | loss: 0.46355 | val_0_auc: 0.86024 |  0:02:32s\n",
      "epoch 67 | loss: 0.41967 | val_0_auc: 0.8729  |  0:02:32s\n",
      "epoch 9  | loss: 0.4914  | val_0_auc: 0.84611 |  0:02:36s\n",
      "epoch 68 | loss: 0.41545 | val_0_auc: 0.87097 |  0:02:34s\n",
      "epoch 7  | loss: 0.48477 | val_0_auc: 0.85266 |  0:02:34s\n",
      "epoch 69 | loss: 0.41089 | val_0_auc: 0.87411 |  0:02:37s\n",
      "epoch 70 | loss: 0.4062  | val_0_auc: 0.86839 |  0:02:39s\n",
      "epoch 15 | loss: 0.46588 | val_0_auc: 0.85937 |  0:02:42s\n",
      "epoch 71 | loss: 0.4111  | val_0_auc: 0.87202 |  0:02:42s\n",
      "epoch 72 | loss: 0.41119 | val_0_auc: 0.87212 |  0:02:44s\n",
      "epoch 10 | loss: 0.4761  | val_0_auc: 0.8582  |  0:02:46s\n",
      "epoch 7  | loss: 0.49153 | val_0_auc: 0.84087 |  0:02:47s\n",
      "epoch 7  | loss: 0.50457 | val_0_auc: 0.84867 |  0:02:48s\n",
      "epoch 73 | loss: 0.41413 | val_0_auc: 0.87221 |  0:02:46s\n",
      "epoch 74 | loss: 0.40966 | val_0_auc: 0.87344 |  0:02:49s\n",
      "epoch 7  | loss: 0.50446 | val_0_auc: 0.8358  |  0:02:49s\n",
      "epoch 10 | loss: 0.48888 | val_0_auc: 0.83793 |  0:02:52s\n",
      "epoch 16 | loss: 0.46271 | val_0_auc: 0.86088 |  0:02:52s\n",
      "epoch 75 | loss: 0.4037  | val_0_auc: 0.87448 |  0:02:51s\n",
      "epoch 76 | loss: 0.41269 | val_0_auc: 0.86882 |  0:02:53s\n",
      "epoch 8  | loss: 0.4915  | val_0_auc: 0.85478 |  0:02:53s\n",
      "epoch 77 | loss: 0.4143  | val_0_auc: 0.86876 |  0:02:55s\n",
      "epoch 78 | loss: 0.41339 | val_0_auc: 0.87765 |  0:02:58s\n",
      "epoch 11 | loss: 0.46858 | val_0_auc: 0.85616 |  0:03:02s\n",
      "epoch 79 | loss: 0.40124 | val_0_auc: 0.87379 |  0:03:00s\n",
      "epoch 17 | loss: 0.45886 | val_0_auc: 0.86247 |  0:03:02s\n",
      "epoch 80 | loss: 0.41478 | val_0_auc: 0.87264 |  0:03:02s\n",
      "epoch 81 | loss: 0.40655 | val_0_auc: 0.87422 |  0:03:04s\n",
      "epoch 11 | loss: 0.48195 | val_0_auc: 0.84291 |  0:03:07s\n",
      "epoch 8  | loss: 0.48589 | val_0_auc: 0.84638 |  0:03:07s\n",
      "epoch 82 | loss: 0.4089  | val_0_auc: 0.87767 |  0:03:06s\n",
      "epoch 8  | loss: 0.48496 | val_0_auc: 0.85285 |  0:03:09s\n",
      "epoch 83 | loss: 0.40436 | val_0_auc: 0.87557 |  0:03:09s\n",
      "epoch 8  | loss: 0.48113 | val_0_auc: 0.85075 |  0:03:10s\n",
      "epoch 84 | loss: 0.40318 | val_0_auc: 0.87962 |  0:03:11s\n",
      "epoch 18 | loss: 0.46328 | val_0_auc: 0.8571  |  0:03:12s\n",
      "epoch 9  | loss: 0.47688 | val_0_auc: 0.84609 |  0:03:13s\n",
      "epoch 85 | loss: 0.39963 | val_0_auc: 0.88077 |  0:03:13s\n",
      "epoch 12 | loss: 0.46102 | val_0_auc: 0.86235 |  0:03:17s\n",
      "epoch 86 | loss: 0.40193 | val_0_auc: 0.8784  |  0:03:15s\n",
      "epoch 87 | loss: 0.40319 | val_0_auc: 0.87602 |  0:03:18s\n",
      "epoch 88 | loss: 0.40338 | val_0_auc: 0.87343 |  0:03:20s\n",
      "epoch 19 | loss: 0.45888 | val_0_auc: 0.86374 |  0:03:23s\n",
      "epoch 12 | loss: 0.47896 | val_0_auc: 0.84515 |  0:03:23s\n",
      "epoch 89 | loss: 0.39857 | val_0_auc: 0.88013 |  0:03:22s\n",
      "epoch 90 | loss: 0.3976  | val_0_auc: 0.87069 |  0:03:25s\n",
      "epoch 91 | loss: 0.40476 | val_0_auc: 0.87252 |  0:03:27s\n",
      "epoch 9  | loss: 0.47282 | val_0_auc: 0.85653 |  0:03:28s\n",
      "epoch 9  | loss: 0.47302 | val_0_auc: 0.85458 |  0:03:31s\n",
      "epoch 92 | loss: 0.39506 | val_0_auc: 0.87597 |  0:03:29s\n",
      "epoch 13 | loss: 0.45999 | val_0_auc: 0.85856 |  0:03:32s\n",
      "epoch 9  | loss: 0.48396 | val_0_auc: 0.85366 |  0:03:31s\n",
      "epoch 93 | loss: 0.40155 | val_0_auc: 0.88018 |  0:03:31s\n",
      "epoch 20 | loss: 0.45624 | val_0_auc: 0.86347 |  0:03:33s\n",
      "epoch 10 | loss: 0.48835 | val_0_auc: 0.84569 |  0:03:32s\n",
      "epoch 94 | loss: 0.40003 | val_0_auc: 0.87586 |  0:03:33s\n",
      "epoch 95 | loss: 0.40074 | val_0_auc: 0.87638 |  0:03:36s\n",
      "epoch 96 | loss: 0.40462 | val_0_auc: 0.87822 |  0:03:38s\n",
      "epoch 13 | loss: 0.47658 | val_0_auc: 0.85176 |  0:03:40s\n",
      "epoch 97 | loss: 0.40494 | val_0_auc: 0.87663 |  0:03:40s\n",
      "epoch 21 | loss: 0.45124 | val_0_auc: 0.86167 |  0:03:43s\n",
      "epoch 98 | loss: 0.39994 | val_0_auc: 0.87509 |  0:03:42s\n",
      "epoch 99 | loss: 0.39466 | val_0_auc: 0.87557 |  0:03:44s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 85 and best_val_0_auc = 0.88077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.45711 | val_0_auc: 0.85881 |  0:03:47s\n",
      "epoch 10 | loss: 0.4652  | val_0_auc: 0.85768 |  0:03:49s\n",
      "epoch 10 | loss: 0.46852 | val_0_auc: 0.85563 |  0:03:52s\n",
      "epoch 11 | loss: 0.48205 | val_0_auc: 0.8523  |  0:03:51s\n",
      "epoch 22 | loss: 0.45716 | val_0_auc: 0.86406 |  0:03:53s\n",
      "epoch 10 | loss: 0.49021 | val_0_auc: 0.85147 |  0:03:52s\n",
      "epoch 14 | loss: 0.48953 | val_0_auc: 0.84692 |  0:03:56s\n",
      "epoch 0  | loss: 1.56151 | val_0_auc: 0.69811 |  0:00:10s\n",
      "epoch 15 | loss: 0.45542 | val_0_auc: 0.86173 |  0:04:02s\n",
      "epoch 23 | loss: 0.44974 | val_0_auc: 0.86415 |  0:04:03s\n",
      "epoch 1  | loss: 0.95974 | val_0_auc: 0.77292 |  0:00:21s\n",
      "epoch 11 | loss: 0.46651 | val_0_auc: 0.85883 |  0:04:10s\n",
      "epoch 15 | loss: 0.46676 | val_0_auc: 0.84766 |  0:04:12s\n",
      "epoch 12 | loss: 0.4815  | val_0_auc: 0.85255 |  0:04:10s\n",
      "epoch 11 | loss: 0.47557 | val_0_auc: 0.85704 |  0:04:13s\n",
      "epoch 24 | loss: 0.45    | val_0_auc: 0.86593 |  0:04:13s\n",
      "epoch 11 | loss: 0.47532 | val_0_auc: 0.85481 |  0:04:14s\n",
      "epoch 16 | loss: 0.45152 | val_0_auc: 0.8638  |  0:04:18s\n",
      "epoch 2  | loss: 0.71965 | val_0_auc: 0.78467 |  0:00:32s\n",
      "epoch 25 | loss: 0.44925 | val_0_auc: 0.86975 |  0:04:23s\n",
      "epoch 16 | loss: 0.46347 | val_0_auc: 0.85153 |  0:04:28s\n",
      "epoch 3  | loss: 0.63647 | val_0_auc: 0.81801 |  0:00:42s\n",
      "epoch 13 | loss: 0.47778 | val_0_auc: 0.84668 |  0:04:30s\n",
      "epoch 12 | loss: 0.45894 | val_0_auc: 0.85684 |  0:04:31s\n",
      "epoch 17 | loss: 0.45035 | val_0_auc: 0.85648 |  0:04:33s\n",
      "epoch 26 | loss: 0.44944 | val_0_auc: 0.86812 |  0:04:33s\n",
      "epoch 12 | loss: 0.47995 | val_0_auc: 0.85353 |  0:04:34s\n",
      "epoch 12 | loss: 0.47721 | val_0_auc: 0.85121 |  0:04:35s\n",
      "epoch 4  | loss: 0.62501 | val_0_auc: 0.82747 |  0:00:54s\n",
      "epoch 27 | loss: 0.44872 | val_0_auc: 0.86809 |  0:04:43s\n",
      "epoch 17 | loss: 0.46186 | val_0_auc: 0.85009 |  0:04:44s\n",
      "epoch 18 | loss: 0.44835 | val_0_auc: 0.86471 |  0:04:49s\n",
      "epoch 14 | loss: 0.4753  | val_0_auc: 0.84914 |  0:04:49s\n",
      "epoch 13 | loss: 0.45681 | val_0_auc: 0.86038 |  0:04:53s\n",
      "epoch 28 | loss: 0.45075 | val_0_auc: 0.8678  |  0:04:53s\n",
      "epoch 5  | loss: 0.56717 | val_0_auc: 0.82947 |  0:01:05s\n",
      "epoch 13 | loss: 0.48073 | val_0_auc: 0.85739 |  0:04:56s\n",
      "epoch 13 | loss: 0.47813 | val_0_auc: 0.85073 |  0:04:57s\n",
      "epoch 18 | loss: 0.46077 | val_0_auc: 0.85689 |  0:05:00s\n",
      "epoch 29 | loss: 0.44704 | val_0_auc: 0.86898 |  0:05:03s\n",
      "epoch 19 | loss: 0.44995 | val_0_auc: 0.85879 |  0:05:04s\n",
      "epoch 6  | loss: 0.53596 | val_0_auc: 0.82974 |  0:01:16s\n",
      "epoch 15 | loss: 0.4783  | val_0_auc: 0.85922 |  0:05:08s\n",
      "epoch 30 | loss: 0.44853 | val_0_auc: 0.86775 |  0:05:14s\n",
      "epoch 14 | loss: 0.45812 | val_0_auc: 0.85605 |  0:05:14s\n",
      "epoch 19 | loss: 0.45289 | val_0_auc: 0.85456 |  0:05:16s\n",
      "epoch 7  | loss: 0.52834 | val_0_auc: 0.83675 |  0:01:27s\n",
      "epoch 14 | loss: 0.46852 | val_0_auc: 0.85927 |  0:05:18s\n",
      "epoch 20 | loss: 0.45003 | val_0_auc: 0.86232 |  0:05:19s\n",
      "epoch 14 | loss: 0.46499 | val_0_auc: 0.85698 |  0:05:18s\n",
      "epoch 31 | loss: 0.44607 | val_0_auc: 0.87055 |  0:05:23s\n",
      "epoch 8  | loss: 0.51631 | val_0_auc: 0.84708 |  0:01:37s\n",
      "epoch 16 | loss: 0.4672  | val_0_auc: 0.85847 |  0:05:27s\n",
      "epoch 20 | loss: 0.45642 | val_0_auc: 0.85681 |  0:05:32s\n",
      "epoch 32 | loss: 0.43443 | val_0_auc: 0.87189 |  0:05:34s\n",
      "epoch 21 | loss: 0.4466  | val_0_auc: 0.86403 |  0:05:34s\n",
      "epoch 15 | loss: 0.45269 | val_0_auc: 0.86214 |  0:05:35s\n",
      "epoch 9  | loss: 0.51499 | val_0_auc: 0.84321 |  0:01:48s\n",
      "epoch 15 | loss: 0.46439 | val_0_auc: 0.86112 |  0:05:39s\n",
      "epoch 15 | loss: 0.46017 | val_0_auc: 0.85746 |  0:05:39s\n",
      "epoch 33 | loss: 0.43686 | val_0_auc: 0.87088 |  0:05:44s\n",
      "epoch 21 | loss: 0.4628  | val_0_auc: 0.85548 |  0:05:48s\n",
      "epoch 10 | loss: 0.51684 | val_0_auc: 0.85144 |  0:01:59s\n",
      "epoch 17 | loss: 0.47123 | val_0_auc: 0.85464 |  0:05:47s\n",
      "epoch 22 | loss: 0.44287 | val_0_auc: 0.86108 |  0:05:49s\n",
      "epoch 34 | loss: 0.44271 | val_0_auc: 0.8726  |  0:05:54s\n",
      "epoch 16 | loss: 0.45852 | val_0_auc: 0.85252 |  0:05:57s\n",
      "epoch 11 | loss: 0.50763 | val_0_auc: 0.844   |  0:02:09s\n",
      "epoch 16 | loss: 0.46219 | val_0_auc: 0.86458 |  0:06:01s\n",
      "epoch 16 | loss: 0.46044 | val_0_auc: 0.85816 |  0:06:00s\n",
      "epoch 22 | loss: 0.451   | val_0_auc: 0.85964 |  0:06:03s\n",
      "epoch 35 | loss: 0.43298 | val_0_auc: 0.87081 |  0:06:04s\n",
      "epoch 23 | loss: 0.43643 | val_0_auc: 0.86257 |  0:06:05s\n",
      "epoch 18 | loss: 0.46601 | val_0_auc: 0.85607 |  0:06:06s\n",
      "epoch 12 | loss: 0.495   | val_0_auc: 0.84466 |  0:02:20s\n",
      "epoch 36 | loss: 0.43093 | val_0_auc: 0.87316 |  0:06:14s\n",
      "epoch 17 | loss: 0.45614 | val_0_auc: 0.86261 |  0:06:19s\n",
      "epoch 23 | loss: 0.44865 | val_0_auc: 0.85556 |  0:06:20s\n",
      "epoch 13 | loss: 0.49234 | val_0_auc: 0.84733 |  0:02:31s\n",
      "epoch 24 | loss: 0.44145 | val_0_auc: 0.86701 |  0:06:20s\n",
      "epoch 17 | loss: 0.46878 | val_0_auc: 0.85381 |  0:06:22s\n",
      "epoch 17 | loss: 0.46831 | val_0_auc: 0.86007 |  0:06:21s\n",
      "epoch 37 | loss: 0.43548 | val_0_auc: 0.87082 |  0:06:24s\n",
      "epoch 19 | loss: 0.45813 | val_0_auc: 0.86042 |  0:06:25s\n",
      "epoch 14 | loss: 0.48881 | val_0_auc: 0.8478  |  0:02:41s\n",
      "epoch 38 | loss: 0.42878 | val_0_auc: 0.8727  |  0:06:35s\n",
      "epoch 25 | loss: 0.43604 | val_0_auc: 0.86492 |  0:06:36s\n",
      "epoch 24 | loss: 0.45017 | val_0_auc: 0.85304 |  0:06:36s\n",
      "epoch 18 | loss: 0.45216 | val_0_auc: 0.85927 |  0:06:40s\n",
      "epoch 15 | loss: 0.48236 | val_0_auc: 0.85325 |  0:02:52s\n",
      "epoch 18 | loss: 0.45983 | val_0_auc: 0.86178 |  0:06:43s\n",
      "epoch 18 | loss: 0.46197 | val_0_auc: 0.85792 |  0:06:43s\n",
      "epoch 39 | loss: 0.42451 | val_0_auc: 0.87392 |  0:06:45s\n",
      "epoch 20 | loss: 0.46212 | val_0_auc: 0.86042 |  0:06:44s\n",
      "epoch 26 | loss: 0.4433  | val_0_auc: 0.85931 |  0:06:51s\n",
      "epoch 25 | loss: 0.44464 | val_0_auc: 0.85584 |  0:06:52s\n",
      "epoch 16 | loss: 0.48089 | val_0_auc: 0.85031 |  0:03:03s\n",
      "epoch 40 | loss: 0.43653 | val_0_auc: 0.87274 |  0:06:56s\n",
      "epoch 19 | loss: 0.44487 | val_0_auc: 0.86052 |  0:07:01s\n",
      "epoch 17 | loss: 0.4696  | val_0_auc: 0.84982 |  0:03:13s\n",
      "epoch 19 | loss: 0.46031 | val_0_auc: 0.85669 |  0:07:05s\n",
      "epoch 21 | loss: 0.45242 | val_0_auc: 0.85835 |  0:07:03s\n",
      "epoch 19 | loss: 0.46258 | val_0_auc: 0.862   |  0:07:04s\n",
      "epoch 27 | loss: 0.4452  | val_0_auc: 0.86194 |  0:07:06s\n",
      "epoch 41 | loss: 0.42169 | val_0_auc: 0.87213 |  0:07:06s\n",
      "epoch 26 | loss: 0.44311 | val_0_auc: 0.85731 |  0:07:08s\n",
      "epoch 18 | loss: 0.47685 | val_0_auc: 0.86142 |  0:03:24s\n",
      "epoch 42 | loss: 0.42818 | val_0_auc: 0.87469 |  0:07:16s\n",
      "epoch 28 | loss: 0.43709 | val_0_auc: 0.86588 |  0:07:21s\n",
      "epoch 20 | loss: 0.47199 | val_0_auc: 0.85903 |  0:07:21s\n",
      "epoch 27 | loss: 0.44361 | val_0_auc: 0.86044 |  0:07:24s\n",
      "epoch 19 | loss: 0.47265 | val_0_auc: 0.86096 |  0:03:35s\n",
      "epoch 22 | loss: 0.4552  | val_0_auc: 0.8633  |  0:07:23s\n",
      "epoch 20 | loss: 0.45365 | val_0_auc: 0.86354 |  0:07:26s\n",
      "epoch 43 | loss: 0.42709 | val_0_auc: 0.87373 |  0:07:26s\n",
      "epoch 20 | loss: 0.44963 | val_0_auc: 0.86218 |  0:07:26s\n",
      "epoch 20 | loss: 0.47002 | val_0_auc: 0.86215 |  0:03:45s\n",
      "epoch 44 | loss: 0.4441  | val_0_auc: 0.86924 |  0:07:36s\n",
      "epoch 29 | loss: 0.43189 | val_0_auc: 0.86783 |  0:07:37s\n",
      "epoch 28 | loss: 0.44624 | val_0_auc: 0.8632  |  0:07:40s\n",
      "epoch 21 | loss: 0.48298 | val_0_auc: 0.85985 |  0:07:42s\n",
      "epoch 23 | loss: 0.45337 | val_0_auc: 0.86018 |  0:07:42s\n",
      "epoch 21 | loss: 0.46775 | val_0_auc: 0.86342 |  0:03:56s\n",
      "epoch 45 | loss: 0.44341 | val_0_auc: 0.86922 |  0:07:46s\n",
      "epoch 21 | loss: 0.44501 | val_0_auc: 0.85967 |  0:07:48s\n",
      "epoch 21 | loss: 0.44814 | val_0_auc: 0.86299 |  0:07:48s\n",
      "epoch 30 | loss: 0.42661 | val_0_auc: 0.87107 |  0:07:52s\n",
      "epoch 22 | loss: 0.46406 | val_0_auc: 0.85959 |  0:04:07s\n",
      "epoch 29 | loss: 0.44038 | val_0_auc: 0.85937 |  0:07:56s\n",
      "epoch 46 | loss: 0.44418 | val_0_auc: 0.87219 |  0:07:56s\n",
      "epoch 24 | loss: 0.45042 | val_0_auc: 0.86008 |  0:08:01s\n",
      "epoch 22 | loss: 0.46334 | val_0_auc: 0.85992 |  0:08:03s\n",
      "epoch 47 | loss: 0.44913 | val_0_auc: 0.87341 |  0:08:06s\n",
      "epoch 23 | loss: 0.46044 | val_0_auc: 0.85984 |  0:04:18s\n",
      "epoch 31 | loss: 0.42746 | val_0_auc: 0.8711  |  0:08:07s\n",
      "epoch 22 | loss: 0.45273 | val_0_auc: 0.86878 |  0:08:09s\n",
      "epoch 22 | loss: 0.44645 | val_0_auc: 0.86042 |  0:08:09s\n",
      "epoch 30 | loss: 0.44845 | val_0_auc: 0.86583 |  0:08:12s\n",
      "epoch 48 | loss: 0.4391  | val_0_auc: 0.8697  |  0:08:17s\n",
      "epoch 24 | loss: 0.45657 | val_0_auc: 0.86387 |  0:04:29s\n",
      "epoch 25 | loss: 0.44976 | val_0_auc: 0.86007 |  0:08:20s\n",
      "epoch 32 | loss: 0.42492 | val_0_auc: 0.86682 |  0:08:22s\n",
      "epoch 23 | loss: 0.44489 | val_0_auc: 0.86081 |  0:08:24s\n",
      "epoch 49 | loss: 0.43345 | val_0_auc: 0.87298 |  0:08:27s\n",
      "epoch 31 | loss: 0.4414  | val_0_auc: 0.8585  |  0:08:27s\n",
      "epoch 25 | loss: 0.44908 | val_0_auc: 0.86235 |  0:04:39s\n",
      "epoch 23 | loss: 0.44928 | val_0_auc: 0.86546 |  0:08:30s\n",
      "epoch 23 | loss: 0.44744 | val_0_auc: 0.86144 |  0:08:30s\n",
      "epoch 50 | loss: 0.43676 | val_0_auc: 0.87188 |  0:08:37s\n",
      "epoch 33 | loss: 0.42871 | val_0_auc: 0.86807 |  0:08:37s\n",
      "epoch 26 | loss: 0.44721 | val_0_auc: 0.86484 |  0:04:50s\n",
      "epoch 26 | loss: 0.44952 | val_0_auc: 0.86097 |  0:08:39s\n",
      "epoch 32 | loss: 0.45007 | val_0_auc: 0.86407 |  0:08:44s\n",
      "epoch 24 | loss: 0.45196 | val_0_auc: 0.86366 |  0:08:45s\n",
      "epoch 51 | loss: 0.43325 | val_0_auc: 0.86781 |  0:08:47s\n",
      "epoch 24 | loss: 0.44818 | val_0_auc: 0.86948 |  0:08:51s\n",
      "epoch 27 | loss: 0.44726 | val_0_auc: 0.86374 |  0:05:01s\n",
      "epoch 34 | loss: 0.42492 | val_0_auc: 0.85705 |  0:08:53s\n",
      "epoch 24 | loss: 0.44815 | val_0_auc: 0.86547 |  0:08:51s\n",
      "epoch 52 | loss: 0.43448 | val_0_auc: 0.87039 |  0:08:57s\n",
      "epoch 33 | loss: 0.44036 | val_0_auc: 0.86387 |  0:09:00s\n",
      "epoch 27 | loss: 0.44543 | val_0_auc: 0.86385 |  0:08:58s\n",
      "epoch 28 | loss: 0.45251 | val_0_auc: 0.86131 |  0:05:12s\n",
      "epoch 25 | loss: 0.44465 | val_0_auc: 0.85705 |  0:09:06s\n",
      "epoch 53 | loss: 0.4306  | val_0_auc: 0.87128 |  0:09:07s\n",
      "epoch 35 | loss: 0.43434 | val_0_auc: 0.86267 |  0:09:08s\n",
      "epoch 25 | loss: 0.44815 | val_0_auc: 0.86058 |  0:09:12s\n",
      "epoch 29 | loss: 0.44271 | val_0_auc: 0.86275 |  0:05:24s\n",
      "epoch 25 | loss: 0.44672 | val_0_auc: 0.85898 |  0:09:12s\n",
      "epoch 34 | loss: 0.43809 | val_0_auc: 0.86833 |  0:09:16s\n",
      "epoch 54 | loss: 0.42873 | val_0_auc: 0.87168 |  0:09:17s\n",
      "epoch 28 | loss: 0.4532  | val_0_auc: 0.86551 |  0:09:18s\n",
      "epoch 36 | loss: 0.42539 | val_0_auc: 0.86365 |  0:09:23s\n",
      "epoch 30 | loss: 0.44875 | val_0_auc: 0.86111 |  0:05:35s\n",
      "epoch 55 | loss: 0.42344 | val_0_auc: 0.86999 |  0:09:27s\n",
      "epoch 26 | loss: 0.43946 | val_0_auc: 0.86229 |  0:09:27s\n",
      "epoch 35 | loss: 0.43733 | val_0_auc: 0.86418 |  0:09:32s\n",
      "epoch 26 | loss: 0.44247 | val_0_auc: 0.8683  |  0:09:34s\n",
      "epoch 31 | loss: 0.43927 | val_0_auc: 0.86497 |  0:05:46s\n",
      "epoch 26 | loss: 0.44442 | val_0_auc: 0.86519 |  0:09:34s\n",
      "epoch 56 | loss: 0.43118 | val_0_auc: 0.86777 |  0:09:38s\n",
      "epoch 29 | loss: 0.45582 | val_0_auc: 0.86281 |  0:09:37s\n",
      "epoch 37 | loss: 0.43316 | val_0_auc: 0.86112 |  0:09:39s\n",
      "epoch 32 | loss: 0.44815 | val_0_auc: 0.86475 |  0:05:56s\n",
      "epoch 57 | loss: 0.42857 | val_0_auc: 0.86627 |  0:09:48s\n",
      "epoch 36 | loss: 0.42831 | val_0_auc: 0.86208 |  0:09:48s\n",
      "epoch 27 | loss: 0.45327 | val_0_auc: 0.86579 |  0:09:48s\n",
      "epoch 38 | loss: 0.43541 | val_0_auc: 0.85886 |  0:09:54s\n",
      "epoch 27 | loss: 0.45418 | val_0_auc: 0.86729 |  0:09:55s\n",
      "epoch 27 | loss: 0.45823 | val_0_auc: 0.86543 |  0:09:55s\n",
      "epoch 33 | loss: 0.44474 | val_0_auc: 0.86681 |  0:06:07s\n",
      "epoch 30 | loss: 0.44854 | val_0_auc: 0.86423 |  0:09:56s\n",
      "epoch 58 | loss: 0.41993 | val_0_auc: 0.87282 |  0:09:58s\n",
      "epoch 37 | loss: 0.43306 | val_0_auc: 0.86918 |  0:10:04s\n",
      "epoch 34 | loss: 0.43486 | val_0_auc: 0.86537 |  0:06:18s\n",
      "epoch 59 | loss: 0.42209 | val_0_auc: 0.87384 |  0:10:08s\n",
      "epoch 39 | loss: 0.43639 | val_0_auc: 0.86971 |  0:10:09s\n",
      "epoch 28 | loss: 0.44443 | val_0_auc: 0.86301 |  0:10:09s\n",
      "epoch 28 | loss: 0.44367 | val_0_auc: 0.87094 |  0:10:16s\n",
      "epoch 28 | loss: 0.46056 | val_0_auc: 0.8644  |  0:10:16s\n",
      "epoch 31 | loss: 0.44613 | val_0_auc: 0.86355 |  0:10:16s\n",
      "epoch 35 | loss: 0.44441 | val_0_auc: 0.86218 |  0:06:29s\n",
      "epoch 60 | loss: 0.41767 | val_0_auc: 0.87702 |  0:10:18s\n",
      "epoch 38 | loss: 0.43256 | val_0_auc: 0.86642 |  0:10:20s\n",
      "epoch 40 | loss: 0.42478 | val_0_auc: 0.86815 |  0:10:24s\n",
      "epoch 61 | loss: 0.41766 | val_0_auc: 0.87229 |  0:10:28s\n",
      "epoch 36 | loss: 0.44592 | val_0_auc: 0.8646  |  0:06:39s\n",
      "epoch 29 | loss: 0.4359  | val_0_auc: 0.86359 |  0:10:30s\n",
      "epoch 39 | loss: 0.42784 | val_0_auc: 0.86415 |  0:10:36s\n",
      "epoch 29 | loss: 0.43911 | val_0_auc: 0.87399 |  0:10:37s\n",
      "epoch 32 | loss: 0.44449 | val_0_auc: 0.85953 |  0:10:35s\n",
      "epoch 62 | loss: 0.41517 | val_0_auc: 0.87467 |  0:10:37s\n",
      "epoch 29 | loss: 0.45402 | val_0_auc: 0.86614 |  0:10:36s\n",
      "epoch 41 | loss: 0.41857 | val_0_auc: 0.86793 |  0:10:39s\n",
      "epoch 37 | loss: 0.4412  | val_0_auc: 0.86594 |  0:06:50s\n",
      "epoch 63 | loss: 0.4156  | val_0_auc: 0.87805 |  0:10:48s\n",
      "epoch 38 | loss: 0.44917 | val_0_auc: 0.86758 |  0:07:01s\n",
      "epoch 30 | loss: 0.43384 | val_0_auc: 0.86596 |  0:10:51s\n",
      "epoch 40 | loss: 0.43411 | val_0_auc: 0.86487 |  0:10:52s\n",
      "epoch 42 | loss: 0.42324 | val_0_auc: 0.86738 |  0:10:54s\n",
      "epoch 33 | loss: 0.43989 | val_0_auc: 0.86762 |  0:10:54s\n",
      "epoch 30 | loss: 0.4415  | val_0_auc: 0.86854 |  0:10:59s\n",
      "epoch 64 | loss: 0.41401 | val_0_auc: 0.87617 |  0:10:58s\n",
      "epoch 30 | loss: 0.44338 | val_0_auc: 0.86029 |  0:10:58s\n",
      "epoch 39 | loss: 0.43962 | val_0_auc: 0.86227 |  0:07:11s\n",
      "epoch 41 | loss: 0.43342 | val_0_auc: 0.86652 |  0:11:08s\n",
      "epoch 65 | loss: 0.4187  | val_0_auc: 0.87345 |  0:11:09s\n",
      "epoch 43 | loss: 0.42587 | val_0_auc: 0.86429 |  0:11:09s\n",
      "epoch 40 | loss: 0.44468 | val_0_auc: 0.8602  |  0:07:22s\n",
      "epoch 31 | loss: 0.43029 | val_0_auc: 0.86337 |  0:11:12s\n",
      "epoch 34 | loss: 0.44505 | val_0_auc: 0.8697  |  0:11:13s\n",
      "epoch 66 | loss: 0.42047 | val_0_auc: 0.87481 |  0:11:19s\n",
      "epoch 31 | loss: 0.43997 | val_0_auc: 0.87019 |  0:11:21s\n",
      "epoch 31 | loss: 0.43819 | val_0_auc: 0.86428 |  0:11:19s\n",
      "epoch 41 | loss: 0.43658 | val_0_auc: 0.86626 |  0:07:33s\n",
      "epoch 42 | loss: 0.44    | val_0_auc: 0.86917 |  0:11:24s\n",
      "epoch 44 | loss: 0.43116 | val_0_auc: 0.85983 |  0:11:25s\n",
      "epoch 67 | loss: 0.40854 | val_0_auc: 0.87502 |  0:11:29s\n",
      "epoch 42 | loss: 0.43869 | val_0_auc: 0.86804 |  0:07:43s\n",
      "epoch 32 | loss: 0.43101 | val_0_auc: 0.86676 |  0:11:34s\n",
      "epoch 35 | loss: 0.44057 | val_0_auc: 0.8607  |  0:11:33s\n",
      "epoch 68 | loss: 0.41355 | val_0_auc: 0.87418 |  0:11:39s\n",
      "epoch 43 | loss: 0.42754 | val_0_auc: 0.87123 |  0:11:40s\n",
      "epoch 45 | loss: 0.42458 | val_0_auc: 0.86924 |  0:11:40s\n",
      "epoch 32 | loss: 0.43992 | val_0_auc: 0.87191 |  0:11:43s\n",
      "epoch 32 | loss: 0.43436 | val_0_auc: 0.86784 |  0:11:41s\n",
      "epoch 43 | loss: 0.43569 | val_0_auc: 0.86626 |  0:07:54s\n",
      "epoch 69 | loss: 0.41373 | val_0_auc: 0.87412 |  0:11:49s\n",
      "epoch 36 | loss: 0.4414  | val_0_auc: 0.86087 |  0:11:52s\n",
      "epoch 44 | loss: 0.43591 | val_0_auc: 0.86252 |  0:08:05s\n",
      "epoch 46 | loss: 0.41662 | val_0_auc: 0.86585 |  0:11:55s\n",
      "epoch 44 | loss: 0.43263 | val_0_auc: 0.86548 |  0:11:56s\n",
      "epoch 33 | loss: 0.43214 | val_0_auc: 0.86735 |  0:11:55s\n",
      "epoch 70 | loss: 0.41266 | val_0_auc: 0.8767  |  0:11:59s\n",
      "epoch 33 | loss: 0.4331  | val_0_auc: 0.87151 |  0:12:04s\n",
      "epoch 33 | loss: 0.43797 | val_0_auc: 0.86425 |  0:12:02s\n",
      "epoch 45 | loss: 0.43982 | val_0_auc: 0.85935 |  0:08:15s\n",
      "epoch 71 | loss: 0.41204 | val_0_auc: 0.8731  |  0:12:09s\n",
      "epoch 47 | loss: 0.42104 | val_0_auc: 0.86299 |  0:12:10s\n",
      "epoch 45 | loss: 0.42788 | val_0_auc: 0.86956 |  0:12:12s\n",
      "epoch 37 | loss: 0.43982 | val_0_auc: 0.85824 |  0:12:11s\n",
      "epoch 46 | loss: 0.43152 | val_0_auc: 0.86032 |  0:08:26s\n",
      "epoch 34 | loss: 0.43071 | val_0_auc: 0.86824 |  0:12:16s\n",
      "epoch 72 | loss: 0.40954 | val_0_auc: 0.8787  |  0:12:20s\n",
      "epoch 34 | loss: 0.44459 | val_0_auc: 0.86716 |  0:12:25s\n",
      "epoch 34 | loss: 0.43501 | val_0_auc: 0.87148 |  0:12:23s\n",
      "epoch 48 | loss: 0.41738 | val_0_auc: 0.87137 |  0:12:26s\n",
      "epoch 47 | loss: 0.4384  | val_0_auc: 0.86845 |  0:08:37s\n",
      "epoch 46 | loss: 0.4251  | val_0_auc: 0.86674 |  0:12:27s\n",
      "epoch 73 | loss: 0.41608 | val_0_auc: 0.87425 |  0:12:30s\n",
      "epoch 38 | loss: 0.4358  | val_0_auc: 0.86346 |  0:12:31s\n",
      "epoch 48 | loss: 0.43929 | val_0_auc: 0.86928 |  0:08:48s\n",
      "epoch 35 | loss: 0.43127 | val_0_auc: 0.86362 |  0:12:38s\n",
      "epoch 74 | loss: 0.4123  | val_0_auc: 0.87552 |  0:12:40s\n",
      "epoch 49 | loss: 0.42006 | val_0_auc: 0.87283 |  0:12:41s\n",
      "epoch 47 | loss: 0.42381 | val_0_auc: 0.87031 |  0:12:43s\n",
      "epoch 35 | loss: 0.44892 | val_0_auc: 0.86658 |  0:12:46s\n",
      "epoch 35 | loss: 0.44749 | val_0_auc: 0.86556 |  0:12:45s\n",
      "epoch 49 | loss: 0.43071 | val_0_auc: 0.8697  |  0:08:59s\n",
      "epoch 75 | loss: 0.41274 | val_0_auc: 0.86772 |  0:12:50s\n",
      "epoch 39 | loss: 0.44024 | val_0_auc: 0.8658  |  0:12:50s\n",
      "epoch 50 | loss: 0.41101 | val_0_auc: 0.86882 |  0:12:56s\n",
      "epoch 50 | loss: 0.43213 | val_0_auc: 0.86774 |  0:09:09s\n",
      "epoch 48 | loss: 0.41747 | val_0_auc: 0.86962 |  0:12:59s\n",
      "epoch 36 | loss: 0.43767 | val_0_auc: 0.8578  |  0:12:59s\n",
      "epoch 76 | loss: 0.41153 | val_0_auc: 0.87715 |  0:13:01s\n",
      "epoch 36 | loss: 0.4409  | val_0_auc: 0.8696  |  0:13:07s\n",
      "epoch 36 | loss: 0.44072 | val_0_auc: 0.86983 |  0:13:06s\n",
      "epoch 51 | loss: 0.43643 | val_0_auc: 0.86472 |  0:09:20s\n",
      "epoch 40 | loss: 0.43895 | val_0_auc: 0.86356 |  0:13:09s\n",
      "epoch 77 | loss: 0.41436 | val_0_auc: 0.87629 |  0:13:11s\n",
      "epoch 51 | loss: 0.40714 | val_0_auc: 0.87181 |  0:13:12s\n",
      "epoch 49 | loss: 0.4175  | val_0_auc: 0.86645 |  0:13:15s\n",
      "epoch 37 | loss: 0.44004 | val_0_auc: 0.86602 |  0:13:20s\n",
      "epoch 52 | loss: 0.43397 | val_0_auc: 0.86479 |  0:09:32s\n",
      "epoch 78 | loss: 0.41906 | val_0_auc: 0.87544 |  0:13:21s\n",
      "epoch 52 | loss: 0.41113 | val_0_auc: 0.86808 |  0:13:28s\n",
      "epoch 37 | loss: 0.43853 | val_0_auc: 0.86644 |  0:13:28s\n",
      "epoch 37 | loss: 0.4302  | val_0_auc: 0.87189 |  0:13:27s\n",
      "epoch 41 | loss: 0.43254 | val_0_auc: 0.86261 |  0:13:28s\n",
      "epoch 50 | loss: 0.41667 | val_0_auc: 0.86758 |  0:13:31s\n",
      "epoch 53 | loss: 0.43178 | val_0_auc: 0.86371 |  0:09:42s\n",
      "epoch 79 | loss: 0.40211 | val_0_auc: 0.87451 |  0:13:32s\n",
      "epoch 38 | loss: 0.42809 | val_0_auc: 0.8656  |  0:13:41s\n",
      "epoch 54 | loss: 0.43115 | val_0_auc: 0.86537 |  0:09:53s\n",
      "epoch 80 | loss: 0.4173  | val_0_auc: 0.87874 |  0:13:42s\n",
      "epoch 53 | loss: 0.40557 | val_0_auc: 0.86956 |  0:13:43s\n",
      "epoch 51 | loss: 0.41717 | val_0_auc: 0.86842 |  0:13:47s\n",
      "epoch 38 | loss: 0.44772 | val_0_auc: 0.86632 |  0:13:49s\n",
      "epoch 42 | loss: 0.42965 | val_0_auc: 0.86354 |  0:13:48s\n",
      "epoch 38 | loss: 0.43753 | val_0_auc: 0.86552 |  0:13:48s\n",
      "epoch 81 | loss: 0.41133 | val_0_auc: 0.8805  |  0:13:52s\n",
      "epoch 55 | loss: 0.41464 | val_0_auc: 0.86468 |  0:10:04s\n",
      "epoch 54 | loss: 0.40689 | val_0_auc: 0.86529 |  0:13:58s\n",
      "epoch 39 | loss: 0.42439 | val_0_auc: 0.8702  |  0:14:02s\n",
      "epoch 52 | loss: 0.41511 | val_0_auc: 0.87031 |  0:14:03s\n",
      "epoch 82 | loss: 0.40873 | val_0_auc: 0.87674 |  0:14:03s\n",
      "epoch 56 | loss: 0.42018 | val_0_auc: 0.86554 |  0:10:15s\n",
      "epoch 43 | loss: 0.43445 | val_0_auc: 0.86747 |  0:14:06s\n",
      "epoch 39 | loss: 0.43849 | val_0_auc: 0.86505 |  0:14:11s\n",
      "epoch 39 | loss: 0.44141 | val_0_auc: 0.87484 |  0:14:10s\n",
      "epoch 83 | loss: 0.4031  | val_0_auc: 0.87859 |  0:14:13s\n",
      "epoch 55 | loss: 0.4109  | val_0_auc: 0.87133 |  0:14:14s\n",
      "epoch 57 | loss: 0.42461 | val_0_auc: 0.86168 |  0:10:26s\n",
      "epoch 53 | loss: 0.43877 | val_0_auc: 0.86676 |  0:14:19s\n",
      "epoch 84 | loss: 0.40042 | val_0_auc: 0.87697 |  0:14:24s\n",
      "epoch 40 | loss: 0.42774 | val_0_auc: 0.86532 |  0:14:24s\n",
      "epoch 58 | loss: 0.42479 | val_0_auc: 0.86517 |  0:10:37s\n",
      "epoch 44 | loss: 0.43909 | val_0_auc: 0.86613 |  0:14:25s\n",
      "epoch 56 | loss: 0.40648 | val_0_auc: 0.87122 |  0:14:29s\n",
      "epoch 40 | loss: 0.43015 | val_0_auc: 0.86682 |  0:14:32s\n",
      "epoch 40 | loss: 0.43141 | val_0_auc: 0.8701  |  0:14:31s\n",
      "epoch 54 | loss: 0.41978 | val_0_auc: 0.86951 |  0:14:35s\n",
      "epoch 85 | loss: 0.40748 | val_0_auc: 0.88105 |  0:14:35s\n",
      "epoch 59 | loss: 0.41826 | val_0_auc: 0.86343 |  0:10:48s\n",
      "epoch 57 | loss: 0.40495 | val_0_auc: 0.87041 |  0:14:44s\n",
      "epoch 41 | loss: 0.43163 | val_0_auc: 0.86771 |  0:14:44s\n",
      "epoch 86 | loss: 0.39742 | val_0_auc: 0.87705 |  0:14:45s\n",
      "epoch 45 | loss: 0.44172 | val_0_auc: 0.86165 |  0:14:44s\n",
      "epoch 60 | loss: 0.41515 | val_0_auc: 0.86377 |  0:10:58s\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "'Timed Out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m profiler\u001b[39m.\u001b[39madd_function(main)\n\u001b[1;32m     51\u001b[0m profiler\u001b[39m.\u001b[39menable()\n\u001b[0;32m---> 53\u001b[0m main()\n\u001b[1;32m     55\u001b[0m profiler\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m     56\u001b[0m profiler\u001b[39m.\u001b[39mprint_stats()\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:82\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.new_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     81\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m new_seconds:\n",
      "Cell \u001b[0;32mIn[95], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moptimiizing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(smac), \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m, smac)\n\u001b[0;32m---> 32\u001b[0m incumbent \u001b[39m=\u001b[39m smac\u001b[39m.\u001b[39;49moptimize()\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mincumbent:\u001b[39m\u001b[39m\"\u001b[39m, incumbent)\n\u001b[1;32m     34\u001b[0m default_cost \u001b[39m=\u001b[39m smac\u001b[39m.\u001b[39mvalidate(Tab\u001b[39m.\u001b[39mconfigspace\u001b[39m.\u001b[39mget_default_configuration())\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/smac/facade/abstract_facade.py:322\u001b[0m, in \u001b[0;36mAbstractFacade.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata_to_scatter must be None or dict with some elements, but got an empty dict.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     incumbents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer\u001b[39m.\u001b[39;49moptimize(data_to_scatter\u001b[39m=\u001b[39;49mdata_to_scatter)\n\u001b[1;32m    323\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/smac/main/smbo.py:304\u001b[0m, in \u001b[0;36mSMBO.optimize\u001b[0;34m(self, data_to_scatter)\u001b[0m\n\u001b[1;32m    300\u001b[0m     trial_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mask()\n\u001b[1;32m    302\u001b[0m     \u001b[39m# We submit the trial to the runner\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# In multi-worker mode, SMAC waits till a new worker is available here\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runner\u001b[39m.\u001b[39;49msubmit_trial(trial_info\u001b[39m=\u001b[39;49mtrial_info, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdask_data_to_scatter)\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/smac/runner/dask_runner.py:130\u001b[0m, in \u001b[0;36mDaskParallelRunner.submit_trial\u001b[0;34m(self, trial_info, **dask_data_to_scatter)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_available_workers() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    129\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mNo worker available. Waiting for one to be available...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     wait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pending_trials, return_when\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFIRST_COMPLETED\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_pending_trials()\n\u001b[1;32m    133\u001b[0m \u001b[39m# Check again to make sure that there are resources\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/client.py:5462\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m   5460\u001b[0m     timeout \u001b[39m=\u001b[39m parse_timedelta(timeout, default\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5461\u001b[0m client \u001b[39m=\u001b[39m default_client()\n\u001b[0;32m-> 5462\u001b[0m result \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49msync(_wait, fs, timeout\u001b[39m=\u001b[39;49mtimeout, return_when\u001b[39m=\u001b[39;49mreturn_when)\n\u001b[1;32m   5463\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils.py:358\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[1;32m    359\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, callback_timeout\u001b[39m=\u001b[39;49mcallback_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    360\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils.py:431\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m e\u001b[39m.\u001b[39mis_set():\n\u001b[0;32m--> 431\u001b[0m         wait(\u001b[39m10\u001b[39;49m)\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils.py:420\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    419\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m         \u001b[39mreturn\u001b[39;00m e\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    421\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m         loop\u001b[39m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:69\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[0;32m---> 69\u001b[0m     _raise_exception(timeout_exception, exception_message)\n",
      "File \u001b[0;32m~/miniconda3/envs/NeuroData/lib/python3.9/site-packages/timeout_decorator/timeout_decorator.py:45\u001b[0m, in \u001b[0;36m_raise_exception\u001b[0;34m(exception, exception_message)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" This function checks if a exception message is given.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39mIf there is no exception message, the default behaviour is maintained.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mIf there is an exception message, the message is passed to the exception with the 'value' keyword.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m exception_message \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m exception()\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m exception(exception_message)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: 'Timed Out'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 | loss: 0.425   | val_0_auc: 0.87356 |  0:14:50s\n",
      "epoch 41 | loss: 0.43985 | val_0_auc: 0.86968 |  0:14:53s\n",
      "epoch 41 | loss: 0.42583 | val_0_auc: 0.868   |  0:14:52s\n",
      "epoch 87 | loss: 0.39997 | val_0_auc: 0.8774  |  0:14:55s\n",
      "epoch 61 | loss: 0.41461 | val_0_auc: 0.86091 |  0:11:09s\n",
      "epoch 58 | loss: 0.41145 | val_0_auc: 0.86838 |  0:14:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 16:21:26,339 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-08 16:21:26,343 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-08 16:21:26,347 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-08 16:21:26,348 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-08 16:21:26,393 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 | loss: 0.4233  | val_0_auc: 0.86757 |  0:15:05s\n",
      "epoch 46 | loss: 0.44357 | val_0_auc: 0.86777 |  0:15:04s\n",
      "epoch 88 | loss: 0.40064 | val_0_auc: 0.883   |  0:15:06s\n",
      "epoch 56 | loss: 0.42195 | val_0_auc: 0.87193 |  0:15:07s\n",
      "epoch 62 | loss: 0.41485 | val_0_auc: 0.8628  |  0:11:20s\n",
      "epoch 42 | loss: 0.43201 | val_0_auc: 0.86954 |  0:15:14s\n",
      "epoch 59 | loss: 0.40129 | val_0_auc: 0.86736 |  0:15:15s\n",
      "epoch 42 | loss: 0.42259 | val_0_auc: 0.87454 |  0:15:14s\n",
      "epoch 89 | loss: 0.3982  | val_0_auc: 0.87739 |  0:15:16s\n",
      "epoch 63 | loss: 0.41838 | val_0_auc: 0.86553 |  0:11:31s\n",
      "epoch 57 | loss: 0.41706 | val_0_auc: 0.87332 |  0:15:23s\n",
      "epoch 47 | loss: 0.43356 | val_0_auc: 0.86652 |  0:15:23s\n",
      "epoch 43 | loss: 0.42554 | val_0_auc: 0.86861 |  0:15:26s\n",
      "epoch 90 | loss: 0.39733 | val_0_auc: 0.87967 |  0:15:27s\n",
      "epoch 60 | loss: 0.40623 | val_0_auc: 0.87181 |  0:15:30s\n",
      "epoch 64 | loss: 0.42461 | val_0_auc: 0.86451 |  0:11:42s\n",
      "epoch 43 | loss: 0.42892 | val_0_auc: 0.87135 |  0:15:36s\n",
      "epoch 43 | loss: 0.43295 | val_0_auc: 0.86655 |  0:15:35s\n",
      "epoch 91 | loss: 0.39323 | val_0_auc: 0.88058 |  0:15:37s\n",
      "epoch 58 | loss: 0.41157 | val_0_auc: 0.869   |  0:15:39s\n",
      "epoch 65 | loss: 0.4318  | val_0_auc: 0.86833 |  0:11:53s\n",
      "epoch 48 | loss: 0.4349  | val_0_auc: 0.86701 |  0:15:42s\n",
      "epoch 61 | loss: 0.40856 | val_0_auc: 0.86467 |  0:15:46s\n",
      "epoch 44 | loss: 0.41876 | val_0_auc: 0.86866 |  0:15:47s\n",
      "epoch 92 | loss: 0.38616 | val_0_auc: 0.87754 |  0:15:48s\n",
      "epoch 66 | loss: 0.42911 | val_0_auc: 0.85956 |  0:12:04s\n",
      "epoch 59 | loss: 0.42207 | val_0_auc: 0.87017 |  0:15:55s\n",
      "epoch 44 | loss: 0.43189 | val_0_auc: 0.87354 |  0:15:58s\n",
      "epoch 44 | loss: 0.4284  | val_0_auc: 0.86945 |  0:15:56s\n",
      "epoch 93 | loss: 0.39485 | val_0_auc: 0.87925 |  0:15:59s\n",
      "epoch 62 | loss: 0.40809 | val_0_auc: 0.86991 |  0:16:01s\n",
      "epoch 49 | loss: 0.43213 | val_0_auc: 0.86612 |  0:16:01s\n",
      "epoch 67 | loss: 0.41615 | val_0_auc: 0.86731 |  0:12:14s\n"
     ]
    }
   ],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    Tab = TabWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            Tab.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_Tab\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            Tab.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(Tab.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here 1\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "Here 2\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_auc = 0.86199\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.86813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.87585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.86792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 13:07:16,190 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-24112b486da4c31b29f3b25444e807a7')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:16,192 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:46888 remote=tcp://127.0.0.1:42335>: Stream is closed\n",
      "2024-08-13 13:07:16,193 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-f39d4eb3d7e8ca9820ba5949395df315')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:16,193 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:46890 remote=tcp://127.0.0.1:42335>: Stream is closed\n",
      "2024-08-13 13:07:16,207 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-dc30fa3071f01aa23976c8e63fd1099f')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:16,208 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-90cec3ec673ecc1cea411ff56fb3264d')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:16,217 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-0292255fc978521f71fad8b207c62100')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:16,222 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:46900 remote=tcp://127.0.0.1:42335>: Stream is closed\n",
      "2024-08-13 13:07:20,186 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:20,193 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:20,203 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:20,230 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:20,234 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:21,802 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-b13d183549ed7d8dce60619381a14381')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:21,802 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58256 remote=tcp://127.0.0.1:43773>: Stream is closed\n",
      "2024-08-13 13:07:21,805 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58266 remote=tcp://127.0.0.1:43773>: Stream is closed\n",
      "2024-08-13 13:07:21,813 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-22ee6ac9173e4e9e20decf99cc9ae3b3')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:21,822 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute('run_wrapper-98b1cd88b6d17d6d955f266ceefb1879')\" coro=<Worker.execute() done, defined at /home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-08-13 13:07:21,818 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58264 remote=tcp://127.0.0.1:43773>: Stream is closed\n",
      "2024-08-13 13:07:25,794 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:25,801 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-13 13:07:25,808 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_auc = 0.86199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.86813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.86277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.86813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.87585\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87534\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.87449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.8727\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.87585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.86902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.87967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_auc = 0.86357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.87449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.86733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.86422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.86902\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.87156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 546335 as new incumbent because there are no incumbents yet.\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.86241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.87967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_auc = 0.85015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.86577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.86711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.86933\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.86991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.86782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.86551\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_auc = 0.85664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.86739\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.86814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.87221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_auc = 0.87055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.86698\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_auc = 0.86298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_auc = 0.87474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.86008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.8729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.86814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config d31eb5 and rejected config 546335 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 7fcf93 and rejected config d31eb5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_auc = 0.85674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_auc = 0.85732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_auc = 0.86956\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 26 and best_val_0_auc = 0.87477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.86556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.87126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.87585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.87449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.87221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.86698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_auc = 0.87282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.87967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.87378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_auc = 0.85801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.87441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.87578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_auc = 0.86313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_auc = 0.87272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.87592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_auc = 0.87188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.87081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -9.492891550064087\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9933\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_auc = 0.87107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_auc = 0.85076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.87221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 571c93 and rejected config 7fcf93 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_auc = 0.87282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 26 and best_val_0_auc = 0.87477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.87449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.87441\n",
      "Parameters: {'n_a': 21, 'n_d': 119, 'verbose': 0}\n",
      "Cost: 0.1985 | Config ID: 26\n",
      "Here 3\n",
      "Here 1\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "Here 2\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_auc = 0.57786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2024-08-13 13:43:14,646 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:33074 remote=tcp://127.0.0.1:35559>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.52945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_auc = 0.59027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.62517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.60774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_auc = 0.6281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.61088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_auc = 0.56136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.58836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.59615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_auc = 0.57254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_auc = 0.58993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_auc = 0.57645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 546335 as new incumbent because there are no incumbents yet.\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_auc = 0.55482\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.5854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 037a0b and rejected config 546335 as incumbent because it is not better than the incumbents on 1 instances:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61773\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.59813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.60627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 3587c7 and rejected config 037a0b as incumbent because it is not better than the incumbents on 1 instances:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_auc = 0.55046\n",
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_auc = 0.57919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.60284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.58467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.59336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.60882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.58383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_auc = 0.56356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_auc = 0.6281\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_auc = 0.54745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.62517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.58901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_auc = 0.61004\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_auc = 0.60284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.55027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_auc = 0.55717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 2 and best_val_0_auc = 0.54934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.57092\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.61439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_auc = 0.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_auc = 0.58922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.60882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_auc = 0.59336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_auc = 0.56143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_auc = 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.63184\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.62135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_auc = 0.59056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.54534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.61842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.55149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_auc = 0.55434\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.56399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.61439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config d17be8 and rejected config 3587c7 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_auc = 0.57186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.57509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 548676 and rejected config d17be8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_auc = 0.57662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_auc = 0.60882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_auc = 0.55149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_auc = 0.56498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_auc = 0.62135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.63184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_auc = 0.61842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.60839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_auc = 0.59716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.59849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -17.96246647834778\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9924\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_auc = 0.57745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 21 and best_val_0_auc = 0.60801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_auc = 0.60311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 25 and best_val_0_auc = 0.62591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_auc = 0.60156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.63184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config 007b97 and rejected config 548676 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 26 and best_val_0_auc = 0.63319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 30 with best_epoch = 27 and best_val_0_auc = 0.61603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'n_a': 144, 'n_d': 145, 'verbose': 0}\n",
      "Cost: 0.397503285151117 | Config ID: 37\n",
      "Here 3\n",
      "Here 1\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "Here 2\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 16 and best_val_0_auc = 0.83557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "2024-08-13 14:19:48,540 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56898 remote=tcp://127.0.0.1:41437>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "params_dict_Tab = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    class TabWrapper(BaseEstimator):\n",
    "        def __init__(self, n_d=64, n_a=64, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax', verbose=0):\n",
    "            self.n_d = n_d\n",
    "            self.n_a = n_a\n",
    "            self.n_steps = n_steps\n",
    "            self.gamma = gamma\n",
    "            self.n_independent = n_independent\n",
    "            self.n_shared = n_shared\n",
    "            self.seed = seed\n",
    "            self.optimizer_fn = optimizer_fn\n",
    "            self.optimizer_params = optimizer_params\n",
    "            self.scheduler_params = scheduler_params\n",
    "            self.scheduler_fn = scheduler_fn\n",
    "            self.mask_type = mask_type\n",
    "            self.verbose = 0\n",
    "            self.model = TabNetClassifier(n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, gamma=self.gamma, n_independent=self.n_independent, n_shared=self.n_shared, seed=self.seed, optimizer_fn=self.optimizer_fn, optimizer_params=self.optimizer_params, scheduler_params=self.scheduler_params, scheduler_fn=self.scheduler_fn, mask_type=self.mask_type, verbose=self.verbose)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_d = Integer(\"n_d\", (4, 256), default=64)\n",
    "            n_a = Integer(\"n_a\", (4, 256), default=64)\n",
    "            # n_steps = Integer(\"n_steps\", (3, 10), default=5)\n",
    "            # gamma = Float(\"gamma\", (0.9, 2.0), default=1.3)\n",
    "            # n_independent = Integer(\"n_independent\", (1, 10), default=2)\n",
    "            # n_shared = Integer(\"n_shared\", (1, 10), default=2)\n",
    "            # seed = Integer(\"seed\", (0, 1000), default=317)\n",
    "            # optimizer_fn = Categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.AdamW], default=torch.optim.Adam)\n",
    "            # scheduler_fn = Categorical(\"scheduler_fn\", [torch.optim.lr_scheduler.StepLR, torch.optim.lr_scheduler.MultiStepLR], default=torch.optim.lr_scheduler.StepLR)\n",
    "            # mask_type = Categorical(\"mask_type\", ['sparsemax', 'entmax'], default='entmax')\n",
    "            verbose = Categorical(\"verbose\", [0], default=0)\n",
    "            cs.add_hyperparameters([n_d, n_a, verbose])\n",
    "            # cs.add_hyperparameters([n_d, n_a, n_steps, gamma, n_independent, n_shared, seed, optimizer_fn, scheduler_fn, mask_type])\n",
    "            return cs\n",
    "\n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float:\n",
    "            config = dict(config)\n",
    "            self.model.set_params(**config)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], patience=10)\n",
    "            preds = self.model.predict(X_val)\n",
    "            score = accuracy_score(y_val, preds)\n",
    "            return 1 - score    \n",
    "\n",
    "    @timeout(3900)\n",
    "    def main():\n",
    "        start_time = time.time()\n",
    "        print(\"Here 1\")\n",
    "        Tab = TabWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                Tab.configspace,\n",
    "                walltime_limit=1800,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_30mins_Tab/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "            \n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                Tab.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "            print(\"Here 2\")\n",
    "\n",
    "            print(\"optimizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_Tab[dataset_names[i]] = best_params\n",
    "\n",
    "            print(\"Here 3\")\n",
    "            incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "            incumbent_run_id = incumbent.config_id\n",
    "\n",
    "            print(f\"Parameters: {best_params}\")\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            # if time.time() - start_time > 60:\n",
    "            #     break\n",
    "\n",
    "            default_cost = smac.validate(Tab.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "        with open(\"SmacResults/TabNet_results.json\", \"w\") as f:\n",
    "            for dataset_name, params in params_dict_Tab.items():\n",
    "                f.write(f\"{dataset_name}: {params}\\n\")\n",
    "\n",
    "        # profiler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, params in params_dict_Tab.items():\n",
    "    print(dataset_name, \":\", params)\n",
    "\n",
    "# with open(\"SmacResults/TabNet_results.json\", \"w\") as f:\n",
    "#     # for dataset_name, params in params_dict_Tab.items():\n",
    "#     #     f.write(f\"{dataset_name}: {params}\\n\")\n",
    "#     f.write(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Kernel",
   "language": "python",
   "name": "neurodata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
