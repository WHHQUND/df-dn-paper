{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import *\n",
    "import warnings\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import librosa\n",
    "import time\n",
    "import re\n",
    "from timeout_decorator import timeout\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ConfigSpace import (\n",
    "    Categorical,\n",
    "    Configuration,\n",
    "    ConfigurationSpace,\n",
    "    EqualsCondition,\n",
    "    Float,\n",
    "    InCondition,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt.callbacks import DeadlineStopper\n",
    "\n",
    "from smac import MultiFidelityFacade as MFFacade\n",
    "from smac import Scenario\n",
    "from smac.facade import AbstractFacade\n",
    "from smac.intensifier.hyperband import Hyperband\n",
    "from smac.intensifier.successive_halving import SuccessiveHalving\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as trans\n",
    "import re\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_combination = [20, 100, 180, 260, 340, 400]\n",
    "dataset_indices_max = 16\n",
    "max_shape_to_run = 10000\n",
    "alpha_range_nn = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.8, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][_api_calls.py:103] Starting [get] request for the URL https://www.openml.org/api/v1/xml/study/337\n",
      "[INFO][_api_calls.py:115] 3.7455928s taken for [get] request for the URL https://www.openml.org/api/v1/xml/study/337\n",
      "[INFO][dataset.py:555] pickle write credit\n",
      "[INFO][dataset.py:555] pickle write electricity\n",
      "[INFO][dataset.py:555] pickle write covertype\n",
      "[INFO][dataset.py:555] pickle write pol\n",
      "[INFO][dataset.py:555] pickle write house_16H\n",
      "[INFO][dataset.py:555] pickle write MagicTelescope\n",
      "[INFO][dataset.py:555] pickle write bank-marketing\n",
      "[INFO][dataset.py:555] pickle write MiniBooNE\n",
      "[INFO][dataset.py:555] pickle write Higgs\n",
      "[INFO][dataset.py:555] pickle write eye_movements\n",
      "[INFO][dataset.py:555] pickle write Diabetes130US\n",
      "[INFO][dataset.py:555] pickle write jannis\n",
      "[INFO][dataset.py:555] pickle write default-of-credit-card-clients\n",
      "[INFO][dataset.py:555] pickle write Bioresponse\n",
      "[INFO][dataset.py:555] pickle write california\n",
      "[INFO][dataset.py:555] pickle write heloc\n",
      "\n",
      "\n",
      "Current Dataset:  0\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  1\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  2\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  3\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  4\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  5\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  6\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  7\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  8\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  9\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  10\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  11\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  12\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  13\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  14\n",
      "No string columns to encode\n",
      "X scalered\n",
      "\n",
      "\n",
      "Current Dataset:  15\n",
      "No string columns to encode\n",
      "X scalered\n"
     ]
    }
   ],
   "source": [
    "dataset_indices = list(range(dataset_indices_max))\n",
    "dict_data_indices = {dataset_ind: {} for dataset_ind in dataset_indices}\n",
    "encode_cnt = 0\n",
    "\n",
    "X_data_list = []\n",
    "y_data_list = []\n",
    "dataset_names = []\n",
    "def import_datasets():\n",
    "    SUITE_ID = [337]\n",
    "    for i in SUITE_ID:\n",
    "        benchmark_suite = openml.study.get_suite(i)\n",
    "        for task_id in benchmark_suite.tasks:  # iterate over all tasks\n",
    "            task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "            dataset = task.get_dataset()\n",
    "            X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "                dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "            )   \n",
    "\n",
    "            # ### Covert labels to numerical values\n",
    "            # le = LabelEncoder()\n",
    "            # y_encoded = le.fit_transform(y)\n",
    "            # y = pd.DataFrame(y_encoded)\n",
    "\n",
    "            X_data_list.append(X)\n",
    "            y_data_list.append(y)\n",
    "            dataset_names.append(dataset.name)\n",
    "\n",
    "            # print(\" \")\n",
    "            # print(\" SUITE_ID:\", i)\n",
    "            # print(\"X_data_list length:\", len(X_data_list))\n",
    "            # print(\" \")\n",
    "    \n",
    "import_datasets()\n",
    "\n",
    "train_x_list = X_data_list.copy()\n",
    "train_y_list = y_data_list.copy()\n",
    "\n",
    "for dataset_index, dataset in enumerate(dataset_indices):\n",
    "    print(\"\\n\\nCurrent Dataset: \", dataset)\n",
    "\n",
    "    X = X_data_list[dataset]\n",
    "    y = y_data_list[dataset]\n",
    "\n",
    "    if X.shape[0] > max_shape_to_run:\n",
    "        X, y = sample_large_datasets(X, y)\n",
    "    \n",
    "    np.random.seed(dataset_index)\n",
    "    dict_data_indices = find_indices_train_val_test(\n",
    "        X.shape[0], dict_data_indices=dict_data_indices, dataset_ind=dataset_index\n",
    "    )\n",
    "    train_indices = dict_data_indices[dataset_index][\"train\"]\n",
    "    val_indices = dict_data_indices[dataset_index][\"val\"]\n",
    "\n",
    "    ### Covert labels to numerical values\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    # y = pd.DataFrame(y_encoded)\n",
    "    y = y_encoded\n",
    "\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    # print(X.dtypes)\n",
    "\n",
    "    ### Convert categories features to numerical features\n",
    "    # print(\"X shape: \", X.shape)\n",
    "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    if len(categorical_columns) > 0:\n",
    "        X_encoded_strings = encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "        X = np.hstack((X[numeric_columns].values, X_encoded_strings))\n",
    "        print(\"Encoded\", len(categorical_columns), \" columns\")\n",
    "        encode_cnt += 1\n",
    "    else:\n",
    "        print(\"No string columns to encode\")\n",
    "    \n",
    "    # print(\"X_encoded shape: \", X.shape)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    # X = pd.DataFrame(X)\n",
    "    train_x_list[dataset] = X\n",
    "    train_y_list[dataset] = y\n",
    "    print(\"X scalered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_list[0]\n",
    "train_y = train_y_list[0]\n",
    "num_class = len(np.unique(train_y))\n",
    "print(np.unique(train_y))\n",
    "print(type(train_x))\n",
    "print(type(train_y))\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(train_y.flatten().shape)\n",
    "\n",
    "# categorical_columns = train_x.select_dtypes(include=['category']).columns\n",
    "\n",
    "# if not categorical_columns.empty:\n",
    "#     print(\"There are categorical columns:\", categorical_columns)\n",
    "# else:\n",
    "#     print(\"No categorical columns found.\")\n",
    "\n",
    "# print(train_x.head(5))\n",
    "# print(train_x['day'].cat.categories.is_numeric())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, random_state=317)\n",
    "XGBT = xgb.XGBClassifier(n_estimators=100, random_state=317)\n",
    "TabNet = TabNetClassifier(n_d=8, n_a=8, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='binary:logistic', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.seed= seed\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.colsample_bynode = colsample_bynode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.objective = objective\n",
    "        # self.num_class = num_class\n",
    "        self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2, 21), default=2)\n",
    "        min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "        gamma = Float(\"gamma\", (0.1, 1.0), default=0.1)\n",
    "        subsample = Float(\"subsample\", (0.5, 1.0), default=0.8)\n",
    "        colsample_bytree = Float(\"colsample_bytree\", (0.3, 1.0), default=0.6)\n",
    "        colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1.0), default=0.6)\n",
    "        colsample_bynode = Float(\"colsample_bynode\", (0.3, 1.0), default=0.6)\n",
    "        learning_rate = Float(\"learning_rate\", (0.001, 0.3), default=0.1)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        # print(\"X shape: \", X.shape)\n",
    "        # print(\"y shape: \", y.shape)\n",
    "        self.model.fit(X, y)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = accuracy_score(y, preds)\n",
    "        \n",
    "        return 1 - scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    GBT = XGBTWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            GBT.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_XGBT\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            GBT.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1550 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1750 trials.\n",
      "[INFO][smbo.py:319] Finished 1800 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -3.8210787773132324\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 7911\n",
      "Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2936068843275964, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Cost: 0.0 | Config ID: 1\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1300 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1750 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -8.220604658126831\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8200\n",
      "Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Cost: 0.0 | Config ID: 15\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -3.0204875469207764\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8833\n",
      "Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Cost: 0.0 | Config ID: 15\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 14:55:29,348 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55530 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,395 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55534 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,410 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55536 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,438 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55532 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,439 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55528 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,462 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55538 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,477 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55540 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:29,503 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55542 remote=tcp://127.0.0.1:34863>: Stream is closed\n",
      "2024-08-29 14:55:33,447 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,449 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,499 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,572 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,594 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,636 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,638 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:33,732 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2024-08-29 14:55:34,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40933', name: 6, status: init, memory: 0, processing: 0>\n",
      "2024-08-29 14:55:34,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40933\n",
      "2024-08-29 14:55:34,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53662\n",
      "2024-08-29 14:55:35,220 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,223 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,224 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,225 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,226 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,228 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 19.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,346 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:35,348 - distributed.core - INFO - Event loop was unresponsive in Nanny for 19.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 14:55:40,899 - distributed.scheduler - INFO - Receive client connection: Client-25aa34a0-6638-11ef-87b1-246e9624e1b0\n",
      "2024-08-29 14:55:40,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 15:00:03,774 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,776 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,777 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,778 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,779 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,780 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:03,792 - distributed.core - INFO - Event loop was unresponsive in Nanny for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:04,187 - distributed.core - INFO - Event loop was unresponsive in Nanny for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:00:04,188 - distributed.core - INFO - Event loop was unresponsive in Nanny for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config e5fd08 as new incumbent because there are no incumbents yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 15:04:21,860 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:21,863 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:21,864 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:21,866 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:21,867 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:22,063 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:22,066 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:22,068 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:04:22,070 - distributed.core - INFO - Event loop was unresponsive in Nanny for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,345 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,347 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,348 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,349 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,350 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,352 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,353 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,355 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "2024-08-29 15:09:11,356 - distributed.core - INFO - Event loop was unresponsive in Nanny for 3.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -5.700629711151123\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9260\n",
      "Parameters: {'colsample_bylevel': 0.7121177269792016, 'colsample_bynode': 0.7961219315098178, 'colsample_bytree': 0.8462050819267193, 'gamma': 0.4596996785735403, 'learning_rate': 0.2796283473545603, 'max_depth': 16, 'min_child_weight': 4, 'n_estimators': 1200, 'subsample': 0.6415111175688668}\n",
      "Cost: 0.0 | Config ID: 15\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 15:28:49,478 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:53680 remote=tcp://127.0.0.1:41261>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config caf58f and rejected config af1315 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 20ceac and rejected config caf58f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config fcab1c and rejected config 20ceac as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9d3a9c and rejected config fcab1c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 55e7a7 and rejected config 9d3a9c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 8146d9 and rejected config 55e7a7 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config afb9ab and rejected config 8146d9 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1300 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1450 trials.\n",
      "[INFO][smbo.py:319] Finished 1550 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -3.203855276107788\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8252\n",
      "Parameters: {'colsample_bylevel': 0.9776210490834212, 'colsample_bynode': 0.8775042258445613, 'colsample_bytree': 0.7542663441024107, 'gamma': 0.10330164670010054, 'learning_rate': 0.2588301011676219, 'max_depth': 18, 'min_child_weight': 1, 'n_estimators': 1003, 'subsample': 0.6477008086095855}\n",
      "Cost: 0.0008000000000000229 | Config ID: 112\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 750 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1300 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1550 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1750 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.3697690963745117\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8196\n",
      "Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Cost: 0.0 | Config ID: 15\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config ed1799 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config af1315 and rejected config ed1799 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 52b1df and rejected config af1315 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c8a3ef and rejected config 52b1df as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 3c96c2 and rejected config c8a3ef as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config fc0235 and rejected config 3c96c2 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 85375e and rejected config fc0235 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 719f4a and rejected config 85375e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 4ceafc and rejected config 719f4a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config cdf6b9 and rejected config 4ceafc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9fe2a0 and rejected config cdf6b9 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2d209e and rejected config 9fe2a0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9c5339 and rejected config 2d209e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 750 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1300 trials.\n",
      "[INFO][smbo.py:319] Finished 1550 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1650 trials.\n",
      "[INFO][smbo.py:319] Finished 1750 trials.\n",
      "[INFO][smbo.py:319] Finished 1850 trials.\n",
      "[INFO][smbo.py:319] Finished 1950 trials.\n",
      "[INFO][smbo.py:319] Finished 1950 trials.\n",
      "[INFO][smbo.py:319] Finished 1950 trials.\n",
      "[INFO][smbo.py:319] Finished 2100 trials.\n",
      "[INFO][smbo.py:319] Finished 2200 trials.\n",
      "[INFO][smbo.py:319] Finished 2350 trials.\n",
      "[INFO][smbo.py:319] Finished 2350 trials.\n",
      "[INFO][smbo.py:319] Finished 2350 trials.\n",
      "[INFO][smbo.py:319] Finished 2350 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.07179021835327148\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 7638\n",
      "Parameters: {'colsample_bylevel': 0.7674631624672195, 'colsample_bynode': 0.7210852233611122, 'colsample_bytree': 0.9995844882631293, 'gamma': 0.10387091870310648, 'learning_rate': 0.28775258862088626, 'max_depth': 21, 'min_child_weight': 1, 'n_estimators': 1106, 'subsample': 0.8019369114916215}\n",
      "Cost: 0.19069673781715668 | Config ID: 221\n"
     ]
    }
   ],
   "source": [
    "params_dict_XGBT = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    \n",
    "    class XGBTWrapper(BaseEstimator):\n",
    "        def __init__(self, n_estimators=100, max_depth=2, seed= 317, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.5, learning_rate=0.1, objective='binary:logistic', colsample_bylevel=0.5, colsample_bynode=0.5):\n",
    "            self.n_estimators = n_estimators\n",
    "            self.max_depth = max_depth\n",
    "            self.seed= seed\n",
    "            self.min_child_weight = min_child_weight\n",
    "            self.gamma = gamma\n",
    "            self.subsample = subsample\n",
    "            self.colsample_bytree = colsample_bytree\n",
    "            self.colsample_bylevel = colsample_bylevel\n",
    "            self.colsample_bynode = colsample_bynode\n",
    "            self.learning_rate = learning_rate\n",
    "            self.objective = objective\n",
    "            # self.num_class = num_class\n",
    "            self.model = xgb.XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, seed=self.seed, min_child_weight=self.min_child_weight, gamma=self.gamma, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, colsample_bynode=self.colsample_bynode ,learning_rate=self.learning_rate, objective=self.objective)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "            max_depth = Integer(\"max_depth\", (2, 21), default=2)\n",
    "            min_child_weight = Integer(\"min_child_weight\", (1, 10), default=1)\n",
    "            gamma = Float(\"gamma\", (0.1, 1.0), default=0.1)\n",
    "            subsample = Float(\"subsample\", (0.5, 1.0), default=0.8)\n",
    "            colsample_bytree = Float(\"colsample_bytree\", (0.3, 1.0), default=0.6)\n",
    "            colsample_bylevel = Float(\"colsample_bylevel\", (0.3, 1.0), default=0.6)\n",
    "            colsample_bynode = Float(\"colsample_bynode\", (0.3, 1.0), default=0.6)\n",
    "            learning_rate = Float(\"learning_rate\", (0.001, 0.3), default=0.1)\n",
    "            cs.add_hyperparameters([n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, learning_rate])\n",
    "            return cs\n",
    "        \n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "            config = dict(config)  \n",
    "            self.model.set_params(**config)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            # print(\"X shape: \", X.shape)\n",
    "            # print(\"y shape: \", y.shape)\n",
    "            self.model.fit(X, y)\n",
    "            preds = self.model.predict(X)\n",
    "            scores = accuracy_score(y, preds)\n",
    "            \n",
    "            return 1 - scores\n",
    "\n",
    "    # @timeout(900)\n",
    "    def main():\n",
    "        GBT = XGBTWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                GBT.configspace,\n",
    "                walltime_limit=1800,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_30mins_XGBT_337/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                GBT.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "            print(\"optimizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_XGBT[dataset_names[i]] = best_params\n",
    "\n",
    "            incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "            incumbent_run_id = incumbent.config_id\n",
    "\n",
    "            print(f\"Parameters: {best_params}\")\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            default_cost = smac.validate(GBT.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "\n",
    "        # profiler.disable()\n",
    "        # profiler.print_stats()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: electricity, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.7521258791466592, 'colsample_bytree': 0.8542075266578653, 'gamma': 0.17841636973138664, 'learning_rate': 0.2936068843275964, 'max_depth': 14, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: eye_movements, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: covertype, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: albert, Best Parameters: {'colsample_bylevel': 0.7121177269792016, 'colsample_bynode': 0.7961219315098178, 'colsample_bytree': 0.8462050819267193, 'gamma': 0.4596996785735403, 'learning_rate': 0.2796283473545603, 'max_depth': 16, 'min_child_weight': 4, 'n_estimators': 1200, 'subsample': 0.6415111175688668}\n",
      "Dataset: default-of-credit-card-clients, Best Parameters: {'colsample_bylevel': 0.9776210490834212, 'colsample_bynode': 0.8775042258445613, 'colsample_bytree': 0.7542663441024107, 'gamma': 0.10330164670010054, 'learning_rate': 0.2588301011676219, 'max_depth': 18, 'min_child_weight': 1, 'n_estimators': 1003, 'subsample': 0.6477008086095855}\n",
      "Dataset: road-safety, Best Parameters: {'colsample_bylevel': 0.6841694527491273, 'colsample_bynode': 0.8150860310502579, 'colsample_bytree': 0.8655054784519463, 'gamma': 0.2573783935536148, 'learning_rate': 0.28815400057691976, 'max_depth': 13, 'min_child_weight': 3, 'n_estimators': 780, 'subsample': 0.679753950286893}\n",
      "Dataset: compas-two-years, Best Parameters: {'colsample_bylevel': 0.7674631624672195, 'colsample_bynode': 0.7210852233611122, 'colsample_bytree': 0.9995844882631293, 'gamma': 0.10387091870310648, 'learning_rate': 0.28775258862088626, 'max_depth': 21, 'min_child_weight': 1, 'n_estimators': 1106, 'subsample': 0.8019369114916215}\n"
     ]
    }
   ],
   "source": [
    "for dataset, params in params_dict_XGBT.items():\n",
    "    print(f\"Dataset: {dataset}, Best Parameters: {params}\")\n",
    "    # print(f\"Best Parameters: {params}\")\n",
    "with open(\"SmacResults/334/XGBT_params.json\", 'w') as f:\n",
    "    json.dump(params_dict_XGBT, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features   \n",
    "        self.criterion = \"gini\"\n",
    "        self.max_samples = max_samples\n",
    "        self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "        max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "        min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "        min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "        criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "        max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "        max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "        return cs\n",
    "    \n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "        config = dict(config)  \n",
    "        if config['max_features'] == 'None':\n",
    "            config['max_features'] = None\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        self.model.fit(X, y)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = accuracy_score(y, preds)\n",
    "\n",
    "        return 1 - scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    RF = RFWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            RF.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_RF\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            RF.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 00b4d2 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config 00b4d2 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2d3e1f and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 934f99 and rejected config 2d3e1f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config cad4de and rejected config 934f99 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config c7e313 and rejected config cad4de as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config fc3351 and rejected config c7e313 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config b9912b and rejected config fc3351 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config ac4dea and rejected config b9912b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0a7e75 and rejected config ac4dea as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config dd3d1e and rejected config 0a7e75 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 348f3b and rejected config dd3d1e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config ca4493 and rejected config 348f3b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config af8221 and rejected config ca4493 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 20df49 and rejected config af8221 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 67c482 and rejected config 20df49 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -10.450206279754639\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9536\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9882603955188162, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 773}\n",
      "Cost: 0.00029999999999996696 | Config ID: 285\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 18:26:47,767 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:37158 remote=tcp://127.0.0.1:39099>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 38f80e and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 2155eb and rejected config 38f80e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config de557f and rejected config 2155eb as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0e443c and rejected config de557f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config dffe9d and rejected config 0e443c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -4.443481683731079\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9416\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.8672333781089705, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 147}\n",
      "Cost: 0.0 | Config ID: 200\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 8b4a50 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 73c6d7 and rejected config 8b4a50 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a296ab and rejected config 73c6d7 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config be238d and rejected config a296ab as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 1317cc and rejected config be238d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c425ee and rejected config 1317cc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 251c4a and rejected config c425ee as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2cd763 and rejected config 251c4a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 1763f4 and rejected config 2cd763 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config dcd09b and rejected config 1763f4 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0c9f51 and rejected config dcd09b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 2f3582 and rejected config 0c9f51 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.7484328746795654\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9556\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.748448206138323, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1161}\n",
      "Cost: 0.0038000000000000256 | Config ID: 299\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config efd348 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config f2f0c6 and rejected config efd348 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 148e3a and rejected config f2f0c6 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 13eae0 and rejected config 148e3a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0c0bee and rejected config 13eae0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config df0e4f and rejected config 0c0bee as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 6dc80c and rejected config df0e4f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 7c71d0 and rejected config 6dc80c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 7650e2 and rejected config 7c71d0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 1bedf8 and rejected config 7650e2 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 650 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 750 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 850 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.89548397064209\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9056\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9237313997922102, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 381}\n",
      "Cost: 0.0 | Config ID: 95\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c03538 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config dc824d and rejected config c03538 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 943d46 and rejected config dc824d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config c0aba8 and rejected config 943d46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 6e5e2d and rejected config c0aba8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -7.232992649078369\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9687\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 20, 'max_features': 'None', 'max_samples': 0.9878469707940801, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 116}\n",
      "Cost: 0.0 | Config ID: 140\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 13eae0 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 67dff3 and rejected config 13eae0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 4aefd3 and rejected config 67dff3 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 357126 and rejected config 4aefd3 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 515fff and rejected config 357126 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 2c9016 and rejected config 515fff as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0ad88f and rejected config 2c9016 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c2c4ce and rejected config 0ad88f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 58dd71 and rejected config c2c4ce as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0363c6 and rejected config 58dd71 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 435a5a and rejected config 0363c6 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 97321e and rejected config 435a5a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 521bc4 and rejected config 97321e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 157ec3 and rejected config 521bc4 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9ab998 and rejected config 157ec3 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config df44ce and rejected config 9ab998 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2ed9cd and rejected config df44ce as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.84456729888916\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9575\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9884503078212304, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 151}\n",
      "Cost: 0.0 | Config ID: 250\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 21:08:58,361 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:51912 remote=tcp://127.0.0.1:45013>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 13eae0 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 19c9cf and rejected config 13eae0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a9343b and rejected config 19c9cf as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 12f08a and rejected config a9343b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c497d1 and rejected config 12f08a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 9e45ec and rejected config c497d1 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 57f081 and rejected config 9e45ec as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config acfaa5 and rejected config 57f081 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 02e645 and rejected config acfaa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 650f60 and rejected config 02e645 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2d2efa and rejected config 650f60 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config abfe78 and rejected config 2d2efa as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 6b872c and rejected config abfe78 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 750 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 850 trials.\n",
      "[INFO][smbo.py:319] Finished 900 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1050 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1150 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:319] Finished 1200 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -3.9967854022979736\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8762\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9882139602439883, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 206}\n",
      "Cost: 0.0 | Config ID: 225\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 21:40:11,300 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:49558 remote=tcp://127.0.0.1:33623>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config d990f5 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config bad242 and rejected config d990f5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config bf959b and rejected config bad242 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 3b5465 and rejected config bf959b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -170.29263949394226\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9842\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'log2', 'max_samples': 0.9894598524157128, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 273}\n",
      "Cost: 9.999999999998899e-05 | Config ID: 82\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 99860e and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 3f6d77 and rejected config 99860e as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config bafa6c and rejected config 3f6d77 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 02e912 and rejected config bafa6c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config c58c04 and rejected config 02e912 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config f0d851 and rejected config c58c04 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.6256823539733887\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9592\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9778409545532449, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 116}\n",
      "Cost: 0.0 | Config ID: 151\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 80c3ec and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config b2b85d and rejected config 80c3ec as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config faba15 and rejected config b2b85d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 56b9d1 and rejected config faba15 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2901a6 and rejected config 56b9d1 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 34abeb and rejected config 2901a6 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -1.3014099597930908\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9740\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 19, 'max_features': 'log2', 'max_samples': 0.982196694630582, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 476}\n",
      "Cost: 0.0 | Config ID: 117\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 13eae0 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config 13eae0 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c420eb and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 481c04 and rejected config c420eb as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 4e7d03 and rejected config 481c04 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a41ad1 and rejected config 4e7d03 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config f3ed3f and rejected config a41ad1 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 11c836 and rejected config f3ed3f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 07ef71 and rejected config 11c836 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 96162d and rejected config 07ef71 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 36d4af and rejected config 96162d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config b033b5 and rejected config 36d4af as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config d29c4b and rejected config b033b5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 251921 and rejected config d29c4b as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 700 trials.\n",
      "[INFO][smbo.py:319] Finished 750 trials.\n",
      "[INFO][smbo.py:319] Finished 800 trials.\n",
      "[INFO][smbo.py:319] Finished 950 trials.\n",
      "[INFO][smbo.py:319] Finished 1000 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1100 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1350 trials.\n",
      "[INFO][smbo.py:319] Finished 1450 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.4379894733428955\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 8484\n",
      "Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9884102562448526, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 258}\n",
      "Cost: 0.02290000000000003 | Config ID: 451\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 00:04:22,519 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:52802 remote=tcp://127.0.0.1:42223>: Stream is closed\n",
      "2024-08-30 00:04:22,525 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:52790 remote=tcp://127.0.0.1:42223>: Stream is closed\n",
      "2024-08-30 00:04:22,524 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/worker.py\", line 1252, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 452, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/utils_comm.py\", line 431, in retry\n",
      "    return await coro()\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1395, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/core.py\", line 1154, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:52806 remote=tcp://127.0.0.1:42223>: Stream is closed\n",
      "2024-08-30 00:04:24,413 - distributed.scheduler - INFO - Scheduler closing due to unknown reason...\n",
      "2024-08-30 00:04:24,420 - distributed.scheduler - INFO - Scheduler closing all comms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config dc1c4d and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config dc1c4d as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 17fd54 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 83ffe9 and rejected config 17fd54 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -37.37585806846619\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9857\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'None', 'max_samples': 0.9891662283054089, 'min_samples_leaf': 3, 'min_samples_split': 7, 'n_estimators': 154}\n",
      "Cost: 0.0033999999999999586 | Config ID: 83\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 47caf5 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 4eb176 and rejected config 47caf5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config e1a198 and rejected config 4eb176 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 5730bc and rejected config e1a198 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 248c4c and rejected config 5730bc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 0a87d9 and rejected config 248c4c as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 9cdd43 and rejected config 0a87d9 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 8a83cc and rejected config 9cdd43 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a606f5 and rejected config 8a83cc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -0.5216498374938965\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9707\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9260907835422265, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 209}\n",
      "Cost: 0.013700000000000045 | Config ID: 220\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 411ea8 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config 411ea8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 602483 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 8e141f and rejected config 602483 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 41e429 and rejected config 8e141f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 6ed0b8 and rejected config 41e429 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 4b5c39 and rejected config 6ed0b8 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 5ee29a and rejected config 4b5c39 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config a56daa and rejected config 5ee29a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -49.265235900878906\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9787\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9808995840436561, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 328}\n",
      "Cost: 0.0 | Config ID: 134\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config eac2cc and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 65b156 and rejected config eac2cc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 6079a5 and rejected config 65b156 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config cbc46c and rejected config 6079a5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][smbo.py:319] Finished 250 trials.\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:319] Finished 550 trials.\n",
      "[INFO][smbo.py:319] Finished 600 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -6.273735284805298\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9320\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9836204130517437, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 114}\n",
      "Cost: 0.0 | Config ID: 101\n",
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimiizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][abstract_intensifier.py:515] Added config 20baa5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config e7fe46 and rejected config 20baa5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][smbo.py:319] Finished 50 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config a60495 and rejected config e7fe46 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 200c86 and rejected config a60495 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 2620f2 and rejected config 200c86 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 15b923 and rejected config 2620f2 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 150 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config d90935 and rejected config 15b923 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config c81b44 and rejected config d90935 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config d370b6 and rejected config c81b44 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 200 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 89caeb and rejected config d370b6 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config b5c341 and rejected config 89caeb as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 300 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config c2eeb7 and rejected config b5c341 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 350 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 34193a and rejected config c2eeb7 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 400 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 526743 and rejected config 34193a as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 450 trials.\n",
      "[INFO][abstract_intensifier.py:594] Added config 617317 and rejected config 526743 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][smbo.py:319] Finished 500 trials.\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -2.382100820541382\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9485\n",
      "Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9150853991794056, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 305}\n",
      "Cost: 0.026599999999999957 | Config ID: 364\n"
     ]
    }
   ],
   "source": [
    "params_dict_RF = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    class RFWrapper(BaseEstimator):\n",
    "        def __init__(self, n_estimators=100, max_depth=2, random_state=317, min_samples_split=2, min_samples_leaf=1, max_features=None, criterion=\"gini\", max_samples = 0.5):\n",
    "            self.n_estimators = n_estimators\n",
    "            self.max_depth = max_depth\n",
    "            self.random_state = random_state\n",
    "            self.min_samples_split = min_samples_split\n",
    "            self.min_samples_leaf = min_samples_leaf\n",
    "            self.max_features = max_features   \n",
    "            self.criterion = \"gini\"\n",
    "            self.max_samples = max_samples\n",
    "            self.model = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, criterion=self.criterion, max_features=self.max_features, max_samples=self.max_samples)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_estimators = Integer(\"n_estimators\", (100, 1200), default=100)\n",
    "            max_depth = Integer(\"max_depth\", (2,21), default=2)\n",
    "            min_samples_split = Integer(\"min_samples_split\", (2, 20), default=2)\n",
    "            min_samples_leaf = Integer(\"min_samples_leaf\", (1, 20), default=1)\n",
    "            criterion = Categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"], default=\"gini\")\n",
    "            max_features = Categorical(\"max_features\", [\"sqrt\", \"log2\", \"None\"], default=\"None\")\n",
    "            max_samples = Float(\"max_samples\", (0.1, 0.99), log=True)\n",
    "            cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion, max_features, max_samples])\n",
    "            return cs\n",
    "        \n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float: \n",
    "            config = dict(config)  \n",
    "            if config['max_features'] == 'None':\n",
    "                config['max_features'] = None\n",
    "            self.model.set_params(**config)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            self.model.fit(X, y)\n",
    "            preds = self.model.predict(X)\n",
    "            scores = accuracy_score(y, preds)\n",
    "\n",
    "            return 1 - scores\n",
    "\n",
    "    # @timeout(90)\n",
    "    def main():\n",
    "        RF = RFWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                RF.configspace,\n",
    "                walltime_limit=1800,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_30mins_RF_337/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                RF.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "            print(\"optimiizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_RF[dataset_names[i]] = best_params\n",
    "\n",
    "            incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "            incumbent_run_id = incumbent.config_id\n",
    "\n",
    "            print(f\"Parameters: {best_params}\")\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            default_cost = smac.validate(RF.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "\n",
    "        # profiler.disable()\n",
    "        # profiler.print_stats()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9882603955188162, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 773}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.8672333781089705, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 147}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.748448206138323, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1161}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9237313997922102, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 381}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 20, 'max_features': 'None', 'max_samples': 0.9878469707940801, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 116}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9884503078212304, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 151}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9882139602439883, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 206}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'log2', 'max_samples': 0.9894598524157128, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 273}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9778409545532449, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 116}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'entropy', 'max_depth': 19, 'max_features': 'log2', 'max_samples': 0.982196694630582, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 476}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'gini', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9884102562448526, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 258}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'None', 'max_samples': 0.9891662283054089, 'min_samples_leaf': 3, 'min_samples_split': 7, 'n_estimators': 154}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9260907835422265, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 209}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9808995840436561, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 328}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'entropy', 'max_depth': 21, 'max_features': 'sqrt', 'max_samples': 0.9836204130517437, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 114}\n",
      "Dataset: 15, Best Parameters: {'criterion': 'log_loss', 'max_depth': 21, 'max_features': 'None', 'max_samples': 0.9150853991794056, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 305}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, params in params_dict_RF.items():\n",
    "    print(f\"Dataset: {dataset}, Best Parameters: {params}\")\n",
    "    # print(f\"Best Parameters: {params}\")\n",
    "with open(\"SmacResults/RF_params.json\", 'w') as f:\n",
    "    json.dump(params_dict_RF, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabWrapper(BaseEstimator):\n",
    "    def __init__(self, n_d=64, n_a=64, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), scheduler_params={\"step_size\":50, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax', verbose=0):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.seed = seed\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.model = TabNetClassifier(n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, gamma=self.gamma, n_independent=self.n_independent, n_shared=self.n_shared, seed=self.seed, optimizer_fn=self.optimizer_fn, optimizer_params=self.optimizer_params, scheduler_params=self.scheduler_params, scheduler_fn=self.scheduler_fn, mask_type=self.mask_type)\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_d = Integer(\"n_d\", (4, 256), default=64)\n",
    "        n_a = Integer(\"n_a\", (4, 256), default=64)\n",
    "        # n_steps = Integer(\"n_steps\", (3, 10), default=5)\n",
    "        # gamma = Float(\"gamma\", (0.9, 2.0), default=1.3)\n",
    "        # n_independent = Integer(\"n_independent\", (1, 10), default=2)\n",
    "        # n_shared = Integer(\"n_shared\", (1, 10), default=2)\n",
    "        # seed = Integer(\"seed\", (0, 1000), default=317)\n",
    "        # optimizer_fn = Categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.AdamW], default=torch.optim.Adam)\n",
    "        # scheduler_fn = Categorical(\"scheduler_fn\", [torch.optim.lr_scheduler.StepLR, torch.optim.lr_scheduler.MultiStepLR], default=torch.optim.lr_scheduler.StepLR)\n",
    "        # mask_type = Categorical(\"mask_type\", ['sparsemax', 'entmax'], default='entmax')\n",
    "        cs.add_hyperparameters([n_d, n_a])\n",
    "        # cs.add_hyperparameters([n_d, n_a, n_steps, gamma, n_independent, n_shared, seed, optimizer_fn, scheduler_fn, mask_type])\n",
    "        return cs\n",
    "\n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float:\n",
    "        config = dict(config)\n",
    "        self.model.set_params(**config)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], patience=50)\n",
    "        preds = self.model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        return 1 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(900)\n",
    "def main():\n",
    "    Tab = TabWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            Tab.configspace,\n",
    "            walltime_limit=600,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_10mins_Tab\"),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            Tab.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimiizing\")\n",
    "        print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        print(\"incumbent:\", incumbent)\n",
    "        default_cost = smac.validate(Tab.configspace.get_default_configuration())\n",
    "        print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "        for arrt in dir(smac):\n",
    "            if not arrt.startswith(\"_\"):\n",
    "                print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    print(\"facades:\", facades)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    profiler = LineProfiler()\n",
    "    profiler.add_function(main)\n",
    "    profiler.enable()\n",
    "\n",
    "    main()\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.84879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "params_dict_Tab = {}\n",
    "for i in range(len(train_x_list)):\n",
    "    train_x = train_x_list[i]\n",
    "    train_y = train_y_list[i]\n",
    "    class TabWrapper(BaseEstimator):\n",
    "        def __init__(self, n_d=8, n_a=8, n_steps=5, gamma=1.3, n_independent=2, n_shared=2, seed=317, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-2), mask_type='entmax', verbose=0):\n",
    "            self.n_d = n_d\n",
    "            self.n_a = n_a\n",
    "            self.n_steps = n_steps\n",
    "            self.gamma = gamma\n",
    "            self.n_independent = n_independent\n",
    "            self.n_shared = n_shared\n",
    "            self.seed = seed\n",
    "            self.optimizer_fn = optimizer_fn\n",
    "            self.optimizer_params = optimizer_params\n",
    "            self.mask_type = mask_type\n",
    "            self.verbose = 0\n",
    "            self.model = TabNetClassifier(n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, gamma=self.gamma, n_independent=self.n_independent, n_shared=self.n_shared, seed=self.seed, optimizer_fn=self.optimizer_fn, optimizer_params=self.optimizer_params, mask_type=self.mask_type, verbose=self.verbose)\n",
    "\n",
    "        @property\n",
    "        def configspace(self) -> ConfigurationSpace:\n",
    "            cs = ConfigurationSpace()\n",
    "            n_d = Integer(\"n_d\", (4, 128), default=8)\n",
    "            n_a = Integer(\"n_a\", (4, 128), default=8)\n",
    "            n_steps = Integer(\"n_steps\", (3, 5), default=5)\n",
    "            gamma = Float(\"gamma\", (1, 2.0), default=1.3)\n",
    "            n_independent = Integer(\"n_independent\", (1, 10), default=2)\n",
    "            n_shared = Integer(\"n_shared\", (1, 10), default=2)\n",
    "            optimizer_fn = Categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.AdamW], default=torch.optim.Adam)\n",
    "            # mask_type = Categorical(\"mask_type\", ['sparsemax', 'entmax'], default='entmax')\n",
    "            # seed = Integer(\"seed\", (317), default=317)\n",
    "            verbose = Categorical(\"verbose\", [0], default=0)\n",
    "            cs.add_hyperparameters([n_d, n_a, verbose])\n",
    "            # cs.add_hyperparameters([n_d, n_a, n_steps, gamma, n_independent, n_shared, seed, optimizer_fn, scheduler_fn, mask_type])\n",
    "            return cs\n",
    "\n",
    "        def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float:\n",
    "            # config = dict(config)\n",
    "            # self.model.set_params(**config)\n",
    "            self.model = TabNetClassifier(n_d=config[\"n_d\"], n_a=config[\"n_a\"], verbose=self.verbose)\n",
    "            X = train_x\n",
    "            y = train_y\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], patience=10)\n",
    "            preds = self.model.predict(X_val)\n",
    "            score = accuracy_score(y_val, preds)\n",
    "            return 1 - score    \n",
    "\n",
    "    @timeout(2100)\n",
    "    def main():\n",
    "        start_time = time.time()\n",
    "        print(\"Here 1\")\n",
    "        Tab = TabWrapper()\n",
    "\n",
    "        facades: list[AbstractFacade] = []\n",
    "        for intensifier_object in [Hyperband]:\n",
    "\n",
    "            scenario = Scenario(\n",
    "                Tab.configspace,\n",
    "                walltime_limit=1800,\n",
    "                output_directory=Path(\"smac_hyperband_output_budget_30mins_Tab/\" + dataset_names[i]),\n",
    "                n_trials=10000,\n",
    "                min_budget=100,\n",
    "                max_budget=1000,\n",
    "                n_workers=8,\n",
    "\n",
    "            )\n",
    "            \n",
    "\n",
    "            initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "            intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "            smac = MFFacade(\n",
    "                scenario,\n",
    "                Tab.fit,\n",
    "                initial_design=initial_design,\n",
    "                intensifier=intensifier,\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "            print(\"optimizing\")\n",
    "            # print(type(smac), \"|\", smac)\n",
    "            incumbent = smac.optimize()\n",
    "            best_params = incumbent.get_dictionary()\n",
    "            params_dict_Tab[dataset_names[i]] = best_params\n",
    "\n",
    "            print(\"Here 3\")\n",
    "            incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "            incumbent_run_id = incumbent.config_id\n",
    "\n",
    "            print(f\"Parameters: {best_params}\")\n",
    "            print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "            # if time.time() - start_time > 60:\n",
    "            #     break\n",
    "\n",
    "            default_cost = smac.validate(Tab.configspace.get_default_configuration())\n",
    "            # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "            incumbent_cost = smac.validate(incumbent)\n",
    "            # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "            facades.append(smac)\n",
    "        #     for arrt in dir(smac):\n",
    "        #         if not arrt.startswith(\"_\"):\n",
    "        #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "        # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        # with open('smac_results_2h.txt', \"w\") as f:\n",
    "        #     pass\n",
    "        # profiler = LineProfiler()\n",
    "        # profiler.add_function(main)\n",
    "        # profiler.enable()\n",
    "\n",
    "        main()\n",
    "\n",
    "        break\n",
    "\n",
    "        # profiler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict_Tab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:82] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:147] Using 5 initial design configurations and 0 additional configurations.\n",
      "optimizing\n",
      "[INFO][successive_halving.py:164] Successive Halving uses budget type BUDGETS with eta 3, min budget 100, and max budget 1000.\n",
      "[INFO][successive_halving.py:323] Number of configs in stage:\n",
      "[INFO][successive_halving.py:325] --- Bracket 0: [9, 3, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 1: [5, 1]\n",
      "[INFO][successive_halving.py:325] --- Bracket 2: [3]\n",
      "[INFO][successive_halving.py:327] Budgets in stage:\n",
      "[INFO][successive_halving.py:329] --- Bracket 0: [111.1111111111111, 333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 1: [333.3333333333333, 1000.0]\n",
      "[INFO][successive_halving.py:329] --- Bracket 2: [1000.0]\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n",
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 0 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.47564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.74436\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 47 and best_val_0_auc = 0.71248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.70329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 67 with best_epoch = 59 and best_val_0_auc = 0.69678\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_auc = 0.73954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.71056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.73267\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.73363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.74109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 63 with best_epoch = 58 and best_val_0_auc = 0.64997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.60444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_auc = 0.72871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_auc = 0.70645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.69571\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.53521\n",
      "[INFO][abstract_intensifier.py:515] Added config 58e444 as new incumbent because there are no incumbents yet.\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.70874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 79 and best_val_0_auc = 0.70092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.74032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.74109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config e709d2 and rejected config 58e444 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_auc = 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_auc = 0.73264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.74436\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 77 and best_val_0_auc = 0.72245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74041\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 40 and best_val_0_auc = 0.74461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.74127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74291\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_auc = 0.73656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.74814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_auc = 0.6168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74199\n",
      "[INFO][abstract_intensifier.py:594] Added config af777f and rejected config e709d2 as incumbent because it is not better than the incumbents on 1 instances:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.73126\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.74032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_auc = 0.73264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.74814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config cf35a9 and rejected config af777f as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.74178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 49 and best_val_0_auc = 0.74193\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.74854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.74399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.74753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 40 and best_val_0_auc = 0.74461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.7447\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.7458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.74079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 50 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.74814\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.74175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.72626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.74045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.73941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 53 with best_epoch = 43 and best_val_0_auc = 0.73763\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.7323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.72894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.75158\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.75159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.74032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.73509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_auc = 0.73683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.74175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_intensifier.py:594] Added config fa3488 and rejected config cf35a9 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.74233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.7284\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.72733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.73258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 66 with best_epoch = 57 and best_val_0_auc = 0.7425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.73468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_auc = 0.66163\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_auc = 0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.75158\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.73997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.75124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_auc = 0.74147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.74483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.56766\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_auc = 0.75176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.74366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.74982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_auc = 0.61449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 84 and best_val_0_auc = 0.63854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.73509\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.74233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_auc = 0.70502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.74853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.74938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.74446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.75024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_auc = 0.67237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.75491\n",
      "[INFO][smbo.py:319] Finished 100 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.74414\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_auc = 0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.74832\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.53673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 46 and best_val_0_auc = 0.74147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.74982\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.73827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74864\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.73033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.74704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.75037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.75377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.75024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.72071\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.73301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.74151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74604\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_auc = 0.74375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.73392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.75112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_auc = 0.74978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_auc = 0.75234\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_auc = 0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.7414\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.73614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.75355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_auc = 0.59934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_auc = 0.72407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.73497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_auc = 0.7455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.73723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.73468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_auc = 0.7417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.74704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_auc = 0.74853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.75377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.73416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.74566\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.75037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_auc = 0.74978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.75066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.7414\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_auc = 0.74985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.75208\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.73723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.74844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.75377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.74958\n",
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.73468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.74462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.74928\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.73705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.74202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.7423\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.74566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.75468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.74905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.73672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.74872\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.74698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.74788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.75038\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.74278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.73468\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.73894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.73664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.74853\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.73705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.74958\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.73885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.75468\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.74802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_auc = 0.71179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.74016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.74153\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.75038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.74162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_auc = 0.74875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.75117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.74958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.75073\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.72601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.74108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.73405\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.7462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.75047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.74162\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.73609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.75117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.74707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.69759\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.74868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.50732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.73555\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.74727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:319] Finished 200 trials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_auc = 0.74683\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.74465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.73405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.75117\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.73678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.75454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.74708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.73991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.73821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_auc = 0.7419\n",
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_val_0_auc = 0.72846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_auc = 0.74594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.74707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.74986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.73041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.74465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.75203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.74914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.7441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_auc = 0.73901\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.73559\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.74815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74399\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_auc = 0.7431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.74068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.73337\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.74388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.74708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.75209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_auc = 0.72668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: -11.277779817581177\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 9762\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.74307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.75255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.74893\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.74399\n",
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_auc = 0.73159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziyan/miniconda3/envs/NeuroData/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here 3\n",
      "Parameters: {'batch_size': 178, 'lr': 0.07833662528769865, 'max_epochs': 140, 'n_a': 61, 'n_d': 5, 'solver': 'adam'}\n",
      "Cost: 0.2997987927565392 | Config ID: 48\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.74325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in range(len(train_x_list)):\n",
    "train_x = train_x_list[6]\n",
    "train_y = train_y_list[6]\n",
    "class TabWrapper(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.model = TabNetClassifier(device_name='cuda' if torch.cuda.is_available() else 'cpu', verbose=0)\n",
    "        self.max_epochs = 100\n",
    "        self.batch_size = 1024\n",
    "        self.optimizer_fn = torch.optim.Adam\n",
    "        self.optimizer_params = dict(lr=1e-2)\n",
    "        self.n_d = 8\n",
    "        self.n_a = 8\n",
    "\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        cs = ConfigurationSpace()\n",
    "        n_d = Integer(\"n_d\", (4, 128), default=8)\n",
    "        n_a = Integer(\"n_a\", (4, 128), default=8)\n",
    "        max_epochs = Integer(\"max_epochs\", (50, 300), default=100)\n",
    "        batch_size = Integer(\"batch_size\", (64, 2048), default=1024)\n",
    "        solver = Categorical(\"solver\", [\"sgd\", \"adam\"])\n",
    "        # optimizer_fn = Categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.SGD], default=torch.optim.Adam)\n",
    "        lr = Float(\"lr\", (0.001, 0.1), default=1e-2)\n",
    "        cs.add_hyperparameters([n_d, n_a, max_epochs, batch_size, solver, lr])\n",
    "        return cs\n",
    "\n",
    "    def fit(self, config: Configuration, seed: int = 0, budget: int = 250) -> float:\n",
    "        self.lr = config.get(\"lr\")\n",
    "        self.optimizer_params.update({\"lr\": self.lr})\n",
    "        self.n_d = config.get(\"n_d\")\n",
    "        self.n_a = config.get(\"n_a\")\n",
    "        self.max_epochs = config.get(\"max_epochs\")\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "        solver_name = config.get(\"solver\")\n",
    "        if solver_name == \"adam\":\n",
    "            self.optimizer_fn = torch.optim.Adam\n",
    "        else:\n",
    "            self.optimizer_fn = torch.optim.SGD\n",
    "        self.model = TabNetClassifier(n_d=self.n_d, n_a=self.n_a, optimizer_params=self.optimizer_params, optimizer_fn=self.optimizer_fn, verbose=0)\n",
    "        X = train_x\n",
    "        y = train_y\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=317)\n",
    "        self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], max_epochs=self.max_epochs, patience=10, batch_size=self.batch_size)\n",
    "        preds = self.model.predict(X_val)\n",
    "        score = accuracy_score(y_val, preds)\n",
    "        return 1 - score\n",
    "        # return None\n",
    "\n",
    "\n",
    "@timeout(2100)\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    Tab = TabWrapper()\n",
    "\n",
    "    facades: list[AbstractFacade] = []\n",
    "    for intensifier_object in [Hyperband]:\n",
    "\n",
    "        scenario = Scenario(\n",
    "            Tab.configspace,\n",
    "            walltime_limit=1800,\n",
    "            output_directory=Path(\"smac_hyperband_output_budget_30mins_Tab_337/\" + dataset_names[6]),\n",
    "            n_trials=10000,\n",
    "            min_budget=100,\n",
    "            max_budget=1000,\n",
    "            n_workers=8,\n",
    "\n",
    "        )\n",
    "        \n",
    "\n",
    "        initial_design = MFFacade.get_initial_design(scenario, n_configs=5)\n",
    "        intensifier = intensifier_object(scenario, incumbent_selection=\"highest_budget\")\n",
    "\n",
    "        smac = MFFacade(\n",
    "            scenario,\n",
    "            Tab.fit,\n",
    "            initial_design=initial_design,\n",
    "            intensifier=intensifier,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        print(\"optimizing\")\n",
    "        # print(type(smac), \"|\", smac)\n",
    "        incumbent = smac.optimize()\n",
    "        best_params = incumbent.get_dictionary()\n",
    "        params_dict_Tab[dataset_names[6]] = best_params\n",
    "\n",
    "        print(\"Here 3\")\n",
    "        incumbent_cost = smac.runhistory.get_cost(incumbent)\n",
    "        incumbent_run_id = incumbent.config_id\n",
    "\n",
    "        print(f\"Parameters: {best_params}\")\n",
    "        print(f\"Cost: {incumbent_cost} | Config ID: {incumbent_run_id}\")\n",
    "\n",
    "        # if time.time() - start_time > 60:\n",
    "        #     break\n",
    "\n",
    "        default_cost = smac.validate(Tab.configspace.get_default_configuration())\n",
    "        # print(f\"Default cost ({intensifier.__class__.__name__}): {default_cost}\")\n",
    "        incumbent_cost = smac.validate(incumbent)\n",
    "        # print(f\"Incumbent cost ({intensifier.__class__.__name__}): {incumbent_cost}\")\n",
    "\n",
    "        facades.append(smac)\n",
    "    #     for arrt in dir(smac):\n",
    "    #         if not arrt.startswith(\"_\"):\n",
    "    #             print(arrt, getattr(smac, arrt))\n",
    "\n",
    "    # print(\"facades:\", facades)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # with open('smac_results_2h.txt', \"w\") as f:\n",
    "    #     pass\n",
    "    # profiler = LineProfiler()\n",
    "    # profiler.add_function(main)\n",
    "    # profiler.enable()\n",
    "\n",
    "    main()\n",
    "    # break\n",
    "\n",
    "    # profiler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of runhistory_data_CNN: 55\n",
      "CNN min 1:\n",
      "Config ID: 1, Cost: 0.15400000000000003, Parameters: {'batch_size': 1153, 'lr': 0.06494351719359896, 'max_epochs': 248, 'n_a': 14, 'n_d': 126, 'solver': 'adam'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "runhistory_path = \"smac_hyperband_output_budget_1hr_Tab/covertype/9c4c5c0cd3a8cf20d4d9b88fbc111330/0/runhistory.json\"\n",
    "\n",
    "with open(runhistory_path, \"r\") as file:\n",
    "    runhistory_data_CNN = json.load(file)\n",
    "\n",
    "print(\"Length of runhistory_data_CNN:\", len(runhistory_data_CNN[\"data\"]))\n",
    "\n",
    "configs_costs_CNN = []\n",
    "for entry in runhistory_data_CNN[\"data\"]:\n",
    "    # print(entry)\n",
    "    config_id = entry[0]  \n",
    "    cost = entry[4]  \n",
    "    configs_costs_CNN.append((entry[0], entry[4]))\n",
    "    configs_costs_CNN = list(set(configs_costs_CNN))\n",
    "\n",
    "    # print(f\"Config ID: {config_id}, Cost: {cost}\")\n",
    "    # if config_id == 44:# or config_id == 21:\n",
    "    #     params_CNN = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "    #     # print(entry)\n",
    "    #     print(f\"CNN config ID: {config_id}, Cost: {cost}, Parameters: {params_CNN}\\n\")\n",
    "\n",
    "\n",
    "# min_cost_config_CNN = []\n",
    "# min_cost_config_CNN = sorted(configs_costs_CNN, key=lambda x: x[1])[:3]\n",
    "# print(\"CNN min 3:\")\n",
    "# for config_id, cost in min_cost_config_CNN:\n",
    "    \n",
    "#     params = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "#     print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "min_cost_config_CNN = sorted(configs_costs_CNN, key=lambda x: x[1])[0]\n",
    "print(\"CNN min 1:\")\n",
    "config_id, cost = min_cost_config_CNN\n",
    "params = runhistory_data_CNN[\"configs\"][str(config_id)]\n",
    "print(f\"Config ID: {config_id}, Cost: {cost}, Parameters: {params}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electricity : {'batch_size': 1448, 'lr': 0.012932601870023621, 'max_epochs': 223, 'n_a': 115, 'n_d': 30, 'solver': 'adam'}\n",
      "eye_movements : {'batch_size': 854, 'lr': 0.04770160470431317, 'max_epochs': 68, 'n_a': 96, 'n_d': 13, 'solver': 'adam'}\n",
      "covertype : {'batch_size': 329, 'lr': 0.07764913525398745, 'max_epochs': 172, 'n_a': 61, 'n_d': 16, 'solver': 'adam'}\n",
      "albert : {'batch_size': 372, 'lr': 0.057986982211462416, 'max_epochs': 250, 'n_a': 32, 'n_d': 21, 'solver': 'adam'}\n",
      "default-of-credit-card-clients : {'batch_size': 431, 'lr': 0.004253519924914082, 'max_epochs': 191, 'n_a': 89, 'n_d': 126, 'solver': 'adam'}\n",
      "road-safety : {'batch_size': 950, 'lr': 0.03263898386975636, 'max_epochs': 160, 'n_a': 117, 'n_d': 86, 'solver': 'adam'}\n",
      "compas-two-years : {'batch_size': 214, 'lr': 0.040674632927953876, 'max_epochs': 105, 'n_a': 103, 'n_d': 112, 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, params in params_dict_Tab.items():\n",
    "    print(dataset_name, \":\", params)\n",
    "\n",
    "# with open('SmacResults/params_dict_Tab.json', 'w') as f:\n",
    "#     json.dump(params_dict_Tab, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter Kernel",
   "language": "python",
   "name": "neurodata"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
